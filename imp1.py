import dotenv
import streamlit as st
import os
from dotenv import load_dotenv

# –ò–º–ø–æ—Ä—Ç—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
try:
    import weaviate
    print("weaviate: —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("weaviate –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    from weaviate.auth import AuthApiKey
    print("AuthApiKey –∏–∑ weaviate.auth: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("AuthApiKey –∏–∑ weaviate.auth –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    import mistralai
    print("mistralai: —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("mistralai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    from mistralai import Mistral
    print("Mistral: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("Mistral –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    import langchain_mistralai
    print("langchain_mistralai: —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("langchain_mistralai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    from langchain_mistralai import ChatMistralAI
    print("ChatMistralAI: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("ChatMistralAI –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    import langchain
    print("langchain:", langchain.__version__)
except ImportError:
    print("langchain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    import langchain_core
    print("langchain_core:", langchain_core.__version__)
except ImportError:
    print("langchain_core –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

try:
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.output_parsers import StrOutputParser
    print("RunnablePassthrough, ChatPromptTemplate, MessagesPlaceholder, StrOutputParser –∏–∑ langchain_core –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
except ImportError as e:
    print(f"–ò–º–ø–æ—Ä—Ç –∏–∑ langchain_core –Ω–µ —É–¥–∞–ª—Å—è: {e}")

try:
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    print("create_history_aware_retriever –∏ create_retrieval_chain: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ –∏–∑ langchain.chains")
except ImportError as e:
    print(f"–ò–º–ø–æ—Ä—Ç create_history_aware_retriever –∏ create_retrieval_chain –Ω–µ —É–¥–∞–ª—Å—è: {e}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ langchain-community –∏ –≤–µ—Ä—Å–∏–∏")

try:
    from langchain.chains.combine_documents import create_stuff_documents_chain
    print("create_stuff_documents_chain: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError as e:
    print(f"–ò–º–ø–æ—Ä—Ç create_stuff_documents_chain –Ω–µ —É–¥–∞–ª—Å—è: {e}")

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    print("RecursiveCharacterTextSplitter: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("langchain_text_splitters –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ RecursiveCharacterTextSplitter –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    from langchain_huggingface import HuggingFaceEmbeddings
    print("HuggingFaceEmbeddings: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("langchain_huggingface –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ HuggingFaceEmbeddings –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    from langchain_community.document_loaders import PyPDFLoader
    print("PyPDFLoader: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("langchain_community.document_loaders –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ PyPDFLoader –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    from langchain_community.vectorstores import Weaviate
    print("Weaviate –∏–∑ langchain_community.vectorstores: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    print("langchain_community.vectorstores –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ Weaviate –Ω–µ –Ω–∞–π–¥–µ–Ω")

try:
    import langchain_community
    print("langchain-community:", langchain_community.__version__)
except ImportError:
    print("langchain-community –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫
try:
    from sentence_transformers import SentenceTransformer, util
    print("sentence-transformers: –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ –¥–ª—è –º–µ—Ç—Ä–∏–∫")
    METRICS_AVAILABLE = True
except ImportError:
    print("sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥—É—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    METRICS_AVAILABLE = False

from langchain_core.documents import Document
from langchain_core.messages import AIMessageChunk
from file_loader import PDFProcessor

dotenv.load_dotenv()

print("weaviate version:", weaviate.__version__)
print("langchain version:", langchain.__version__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
DB_DOCS_LIMIT = 10

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
api_key = os.getenv("MISTRAL_API_KEY")
model = ChatMistralAI(
    api_key=api_key,
    model="mistral-large-latest",
    temperature=0.7,
    max_tokens=1024  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
)
output_parser = StrOutputParser()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
if METRICS_AVAILABLE:
    metrics_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    print("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")


def calculate_answer_relevance(question, answer, context):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –∫ –≤–æ–ø—Ä–æ—Å—É –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
    if not METRICS_AVAILABLE:
        return None
    
    try:
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        question_emb = metrics_model.encode(question, convert_to_tensor=True)
        answer_emb = metrics_model.encode(answer, convert_to_tensor=True)
        context_emb = metrics_model.encode(context, convert_to_tensor=True)
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        q_a_similarity = util.pytorch_cos_sim(question_emb, answer_emb).item()
        a_c_similarity = util.pytorch_cos_sim(answer_emb, context_emb).item()
        
        print(f"\nüìä –ú–ï–¢–†–ò–ö–ò –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò:")
        print(f"  –í–æ–ø—Ä–æ—Å ‚Üî –û—Ç–≤–µ—Ç: {q_a_similarity:.4f}")
        print(f"  –û—Ç–≤–µ—Ç ‚Üî –ö–æ–Ω—Ç–µ–∫—Å—Ç: {a_c_similarity:.4f}")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {(q_a_similarity + a_c_similarity) / 2:.4f}")
        
        return {
            "question_answer_similarity": q_a_similarity,
            "answer_context_similarity": a_c_similarity,
            "average_relevance": (q_a_similarity + a_c_similarity) / 2
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")
        return None


def calculate_context_precision(retrieved_docs, user_query):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    if not METRICS_AVAILABLE or not retrieved_docs:
        return None
    
    try:
        query_emb = metrics_model.encode(user_query, convert_to_tensor=True)
        
        scores = []
        for doc in retrieved_docs:
            doc_emb = metrics_model.encode(doc.page_content, convert_to_tensor=True)
            similarity = util.pytorch_cos_sim(query_emb, doc_emb).item()
            scores.append(similarity)
        
        avg_score = sum(scores) / len(scores) if scores else 0
        
        print(f"\nüìä –ú–ï–¢–†–ò–ö–ò –ö–û–ù–¢–ï–ö–°–¢–ê:")
        print(f"  –ò–∑–≤–ª–µ—á–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(retrieved_docs)}")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {avg_score:.4f}")
        print(f"  –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {[f'{s:.3f}' for s in scores]}")
        
        return {
            "num_retrieved": len(retrieved_docs),
            "average_context_score": avg_score,
            "individual_scores": scores
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è precision: {e}")
        return None


def load_pdf_to_db(pdf_file):
    """–ó–∞–≥—Ä—É–∑–∫–∞ PDF —Ñ–∞–π–ª–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    docs = []
    os.makedirs("docs", exist_ok=True)

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
    unique_sources = list(set(st.session_state.rag_sources))
    print(f"üìö –¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(unique_sources)}/{DB_DOCS_LIMIT}")

    if pdf_file.name not in st.session_state.rag_sources:
        if len(unique_sources) < DB_DOCS_LIMIT:
            file_path = os.path.join("docs", pdf_file.name)
            with open(file_path, "wb") as f:
                f.write(pdf_file.read())

            try:
                loader = PDFProcessor()
                data = loader.process(file_path)

                for page_num, page_text in enumerate(data):
                    doc = Document(
                        page_content=page_text,
                        metadata={"source": pdf_file.name, "page": page_num + 1},
                    )
                    docs.append(doc)
                
                print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(docs)} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ {pdf_file.name}")

                # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö
                st.session_state.rag_sources.append(pdf_file.name)
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ –ë–î
                _split_and_load_docs(docs)
                
                st.toast(f"‚úÖ {pdf_file.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω", icon="‚úÖ")

            except Exception as e:
                st.toast(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF {pdf_file.name}: {e}", icon="‚ö†Ô∏è")
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF {pdf_file.name}: {e}")
        else:
            st.error(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({DB_DOCS_LIMIT}).")
            print(f"‚ùå –õ–∏–º–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç: {len(unique_sources)}/{DB_DOCS_LIMIT}")
    else:
        st.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {pdf_file.name} —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        print(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç {pdf_file.name} —É–∂–µ –≤ –±–∞–∑–µ")


def initialize_vector_db(docs):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö Weaviate"""
    WEAVIATE_CLUSTER = os.getenv("WEAVIATE_CLUSTER")
    WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")
    
    if not WEAVIATE_CLUSTER or not WEAVIATE_API_KEY:
        raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è WEAVIATE_CLUSTER –∏–ª–∏ WEAVIATE_API_KEY")
    
    WEAVIATE_URL = "https://" + WEAVIATE_CLUSTER
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è Weaviate 3.x
    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)
    )
    
    embedding_model_name = "sentence-transformers/all-mpnet-base-v2"
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
    
    vector_db = Weaviate.from_documents(
        docs, embeddings, client=client, by_text=False
    )
    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ vector_db ({len(docs)} —á–∞–Ω–∫–æ–≤)")
    return vector_db


def _split_and_load_docs(pages):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150
    )
    docs = text_splitter.split_documents(pages)
    
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(docs)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

    if "vector_db" not in st.session_state or st.session_state.vector_db is None:
        st.session_state.vector_db = initialize_vector_db(docs)
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ —Å {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    else:
        st.session_state.vector_db.add_documents(docs)
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É")


def stream_llm_response(llm, messages):
    """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –ë–ï–ó RAG"""
    response_message = ""
    for chunk in llm.stream(messages):
        if isinstance(chunk, AIMessageChunk) and chunk.content:
            response_message += chunk.content
            yield chunk.content
        else:
            continue
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
    st.session_state.messages.append({"role": "assistant", "content": response_message})


def _get_context_retriever_chain(vector_db, model):
    """–°–æ–∑–¥–∞–Ω–∏–µ retriever chain –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    if vector_db is None:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: vector_db = None, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—É—é –±–∞–∑—É")
        st.session_state.vector_db = initialize_vector_db([])
        vector_db = st.session_state.vector_db

    retriever = vector_db.as_retriever(
        search_type="mmr",  
        search_kwargs={
            "k": 15,  
            "fetch_k": 30,  
            "lambda_mult": 0.7  
        }
    )
    
    test_results = retriever.get_relevant_documents("test")
    print(f"Retriever —Ç–µ—Å—Ç: –Ω–∞–π–¥–µ–Ω–æ {len(test_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    prompt = ChatPromptTemplate.from_messages([
        ("user", "{input}"),
        ("user", """Generate a comprehensive search query to find ALL key themes and topics from the document. 
        When asked about document contents, search for: main topics, key people, numbers, events, and overall structure."""),
    ])

    retriever_chain = create_history_aware_retriever(model, retriever, prompt)
    return retriever_chain


def get_conversational_rag_chain(model):
    """–°–æ–∑–¥–∞–Ω–∏–µ RAG —Ü–µ–ø–æ—á–∫–∏ —Å –∏—Å—Ç–æ—Ä–∏–µ–π —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
    retriever_chain = _get_context_retriever_chain(st.session_state.vector_db, model)

    if st.session_state.rag_sources:
        unique_sources = list(set(
            item.name if hasattr(item, 'name') else str(item) 
            for item in st.session_state.rag_sources
        ))
        loaded_docs_list = ", ".join(unique_sources)
    else:
        loaded_docs_list = "–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"

    print(f"=== Loaded documents: {loaded_docs_list} ===")

    prompt = ChatPromptTemplate.from_messages([
        ("system",
        f"""You are a helpful AI assistant. Answer user questions based on the provided context.

    CONTEXT FROM DOCUMENTS:
    {{context}}

    LOADED DOCUMENTS: {loaded_docs_list}

    INSTRUCTIONS:
    - The context above contains the actual content from the loaded PDF documents
    - When asked "what's inside the file" or similar questions, the context IS the file content
    - Answer directly based on what you see in the context
    - If asked about file contents, summarize or quote the context directly
    - The context shows exactly what is written in the documents
    - If the context is empty or doesn't contain relevant info, say so clearly
    - If you see [Image: ...] that's a describe of image in text. Use it in answer if you need."""),
        MessagesPlaceholder(variable_name="messages"),
        ("user", "{input}"),
    ])
    stuff_documents_chain = create_stuff_documents_chain(model, prompt)

    return create_retrieval_chain(retriever_chain, stuff_documents_chain)



def stream_llm_rag_response(messages):
    """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –° RAG + –º–µ—Ç—Ä–∏–∫–∏"""
    model = ChatMistralAI(
        api_key=os.getenv("MISTRAL_API_KEY"),
        model="mistral-large-latest",
        temperature=0.3,
        max_tokens=16248,
        streaming=True
    )
    
    user_query = messages[-1].content
    print(f"\n{'='*60}")
    print(f"USER QUERY: {user_query}")
    print(f"{'='*60}")
    
    if st.session_state.vector_db is None:
        print("‚ùå –û–®–ò–ë–ö–ê: vector_db –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
        error_msg = "‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç."
        st.session_state.messages.append({"role": "assistant", "content": error_msg})
        yield error_msg
        return
    
    retrieved_docs = []
    full_context = ""
    try:
        retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 5})
        retrieved_docs = retriever.get_relevant_documents(user_query)
        print(f"\nüìö RETRIEVED: {len(retrieved_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        for i, doc in enumerate(retrieved_docs):
            print(f"\n–î–æ–∫—É–º–µ–Ω—Ç {i+1}:")
            print(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source')}")
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞: {doc.metadata.get('page')}")
            print(f"  –¢–µ–∫—Å—Ç: {doc.page_content[:200]}...")
        
        # –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        full_context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        print(f"\nüìÑ –ü–û–õ–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ({len(full_context)} —Å–∏–º–≤–æ–ª–æ–≤):")
        print(full_context[:500] + "...\n")
        
        # –ú–ï–¢–†–ò–ö–ò –ö–û–ù–¢–ï–ö–°–¢–ê
        calculate_context_precision(retrieved_docs, user_query)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ retrieval: {e}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ RAG-—Ü–µ–ø–æ—á–∫–∏ –∏ —Å—Ç—Ä–∏–º–∏–Ω–≥
    conversation_rag_chain = get_conversational_rag_chain(model)
    response_message = "*(RAG Response)*\n"
    
    for chunk in conversation_rag_chain.pick("answer").stream({
        "messages": messages[:-1], 
        "input": user_query
    }):
        response_message += chunk
        yield chunk
    
    # –ú–ï–¢–†–ò–ö–ò –û–¢–í–ï–¢–ê
    if full_context and len(response_message) > 20:
        calculate_answer_relevance(user_query, response_message, full_context)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏—é
    st.session_state.messages.append({"role": "assistant", "content": response_message})
    
    print(f"\n{'='*60}")
    print("‚úÖ –û–¢–í–ï–¢ –ó–ê–í–ï–†–®–ï–ù")
    print(f"  –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response_message)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"{'='*60}\n")


def clear_weaviate_data():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Weaviate Cloud"""
    try:
        WEAVIATE_CLUSTER = os.getenv("WEAVIATE_CLUSTER")
        WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")
        
        if not WEAVIATE_CLUSTER or not WEAVIATE_API_KEY:
            print("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Weaviate")
            st.toast("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î", icon="‚ö†Ô∏è")
            return
        
        WEAVIATE_URL = "https://" + WEAVIATE_CLUSTER
        
        client = weaviate.Client(
            url=WEAVIATE_URL,
            auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)
        )
        
        schema = client.schema.get()
        
        if 'classes' in schema and len(schema['classes']) > 0:
            for class_obj in schema['classes']:
                class_name = class_obj['class']
                print(f"–£–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞: {class_name}")
                client.schema.delete_class(class_name)
            print("‚úÖ Weaviate –ë–î –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω–∞")
            st.toast("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–µ–Ω–∞", icon="‚úÖ")
        else:
            print("‚ÑπÔ∏è –ë–î —É–∂–µ –ø—É—Å—Ç–∞")
            st.toast("‚ÑπÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –ø—É—Å—Ç–∞", icon="‚ÑπÔ∏è")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ Weaviate: {e}")
        st.toast(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ë–î: {e}", icon="‚ö†Ô∏è")
