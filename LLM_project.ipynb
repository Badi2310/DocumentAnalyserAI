{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c63c161ec504640adce7bb6456ce07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3652b1e0d8f74c3880caf9d9e0ff4ac6",
              "IPY_MODEL_e350952d965142a39b75b42cb061f792",
              "IPY_MODEL_ec832bd8ee034f0695dcb2021d7c9278"
            ],
            "layout": "IPY_MODEL_feb62c60abc64fdf9d511641a86772b6"
          }
        },
        "3652b1e0d8f74c3880caf9d9e0ff4ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e9e450bfc7742c4971982ee6c14487d",
            "placeholder": "​",
            "style": "IPY_MODEL_789b7bf07ed64687ac6e577bebe73937",
            "value": "modules.json: 100%"
          }
        },
        "e350952d965142a39b75b42cb061f792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3fa5994afb488f99f19f783ff542ae",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d031315caed4dacb5a5fee94ff7da73",
            "value": 349
          }
        },
        "ec832bd8ee034f0695dcb2021d7c9278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec11d7fb1d446e69c48aa95daea69ab",
            "placeholder": "​",
            "style": "IPY_MODEL_824f404ec0d84a7193cd1be9666fbaa8",
            "value": " 349/349 [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "feb62c60abc64fdf9d511641a86772b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9e450bfc7742c4971982ee6c14487d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789b7bf07ed64687ac6e577bebe73937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb3fa5994afb488f99f19f783ff542ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d031315caed4dacb5a5fee94ff7da73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ec11d7fb1d446e69c48aa95daea69ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "824f404ec0d84a7193cd1be9666fbaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65569198d5e049cd810336b3d50fc6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7abef1421ca4210a371bf9031979a4f",
              "IPY_MODEL_587704931fd3408b85acb2ed15f3af78",
              "IPY_MODEL_81178efff367451f807cae878c58b354"
            ],
            "layout": "IPY_MODEL_92d883ca481c4ef0af603b97ef1599b0"
          }
        },
        "a7abef1421ca4210a371bf9031979a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13142dfa5b4342eb9b41221dedce2f70",
            "placeholder": "​",
            "style": "IPY_MODEL_f12a0e8657ec4f3fa3aa9750635e8ca0",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "587704931fd3408b85acb2ed15f3af78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e0a3149f4ab437ab4e615eedfc52361",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9393342e318474c8e7c9c33c093f814",
            "value": 116
          }
        },
        "81178efff367451f807cae878c58b354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98e20ab02c6944328b60e4adc545dd80",
            "placeholder": "​",
            "style": "IPY_MODEL_f797c298b18f47de98d8e722ce3dc3bd",
            "value": " 116/116 [00:00&lt;00:00, 6.06kB/s]"
          }
        },
        "92d883ca481c4ef0af603b97ef1599b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13142dfa5b4342eb9b41221dedce2f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f12a0e8657ec4f3fa3aa9750635e8ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e0a3149f4ab437ab4e615eedfc52361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9393342e318474c8e7c9c33c093f814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98e20ab02c6944328b60e4adc545dd80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f797c298b18f47de98d8e722ce3dc3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "594ac05acdb74516a661a2d72aef60ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_428763fc2a2f481fb670dd0eba635bac",
              "IPY_MODEL_be4991b29a9b47258c133d3d44833029",
              "IPY_MODEL_776b4095a4de4c9b86de984a7ea856b1"
            ],
            "layout": "IPY_MODEL_92356ba0ac6244a79c1f219a36b6edec"
          }
        },
        "428763fc2a2f481fb670dd0eba635bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8d37f2080fa4ee7b929167ab385475a",
            "placeholder": "​",
            "style": "IPY_MODEL_7a79f7439fb0432ca38af9fa6505bb40",
            "value": "README.md: "
          }
        },
        "be4991b29a9b47258c133d3d44833029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b6075324fa4a12a7e94f10c09ead3b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ab81a07855c456781406ebefef6d758",
            "value": 1
          }
        },
        "776b4095a4de4c9b86de984a7ea856b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9caf6c75e9504309b96cf31ea95f7183",
            "placeholder": "​",
            "style": "IPY_MODEL_6103707b4777415585103d47c9264ba7",
            "value": " 11.6k/? [00:00&lt;00:00, 561kB/s]"
          }
        },
        "92356ba0ac6244a79c1f219a36b6edec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d37f2080fa4ee7b929167ab385475a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a79f7439fb0432ca38af9fa6505bb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6b6075324fa4a12a7e94f10c09ead3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7ab81a07855c456781406ebefef6d758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9caf6c75e9504309b96cf31ea95f7183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6103707b4777415585103d47c9264ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d739cfe817469399e7161fae16b816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7db72c12c614637be98c0198d6c1e78",
              "IPY_MODEL_6fbff9f1996445ee8d8a293c4970ba5e",
              "IPY_MODEL_c915c1c8c7644ac6b8a489d0e6adcbdb"
            ],
            "layout": "IPY_MODEL_e9e2e48cb5a44d3a8283e9b7e370fe7e"
          }
        },
        "d7db72c12c614637be98c0198d6c1e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e951952fc4764a48900ec794bf039d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_b61defa64ef841989016665e76d021b3",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "6fbff9f1996445ee8d8a293c4970ba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c638e532c34ba38c38cc91f03d41d6",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f66e776a769d446d91acc3586c84a934",
            "value": 53
          }
        },
        "c915c1c8c7644ac6b8a489d0e6adcbdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337055f2e7b14dc9ba9466af21a8df3d",
            "placeholder": "​",
            "style": "IPY_MODEL_462353a18b7145058a077940193a0171",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.87kB/s]"
          }
        },
        "e9e2e48cb5a44d3a8283e9b7e370fe7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e951952fc4764a48900ec794bf039d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61defa64ef841989016665e76d021b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64c638e532c34ba38c38cc91f03d41d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66e776a769d446d91acc3586c84a934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "337055f2e7b14dc9ba9466af21a8df3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "462353a18b7145058a077940193a0171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d62648797e4340f0a0378d70fba0d6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b450a990f5f34b95947b16794afab63a",
              "IPY_MODEL_af14ff8aa1814688a49340e989edead3",
              "IPY_MODEL_fbd2791ed8084434ac2fe783ff1b74bd"
            ],
            "layout": "IPY_MODEL_756eda1229d54f1381bac09b4a7b4159"
          }
        },
        "b450a990f5f34b95947b16794afab63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c31b7f1a7164fa295c15cfbc9c9be4a",
            "placeholder": "​",
            "style": "IPY_MODEL_2d760d849ed546599aa02ae927279fe2",
            "value": "config.json: 100%"
          }
        },
        "af14ff8aa1814688a49340e989edead3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_726ee45e96ac457e946066832b5d1673",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96a25420cd9e422b88143df294f40204",
            "value": 571
          }
        },
        "fbd2791ed8084434ac2fe783ff1b74bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d494e2be5ae64a7f97449c3378980e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_9fbbab679fc144bf8ed410b706a1724c",
            "value": " 571/571 [00:00&lt;00:00, 44.9kB/s]"
          }
        },
        "756eda1229d54f1381bac09b4a7b4159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c31b7f1a7164fa295c15cfbc9c9be4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d760d849ed546599aa02ae927279fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "726ee45e96ac457e946066832b5d1673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a25420cd9e422b88143df294f40204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d494e2be5ae64a7f97449c3378980e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fbbab679fc144bf8ed410b706a1724c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac3b138194654737b85062eec317ead5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_770dc95a3193446eaaaf688bfeb08f97",
              "IPY_MODEL_de9a4b91c3084c7180c30fd4740dd885",
              "IPY_MODEL_4365a2416c784e77889bda1c324b37b8"
            ],
            "layout": "IPY_MODEL_dde8335f6b6d48bcbdc2305f1d9126d1"
          }
        },
        "770dc95a3193446eaaaf688bfeb08f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae18d5a3d484ec5a0a114b9d34ab587",
            "placeholder": "​",
            "style": "IPY_MODEL_51fb9641cb684979acc2f06588fae3eb",
            "value": "model.safetensors: 100%"
          }
        },
        "de9a4b91c3084c7180c30fd4740dd885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e66a8e5ef8e4649a75833f218393441",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d46aa583da8471290093f4328375758",
            "value": 437971872
          }
        },
        "4365a2416c784e77889bda1c324b37b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4fa636f8f8a4c11944743a3ae118dc7",
            "placeholder": "​",
            "style": "IPY_MODEL_b1737aaea82940e5b7ef79f16836fdd2",
            "value": " 438M/438M [00:03&lt;00:00, 205MB/s]"
          }
        },
        "dde8335f6b6d48bcbdc2305f1d9126d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae18d5a3d484ec5a0a114b9d34ab587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fb9641cb684979acc2f06588fae3eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e66a8e5ef8e4649a75833f218393441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d46aa583da8471290093f4328375758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4fa636f8f8a4c11944743a3ae118dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1737aaea82940e5b7ef79f16836fdd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ef43210f4e948e4bbfad66da46062cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9488a5cc84fd48a782753bcd681ab8e6",
              "IPY_MODEL_ad36412ff6804d589c0e742e08079bad",
              "IPY_MODEL_2fc555ae01294bb7a0dc34e7f4465115"
            ],
            "layout": "IPY_MODEL_65743bfc423b4dba9071a08c67a9d234"
          }
        },
        "9488a5cc84fd48a782753bcd681ab8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3128c84e5e04a578c65d6175b762f50",
            "placeholder": "​",
            "style": "IPY_MODEL_5dcf7a0de03a42d893d9e0e6a5295ef3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ad36412ff6804d589c0e742e08079bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd40699cf1f450aaa8fefe6185c4e88",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27cd6a38ee964c0a80f2f8b8233032a4",
            "value": 363
          }
        },
        "2fc555ae01294bb7a0dc34e7f4465115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_851cded58f7741528fa3f75086f7b69b",
            "placeholder": "​",
            "style": "IPY_MODEL_0499bb6b5d1e4d0680bdb73b7299f7a9",
            "value": " 363/363 [00:00&lt;00:00, 14.9kB/s]"
          }
        },
        "65743bfc423b4dba9071a08c67a9d234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3128c84e5e04a578c65d6175b762f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcf7a0de03a42d893d9e0e6a5295ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdd40699cf1f450aaa8fefe6185c4e88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27cd6a38ee964c0a80f2f8b8233032a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "851cded58f7741528fa3f75086f7b69b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0499bb6b5d1e4d0680bdb73b7299f7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bfacf5b88444db987144d4d80ba211e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d355e8018e7411bb987595d3282a16e",
              "IPY_MODEL_b873f912f7d946b8b8922897beaf717e",
              "IPY_MODEL_fb7fa4c16f284e029152a0a18d6471c5"
            ],
            "layout": "IPY_MODEL_9e0ec260461340209471e0fc1db2dc9d"
          }
        },
        "0d355e8018e7411bb987595d3282a16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a7c5d956474ac09605cc080caaab19",
            "placeholder": "​",
            "style": "IPY_MODEL_a07054577ddc44f180e375a38039160c",
            "value": "vocab.txt: "
          }
        },
        "b873f912f7d946b8b8922897beaf717e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85857aa1ac9f4711b22d05f64090aade",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acb56058a5b84bab9e73f33866890bcf",
            "value": 1
          }
        },
        "fb7fa4c16f284e029152a0a18d6471c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76517d9f4124424e9ac830f090936fb6",
            "placeholder": "​",
            "style": "IPY_MODEL_bc8f6197fdea4337ba6f7f0967acc0be",
            "value": " 232k/? [00:00&lt;00:00, 9.66MB/s]"
          }
        },
        "9e0ec260461340209471e0fc1db2dc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a7c5d956474ac09605cc080caaab19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07054577ddc44f180e375a38039160c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85857aa1ac9f4711b22d05f64090aade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "acb56058a5b84bab9e73f33866890bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76517d9f4124424e9ac830f090936fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc8f6197fdea4337ba6f7f0967acc0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96dac914ff1f4febb60047c5415bb96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e57fb762526f46e982af819b4dbfe754",
              "IPY_MODEL_afbfc6b621bb43e69279f2542b63afed",
              "IPY_MODEL_808acd9205384ba8af805cfbe177face"
            ],
            "layout": "IPY_MODEL_7291cc309e7a46abb5d501eb31f1b5c0"
          }
        },
        "e57fb762526f46e982af819b4dbfe754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cd26bc0718f47b499331b836e7cf697",
            "placeholder": "​",
            "style": "IPY_MODEL_d04247ca7bdd4b50ae7d1ff211b075b8",
            "value": "tokenizer.json: "
          }
        },
        "afbfc6b621bb43e69279f2542b63afed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab98e4735844054a97adade5032d0bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_054a6ec2121f48b59e783ac867ff7f9f",
            "value": 1
          }
        },
        "808acd9205384ba8af805cfbe177face": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f3ee01e4754741811d2985afaf80aa",
            "placeholder": "​",
            "style": "IPY_MODEL_fca7817f0b84498189771d67f948416c",
            "value": " 466k/? [00:00&lt;00:00, 19.2MB/s]"
          }
        },
        "7291cc309e7a46abb5d501eb31f1b5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cd26bc0718f47b499331b836e7cf697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04247ca7bdd4b50ae7d1ff211b075b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ab98e4735844054a97adade5032d0bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "054a6ec2121f48b59e783ac867ff7f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86f3ee01e4754741811d2985afaf80aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca7817f0b84498189771d67f948416c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebf520c424bb4a6c9d2212bbf6c6c813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ea8e4adc7ab4fcda458e81f076ac0b6",
              "IPY_MODEL_a32ef385b157447a88e176df4e18722e",
              "IPY_MODEL_51b66c3d6fb94cdcac951e87ae474710"
            ],
            "layout": "IPY_MODEL_b59aafc855d140109a3bce64b9a484e5"
          }
        },
        "5ea8e4adc7ab4fcda458e81f076ac0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f80e56706cab44dda3426de4ce49ec40",
            "placeholder": "​",
            "style": "IPY_MODEL_8c6d1139e257432b91666ae0a04b79f5",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a32ef385b157447a88e176df4e18722e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d79495f2623f474bb5acbbdf08bfe327",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091e87323df44437b85815e384f6f2bc",
            "value": 239
          }
        },
        "51b66c3d6fb94cdcac951e87ae474710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_217e82ba033648d5b94f4a55796f1643",
            "placeholder": "​",
            "style": "IPY_MODEL_d934dac5094540818895c7fe52df0bab",
            "value": " 239/239 [00:00&lt;00:00, 22.4kB/s]"
          }
        },
        "b59aafc855d140109a3bce64b9a484e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80e56706cab44dda3426de4ce49ec40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c6d1139e257432b91666ae0a04b79f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d79495f2623f474bb5acbbdf08bfe327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091e87323df44437b85815e384f6f2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "217e82ba033648d5b94f4a55796f1643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d934dac5094540818895c7fe52df0bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92599f8eb31a41588f6b7fc2a74b5f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a35b78a36d24bc4981374766a81b025",
              "IPY_MODEL_0aadd6e8490941ae9bdf20403d7886e1",
              "IPY_MODEL_a132f90a261c4061bab95edad89c7c9a"
            ],
            "layout": "IPY_MODEL_1b274a4b65cb41fbba46e2e2561f30ce"
          }
        },
        "9a35b78a36d24bc4981374766a81b025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14ef4d65267646319f23195ec3d0b5f3",
            "placeholder": "​",
            "style": "IPY_MODEL_8fc3978e001d49e68447743679f3fe66",
            "value": "config.json: 100%"
          }
        },
        "0aadd6e8490941ae9bdf20403d7886e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e090e21ffa1741e984af4a2035a7ff0a",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_304e179b09094c47a10ed3ae6007f7b5",
            "value": 190
          }
        },
        "a132f90a261c4061bab95edad89c7c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e57f2717fe44f529defa3963dac1e82",
            "placeholder": "​",
            "style": "IPY_MODEL_2722025bc7064f46b4e3e034d7083ac3",
            "value": " 190/190 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "1b274a4b65cb41fbba46e2e2561f30ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14ef4d65267646319f23195ec3d0b5f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc3978e001d49e68447743679f3fe66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e090e21ffa1741e984af4a2035a7ff0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "304e179b09094c47a10ed3ae6007f7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e57f2717fe44f529defa3963dac1e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2722025bc7064f46b4e3e034d7083ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Выбор проекта\n",
        "\n",
        "Дедлайн - 23:59 8 октября\n",
        "После дедлайна я закрою таблицу на редактирование\n",
        "\n",
        "В колонку \"Предложение проекта\" можете записать свои идеи для проекта. Если вы хотите присоединиться к какой то команде и делать их проект, то можете оставить это поле пустым\n",
        "Вам важно за неделю разбиться на группы 3-5 человек и вписать в поле \"Финальный выбор проекта\" название проекта, который вы будете делать и сдавать\n",
        "\n",
        "Вы должны сделать 2 вещи:\n",
        "1) К 8 октября у каждого должна быть заполнена графа \"Финальный выбор проекта\" - это тот проект, который вы будете делать вместе с командой\n",
        "2) Важно не только определиться с проектом, но и найти людей, с кем вы его будете делать\n",
        "\n",
        "На проекте могут работать от 3-5 человек\n",
        "\n",
        "Если вы не выберете проект, то вы не сможете сдать курс, потому что проект составляет 70% оценки за курс\n",
        "\n",
        "Делать проект в одиночку не рекомендуется\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1NVQv2CNGeyoHYbYGBFx0rhv7iDNYAJm8pQBImWeCjwg/edit?usp=sharing\n",
        "\n",
        "Минимальные требования по реализации проекта\n",
        "\n",
        "1) У вас должна использоваться LLM/VLM в вашем проекте (можно API, можно локально разворачивать)\n",
        "2) У вас обязательно в проекте должен быть реализован RAG\n",
        "3) Ваша задача должна быть решена с высоким качеством. Здесь вы сами решаете как будете замерять качество\n",
        "4) Презентация проекта и красивое оформление кода по стилю\n",
        "\n",
        "Описание выше соответствует решению с одним LLM агентом. Но будет приветствоваться, если вы сделаете мультиагентную систему - достаточно будет 2 агентов. Мультиагентность мы будем обсуждать в нашем курсе ближе к ноябрю\n",
        "\n",
        "От себя добавлю, что если вы хотите разворачивать модель локально, то тут запрета нет. Но учитывайте, что вам нужны мощности для больших моделей, а маленькие модели вряд ли обеспечат вам хорошее качество на выходе"
      ],
      "metadata": {
        "id": "6XR5weqSUCh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Разобраться с API Mistral\n",
        "3) Разобраться с RAG\n",
        "4) Придумать метрику качества"
      ],
      "metadata": {
        "id": "AWVZcJMTT0dt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YQy3sl9KdwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f900c35f-b957-4a55-be42-d7a668ba0591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Collecting mistralai\n",
            "  Using cached mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.28.1)\n",
            "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
            "  Using cached invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.11.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from mistralai) (6.0.3)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mistralai) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
            "Using cached mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
            "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Using cached invoke-2.2.1-py3-none-any.whl (160 kB)\n",
            "Installing collected packages: invoke, eval-type-backport, mistralai\n",
            "Successfully installed eval-type-backport-0.2.2 invoke-2.2.1 mistralai-1.9.11\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv==1.0.1\n",
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from IPython.display import display, Markdown\n",
        "import re\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from mistralai import Mistral"
      ],
      "metadata": {
        "id": "v0wLof4h3hh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the secret\n",
        "api_key = userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"❌ MISTRAL_API_KEY not found in Colab secrets!\")\n",
        "else:\n",
        "    print(\"✅ API key loaded successfully from Colab secrets!\")\n",
        "\n",
        "# Initialize Mistral client\n",
        "client_1 = Mistral(api_key=api_key)\n",
        "\n",
        "# Test connection\n",
        "def test_connection():\n",
        "    try:\n",
        "        models = client.models.list()\n",
        "        print(\"✅ Connected successfully!\")\n",
        "        print(f\"Available models: {[m.id for m in models.data]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Connection failed: {e}\")\n",
        "        print(\"💡 If key is not active yet, wait a few minutes and try again\")\n",
        "\n",
        "test_connection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv5IWwJe3jnz",
        "outputId": "86f47cb4-32c4-4c1b-c6fc-f0d12d5fd414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API key loaded successfully from Colab secrets!\n",
            "❌ Connection failed: name 'client' is not defined\n",
            "💡 If key is not active yet, wait a few minutes and try again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- 1) Загрузка pdf-документа в гугл коллаб -->\n",
        "2) Преобразование pdf-документа (doc, docx) (загрузить в гугл диск) (PYpdf) в текст. $\\textbf{Айторе}$\n",
        "\n",
        "\n",
        "3) препроцессинг\n",
        "\n",
        "3.1) Реализовать OCR и CLIP. $\\textbf{Кирилл}$\n",
        "\n",
        "3.2) Убирание знаков припинание, рассмотр заглавных букв и прочего для улучшения качества ответов модельки $\\textbf{Илья}$\n",
        "\n",
        "...\n",
        "\n",
        "4) Векторизация+индексация в RAG (mistral embedding), загрузим в weaviate (что будем хранить?) - (семантику чанка) $\\textbf{Алан, Мавджуда}$\n",
        "\n",
        " <!-- 6) Получение вопроса от пользователя -->\n",
        "\n",
        "7) Нахождение релевантных чанков по вектору вопроса (косинусной метрикой) $\\textbf{изи}$\n",
        "\n",
        " <!-- 8) Использование Mistral ('mistral-large-latest') для ответа (передаем эти чанки как контекст в промпт для Mistral API) -->\n",
        "\n",
        " <!-- 9) Вывод ответа -->"
      ],
      "metadata": {
        "id": "CDXOAs2ICncZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain sentence_transformers langchain_huggingface langchain_community unstructured nltk==3.9.1\n",
        "!pip install jq pypdf langchain_chroma rank_bm25 arxiv langchain-mistralai scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W08VWVnnjnTf",
        "outputId": "61ddb1c6-9cb4-4535-df51-64da865a5d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.18.18-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1) (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.41)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.57.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.4.4)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured) (4.13.5)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.11.11-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.42.3-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured) (2.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (1.0.9)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-6.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (2.0.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.23)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.18.18-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2025.11.11-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.7/167.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.42.3-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=12ca0461c76d456b4483bd56cd15866408d9d5a4f1cfd85f2c8a27633fcecbbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, requests, rapidfuzz, python-magic, python-iso639, pypdf, olefile, mypy-extensions, marshmallow, langdetect, emoji, backoff, typing-inspect, python-oxmsg, unstructured-client, dataclasses-json, unstructured, langchain_huggingface, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.15.0 filetype-1.2.0 langchain_community-0.3.31 langchain_huggingface-0.3.1 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.1.0 olefile-0.47 pypdf-6.2.0 python-iso639-2025.11.11 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.14.3 requests-2.32.5 typing-inspect-0.9.0 unstructured-0.18.18 unstructured-client-0.42.3\n",
            "Collecting jq\n",
            "  Downloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.2.0)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.3.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langchain-mistralai\n",
            "  Downloading langchain_mistralai-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting chromadb<2.0.0,>=1.0.20 (from langchain_chroma)\n",
            "  Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain_chroma)\n",
            "  Downloading langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain_chroma) (2.0.2)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.5)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (0.4.3)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (2.11.10)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from langchain-mistralai) (0.22.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.38.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.37.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (3.11.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.0.20->langchain_chroma) (4.25.1)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.25.2->langchain-mistralai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.25.2->langchain-mistralai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_chroma) (0.4.41)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_chroma) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-mistralai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-mistralai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (0.36.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1.0.0,>=0.15.1->langchain-mistralai) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (2.0.0)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests~=2.32.0->arxiv)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_chroma) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain_chroma) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain_chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.71.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.0.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0,>=0.25.2->langchain-mistralai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain_chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain_chroma) (0.6.1)\n",
            "Downloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading arxiv-2.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading langchain_mistralai-1.0.1-py3-none-any.whl (17 kB)\n",
            "Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika, sgmllib3k\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=d105fa6bb5a466ce0c1904af20d5a0b3026218d177b1334e887acccf2cfe1028\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=fe62ec7e30959c22b5c8174e8e50fa37c99a9f11fd7adef1f24105f12765b666\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built pypika sgmllib3k\n",
            "Installing collected packages: sgmllib3k, pypika, durationpy, uvloop, urllib3, rank_bm25, pybase64, opentelemetry-proto, mmh3, jq, humanfriendly, httptools, feedparser, bcrypt, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, arxiv, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-core, langchain-mistralai, chromadb, langchain_chroma\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 0.3.1 requires langchain-core<1.0.0,>=0.3.70, but you have langchain-core 1.0.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arxiv-2.3.0 bcrypt-5.0.0 chromadb-1.3.4 coloredlogs-15.0.1 durationpy-0.10 feedparser-6.0.12 httptools-0.7.1 humanfriendly-10.0 jq-1.10.0 kubernetes-34.1.0 langchain-core-1.0.4 langchain-mistralai-1.0.1 langchain_chroma-1.0.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 rank_bm25-0.2.2 sgmllib3k-1.0.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "76KVRRc1LKf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.find('tokenizers/punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "kc_eVTNDig0Z",
        "outputId": "a9f53eff-7e18-412d-c5dc-c880a3c66402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileSystemPathPointer('/root/nltk_data/tokenizers/punkt_tab')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/langchain-ai/langchain/9a277cbe007706b48fa98787ab85a11c59ccba2e/docs/docs/integrations/document_loaders/example_data/layout-parser-paper.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3hWmw6QjXhv",
        "outputId": "ddea0c4e-6ba0-418f-974c-6300a264e62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-12 09:09:49--  https://raw.githubusercontent.com/langchain-ai/langchain/9a277cbe007706b48fa98787ab85a11c59ccba2e/docs/docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4686220 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘layout-parser-paper.pdf’\n",
            "\n",
            "layout-parser-paper 100%[===================>]   4.47M  18.8MB/s    in 0.2s    \n",
            "\n",
            "2025-11-12 09:09:50 (18.8 MB/s) - ‘layout-parser-paper.pdf’ saved [4686220/4686220]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "ndycfJzRjfAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive connected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bfBgabcWQAb",
        "outputId": "d1d44422-6e01-43c2-c374-65d71a7ede58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive connected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_FILE_PATH = \"/content/drive/MyDrive/MulRAG.pdf\""
      ],
      "metadata": {
        "id": "Fkt7XYfXkCTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(PDF_FILE_PATH, extract_images=True)\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "cMu77BV4l4vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Un2MY8XFXD1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swi7oW5DDrjt",
        "outputId": "177c5f27-a8a4-46c6-e19d-be7be8dd573d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images – much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the\\nﬁrst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-\\naugmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries: These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model’s parameters. More speciﬁcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model’s predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneﬁcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount\\nof multimodal knowledge available on the web—\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user ﬁrst poses a ques-\\ntion such as “What can be found on the White\\nHouse balconies at Christmas”. The system then\\nretrieves relevant items from its memory, for exam-\\narXiv:2210.02928v2  [cs.CL]  20 Oct 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='ple, the ﬁrst image of Figure 1 with the caption\\n“White House during Christmas”, which it uses to\\nproduce the answer “wreaths and garlands”. Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview: retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciﬁcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nﬁrst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes\\nimage-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciﬁcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model’s input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-\\nerative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides\\nthe model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during ﬁne-\\ntuning Figure 2 the model’s input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We ﬁne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to ﬁrst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-\\ntractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniﬁed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was ﬁrst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another\\nfamily of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneﬁt downstream knowledge in-\\ntensive tasks (e.g. Question Answering). REALM\\nis an encoder-only model trained with masked lan-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal\\nfeatures for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the ﬁrst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA’s\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and\\nMIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-\\nmodalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form\\nfθ(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfθ and\\ndecoder gθ. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ∈RLi×D, where Li is the length of the im-\\nage tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT ∈RLt×D. For k images and n text inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e = [e1\\nI; e1\\nT; ··· ; ek\\nI; en\\nT] ∈R(kLt+nLi)×D,\\nwhich is fed to another bi-directional transformer\\nfθ initialized from T5. We enable cross-attention'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fθ(e) ∈ R(kLt+nLi)×D.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fθ(e)[CLS] ∈RD for dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query q of any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. Speciﬁcally, we apply the backbone encoder\\nfθ to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m ∈M to ﬁnd the Top-K\\nnearest neighbors TopK(M|q) = [ m1, ··· , mk].\\nFormally, we deﬁne TopK(M|q) as follows:\\nTopK(M|q) = TopK\\nm∈M\\nfθ(q)[CLS] ·fθ(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query q as\\nan augmented input [m1, ··· , mk, q], which is fed\\nto the backbone encoder fθ to produce retrieval-\\naugmented encoding. The decoder model gθ uses\\nattention over this representation to generate tex-\\ntual outputs y = y1, ··· , yn token by token.\\np(yi|yi−1) = gθ(yi|fθ(TopK(M|q); q); y1:i−1)\\nwhere y is decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xI plus a text prompt xp. The exter-\\nnal memory Mcontains textual-only entries mT.\\nThe Top-K retrievalsmT\\n1 , ··· , mT\\nk are leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\nory MB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efﬁciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;\\nChangpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containing\\ncrawled image-text pairs ﬁltered by CLIP (Rad-\\nford et al., 2021). We apply rules to ﬁlter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but ﬁltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics\\nFor LAION and CC, we use the input image as\\nxI, and ‘generate caption:’ as the text promptxp.\\nFor VQA, we use the input image as xI and the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MB is con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse ﬁxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L = Lgen +\\nLcon as follows:\\nLcon = −log exp(fθ(xI, xp) ·fθ(mT))∑\\nm∈MB\\nexp(fθ(xI, xp) ·fθ(mT))\\nLgen = −log gθ(y|fθ(Mp; xI; xp))\\nMp =\\n{\\nTopK(MB|xI, xp) If (xI, xp) ∈PAQ/VQA\\nØ If (xI, xp) ∈LAION/CC\\nwhere Mp is the retrieved augmentation: if the\\ninput query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lcon is minimized to dis-\\ncriminate between the positive query-memory pairs'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\\nimplementation, while the lower part deﬁnes ﬁne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fθ(xI; xp)[CLS] and\\ncandidates fθ(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgen is min-\\nimized to generate target tokens y conditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe ﬁnetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge\\ndatastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1 The Top-K retrievals\\n{(mI\\n1, mT\\n1 ), ··· , (mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nﬁxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss functionL = Lcon+Lgen based\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.\\non the in-batch memory MB as follows:\\nLcon = −log exp(fθ(xq) ·fθ(mI; mT))∑\\nm∈MB\\nexp(fθ(xq) ·fθ(mI; mT))\\nLgen = −log gθ(y|fθ(TopK(MB|xq); xq))\\nThe in-batch memory MB is constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk, {mI\\ni, mI\\ni}k, {¯mI\\nj, ¯mT\\nj }k),\\nwhere m represents the positive (image, text)\\nsource, and ¯m represents the hard negative\\n(image, text) source provided by the dataset 2.\\nFor a batch with B examples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB =\\n{{mI\\ni, mI\\ni}1, {¯mI\\nj, ¯mT\\nj }1, ··· , {¯mI\\nj, ¯mT\\nj }B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq) and continue to opti-\\nmize Lgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use ﬁxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conﬁgurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we ﬁrst train the\\nmodel on LAION for 1M steps, and then continue\\ntraining on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then ﬁne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and ﬁxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn’t yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-\\nmodal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.\\nDataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model’s generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nﬂuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-\\nrectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors ﬁrst use the template\\nto generate questions and then ask crowd-workers\\nto ﬁlter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. Speciﬁcally, we choose the\\nquestions with types of ‘TextQ’ and ‘ImageQ’ to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K\\ntext and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During ﬁne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nﬁrst to select the most plausible source si, and then\\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\\nto autoregressively decode answer A with masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nﬁrst applies a question type classiﬁer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. Speciﬁcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to ﬁnd\\na set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input S to the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA’s results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiﬁcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reﬂect the high ﬂuency and accuracy\\nof MuRAG’s generation, and the improvement is\\nmore pronounced for full wiki.\\nWe show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniﬁcant, with 20+% improvement under both\\nsettings. Similarly, we ﬁnd that our model is more\\ncapable of handling full-wiki corpus.\\nEvaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofﬁcial test-set results indicated\\non leaderboard 3 as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to ﬂuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting 35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their ﬁne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,\\nPAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage ﬁne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different ﬁne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractor MuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-Wiki MuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,\\nthe performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the ﬁne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG’s pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model’s perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiﬁcantly behind text accuracy.\\nWe further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difﬁcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciﬁc region is difﬁcult, 3) the questions require\\noptical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the ﬁrst example, the model is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‘the angel is holding a dead body’. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the ﬁrst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiﬁcantly lower than the performance on\\nqueries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufﬁciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus’s format (image -> text) is differ-\\nent from ﬁne-tuning (text -> image+text). This\\nmisalignment limits the model’s performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-\\nformance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic ﬁltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision, pages 2425–2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426.\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition.\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 3558–3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581.\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server.arXiv preprint\\narXiv:1504.00325.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision, pages 104–120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International\\nConference on Learning Representations.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning, pages 3887–3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Ma-\\nchine Learning Research, pages 3929–3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 34, pages\\n7879–7886.\\nGautier Izacard and Édouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 874–880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciﬁc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM, 63(7):67–78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128–3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems, 33:9459–9474.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098–1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision,\\npages 121–137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740–755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language\\ntasks. Advances in neural information processing\\nsystems, 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195–3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning, pages 8748–8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1–67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\\nhit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems, 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189.\\nChristoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nﬁltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A\\ncleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n2556–2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning ,\\npages 4596–4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 5317–5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR.\\nPat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nIn Proceedings of NAACL-HLT, pages 3678–3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems, 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 5579–5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nﬁed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non Artiﬁcial Intelligence , volume 34, pages 13041–\\n13049.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to ﬁrst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a ﬁxed sample ratio. We plot\\nthe ﬁrst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we ﬁrst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-\\nuation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEr\\non CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efﬁciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model Conﬁguration\\nWe demonstrate the ViT conﬁguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" n u m _ l a y e r s \" : 24 ,\\n\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conﬁguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13'}, page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ha_HOAdl8hc",
        "outputId": "34cc81c3-5316-4d96-fd79-a24f47741070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U weaviate-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dp7vNbLrxJt",
        "outputId": "55ce19d0-061a-4614-f277-f49e62bb4b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate-client\n",
            "  Downloading weaviate_client-4.18.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (0.28.1)\n",
            "Collecting validators<1.0.0,>=0.34.0 (from weaviate-client)\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.6.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (2.11.10)\n",
            "Requirement already satisfied: grpcio<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.76.0)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (5.29.5)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from weaviate-client)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (43.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client) (25.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<1.80.0,>=1.59.5->weaviate-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.23)\n",
            "Downloading weaviate_client-4.18.0-py3-none-any.whl (597 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.0/598.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: validators, deprecation, weaviate-client\n",
            "Successfully installed deprecation-2.1.0 validators-0.35.0 weaviate-client-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_API_KEY = \"UU02aHp5NFlBMnVHSWRSTV9LakoyNGY1dzJmQzVsaExKYzR5S0VzeVo0TWtxNExyZGZnek5BU0swU3ZZPV92MjAw\"\n",
        "WEAVIATE_CLUSTER = \"hhnfg8g8ruijh30yl7jya.c0.europe-west3.gcp.weaviate.cloud\""
      ],
      "metadata": {
        "id": "1xccL3AoxXvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'weaviate-client>=3.26.7,<4.0.0'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc0AC7nCA2kN",
        "outputId": "3998671a-5b5a-414c-b4f4-8a8301d1026e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate-client<4.0.0,>=3.26.7\n",
            "  Downloading weaviate_client-3.26.7-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from weaviate-client<4.0.0,>=3.26.7) (2.32.5)\n",
            "Requirement already satisfied: validators<1.0.0,>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from weaviate-client<4.0.0,>=3.26.7) (0.35.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from weaviate-client<4.0.0,>=3.26.7) (1.6.5)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.3.1->weaviate-client<4.0.0,>=3.26.7) (43.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->weaviate-client<4.0.0,>=3.26.7) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->weaviate-client<4.0.0,>=3.26.7) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->weaviate-client<4.0.0,>=3.26.7) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->weaviate-client<4.0.0,>=3.26.7) (2025.10.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.3.1->weaviate-client<4.0.0,>=3.26.7) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.3.1->weaviate-client<4.0.0,>=3.26.7) (2.23)\n",
            "Downloading weaviate_client-3.26.7-py3-none-any.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.1/120.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: weaviate-client\n",
            "  Attempting uninstall: weaviate-client\n",
            "    Found existing installation: weaviate-client 4.18.0\n",
            "    Uninstalling weaviate-client-4.18.0:\n",
            "      Successfully uninstalled weaviate-client-4.18.0\n",
            "Successfully installed weaviate-client-3.26.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from weaviate.auth import AuthApiKey\n",
        "\n",
        "WEAVIATE_URL = \"https://\" + WEAVIATE_CLUSTER\n",
        "auth = AuthApiKey(api_key=WEAVIATE_API_KEY)\n",
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_URL,\n",
        "    auth_client_secret=auth\n",
        ")"
      ],
      "metadata": {
        "id": "7DJKCrLqy_GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Uj7F7DWM0OFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# подготовка к векторизации (будем использовать в Weaviate.from_documents)\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)"
      ],
      "metadata": {
        "id": "LCm1WVD51akE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "3c63c161ec504640adce7bb6456ce07b",
            "3652b1e0d8f74c3880caf9d9e0ff4ac6",
            "e350952d965142a39b75b42cb061f792",
            "ec832bd8ee034f0695dcb2021d7c9278",
            "feb62c60abc64fdf9d511641a86772b6",
            "4e9e450bfc7742c4971982ee6c14487d",
            "789b7bf07ed64687ac6e577bebe73937",
            "fb3fa5994afb488f99f19f783ff542ae",
            "9d031315caed4dacb5a5fee94ff7da73",
            "1ec11d7fb1d446e69c48aa95daea69ab",
            "824f404ec0d84a7193cd1be9666fbaa8",
            "65569198d5e049cd810336b3d50fc6dc",
            "a7abef1421ca4210a371bf9031979a4f",
            "587704931fd3408b85acb2ed15f3af78",
            "81178efff367451f807cae878c58b354",
            "92d883ca481c4ef0af603b97ef1599b0",
            "13142dfa5b4342eb9b41221dedce2f70",
            "f12a0e8657ec4f3fa3aa9750635e8ca0",
            "3e0a3149f4ab437ab4e615eedfc52361",
            "c9393342e318474c8e7c9c33c093f814",
            "98e20ab02c6944328b60e4adc545dd80",
            "f797c298b18f47de98d8e722ce3dc3bd",
            "594ac05acdb74516a661a2d72aef60ef",
            "428763fc2a2f481fb670dd0eba635bac",
            "be4991b29a9b47258c133d3d44833029",
            "776b4095a4de4c9b86de984a7ea856b1",
            "92356ba0ac6244a79c1f219a36b6edec",
            "f8d37f2080fa4ee7b929167ab385475a",
            "7a79f7439fb0432ca38af9fa6505bb40",
            "e6b6075324fa4a12a7e94f10c09ead3b",
            "7ab81a07855c456781406ebefef6d758",
            "9caf6c75e9504309b96cf31ea95f7183",
            "6103707b4777415585103d47c9264ba7",
            "27d739cfe817469399e7161fae16b816",
            "d7db72c12c614637be98c0198d6c1e78",
            "6fbff9f1996445ee8d8a293c4970ba5e",
            "c915c1c8c7644ac6b8a489d0e6adcbdb",
            "e9e2e48cb5a44d3a8283e9b7e370fe7e",
            "e951952fc4764a48900ec794bf039d6d",
            "b61defa64ef841989016665e76d021b3",
            "64c638e532c34ba38c38cc91f03d41d6",
            "f66e776a769d446d91acc3586c84a934",
            "337055f2e7b14dc9ba9466af21a8df3d",
            "462353a18b7145058a077940193a0171",
            "d62648797e4340f0a0378d70fba0d6e7",
            "b450a990f5f34b95947b16794afab63a",
            "af14ff8aa1814688a49340e989edead3",
            "fbd2791ed8084434ac2fe783ff1b74bd",
            "756eda1229d54f1381bac09b4a7b4159",
            "2c31b7f1a7164fa295c15cfbc9c9be4a",
            "2d760d849ed546599aa02ae927279fe2",
            "726ee45e96ac457e946066832b5d1673",
            "96a25420cd9e422b88143df294f40204",
            "d494e2be5ae64a7f97449c3378980e9f",
            "9fbbab679fc144bf8ed410b706a1724c",
            "ac3b138194654737b85062eec317ead5",
            "770dc95a3193446eaaaf688bfeb08f97",
            "de9a4b91c3084c7180c30fd4740dd885",
            "4365a2416c784e77889bda1c324b37b8",
            "dde8335f6b6d48bcbdc2305f1d9126d1",
            "8ae18d5a3d484ec5a0a114b9d34ab587",
            "51fb9641cb684979acc2f06588fae3eb",
            "4e66a8e5ef8e4649a75833f218393441",
            "2d46aa583da8471290093f4328375758",
            "f4fa636f8f8a4c11944743a3ae118dc7",
            "b1737aaea82940e5b7ef79f16836fdd2",
            "0ef43210f4e948e4bbfad66da46062cd",
            "9488a5cc84fd48a782753bcd681ab8e6",
            "ad36412ff6804d589c0e742e08079bad",
            "2fc555ae01294bb7a0dc34e7f4465115",
            "65743bfc423b4dba9071a08c67a9d234",
            "a3128c84e5e04a578c65d6175b762f50",
            "5dcf7a0de03a42d893d9e0e6a5295ef3",
            "bdd40699cf1f450aaa8fefe6185c4e88",
            "27cd6a38ee964c0a80f2f8b8233032a4",
            "851cded58f7741528fa3f75086f7b69b",
            "0499bb6b5d1e4d0680bdb73b7299f7a9",
            "8bfacf5b88444db987144d4d80ba211e",
            "0d355e8018e7411bb987595d3282a16e",
            "b873f912f7d946b8b8922897beaf717e",
            "fb7fa4c16f284e029152a0a18d6471c5",
            "9e0ec260461340209471e0fc1db2dc9d",
            "48a7c5d956474ac09605cc080caaab19",
            "a07054577ddc44f180e375a38039160c",
            "85857aa1ac9f4711b22d05f64090aade",
            "acb56058a5b84bab9e73f33866890bcf",
            "76517d9f4124424e9ac830f090936fb6",
            "bc8f6197fdea4337ba6f7f0967acc0be",
            "96dac914ff1f4febb60047c5415bb96b",
            "e57fb762526f46e982af819b4dbfe754",
            "afbfc6b621bb43e69279f2542b63afed",
            "808acd9205384ba8af805cfbe177face",
            "7291cc309e7a46abb5d501eb31f1b5c0",
            "5cd26bc0718f47b499331b836e7cf697",
            "d04247ca7bdd4b50ae7d1ff211b075b8",
            "2ab98e4735844054a97adade5032d0bc",
            "054a6ec2121f48b59e783ac867ff7f9f",
            "86f3ee01e4754741811d2985afaf80aa",
            "fca7817f0b84498189771d67f948416c",
            "ebf520c424bb4a6c9d2212bbf6c6c813",
            "5ea8e4adc7ab4fcda458e81f076ac0b6",
            "a32ef385b157447a88e176df4e18722e",
            "51b66c3d6fb94cdcac951e87ae474710",
            "b59aafc855d140109a3bce64b9a484e5",
            "f80e56706cab44dda3426de4ce49ec40",
            "8c6d1139e257432b91666ae0a04b79f5",
            "d79495f2623f474bb5acbbdf08bfe327",
            "091e87323df44437b85815e384f6f2bc",
            "217e82ba033648d5b94f4a55796f1643",
            "d934dac5094540818895c7fe52df0bab",
            "92599f8eb31a41588f6b7fc2a74b5f00",
            "9a35b78a36d24bc4981374766a81b025",
            "0aadd6e8490941ae9bdf20403d7886e1",
            "a132f90a261c4061bab95edad89c7c9a",
            "1b274a4b65cb41fbba46e2e2561f30ce",
            "14ef4d65267646319f23195ec3d0b5f3",
            "8fc3978e001d49e68447743679f3fe66",
            "e090e21ffa1741e984af4a2035a7ff0a",
            "304e179b09094c47a10ed3ae6007f7b5",
            "4e57f2717fe44f529defa3963dac1e82",
            "2722025bc7064f46b4e3e034d7083ac3"
          ]
        },
        "outputId": "1afb98aa-54da-4c09-a22f-2c9bbfce6342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2871217763.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c63c161ec504640adce7bb6456ce07b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65569198d5e049cd810336b3d50fc6dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "594ac05acdb74516a661a2d72aef60ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27d739cfe817469399e7161fae16b816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d62648797e4340f0a0378d70fba0d6e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac3b138194654737b85062eec317ead5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ef43210f4e948e4bbfad66da46062cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bfacf5b88444db987144d4d80ba211e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96dac914ff1f4febb60047c5415bb96b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebf520c424bb4a6c9d2212bbf6c6c813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92599f8eb31a41588f6b7fc2a74b5f00"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение текста на чанки (chunks)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Создание экземпляра разделителя текста\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # Размер каждого чанка в символах\n",
        "    chunk_overlap=20   # Перекрытие между чанками в символах\n",
        ")\n",
        "\n",
        "# Применение разделителя к документам (pages)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "La4ry4d96k_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjTkNACU8WVv",
        "outputId": "b1c17cfc-c624-4fd2-f7d8-7f2da40a510c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images – much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='ﬁrst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='augmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries: These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model’s parameters. More speciﬁcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model’s predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneﬁcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='of multimodal knowledge available on the web—\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user ﬁrst poses a ques-\\ntion such as “What can be found on the White\\nHouse balconies at Christmas”. The system then\\nretrieves relevant items from its memory, for exam-\\narXiv:2210.02928v2  [cs.CL]  20 Oct 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='ple, the ﬁrst image of Figure 1 with the caption\\n“White House during Christmas”, which it uses to\\nproduce the answer “wreaths and garlands”. Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview: retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciﬁcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nﬁrst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='image-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciﬁcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model’s input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-\\nerative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='the model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during ﬁne-\\ntuning Figure 2 the model’s input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We ﬁne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to ﬁrst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='tractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniﬁed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was ﬁrst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='family of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneﬁt downstream knowledge in-\\ntensive tasks (e.g. Question Answering). REALM\\nis an encoder-only model trained with masked lan-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='features for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the ﬁrst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA’s\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='MIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-\\nmodalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='fθ(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfθ and\\ndecoder gθ. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ∈RLi×D, where Li is the length of the im-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='age tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT ∈RLt×D. For k images and n text inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e = [e1\\nI; e1\\nT; ··· ; ek\\nI; en\\nT] ∈R(kLt+nLi)×D,\\nwhich is fed to another bi-directional transformer\\nfθ initialized from T5. We enable cross-attention'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fθ(e) ∈ R(kLt+nLi)×D.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fθ(e)[CLS] ∈RD for dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query q of any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. Speciﬁcally, we apply the backbone encoder\\nfθ to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m ∈M to ﬁnd the Top-K\\nnearest neighbors TopK(M|q) = [ m1, ··· , mk].\\nFormally, we deﬁne TopK(M|q) as follows:\\nTopK(M|q) = TopK\\nm∈M\\nfθ(q)[CLS] ·fθ(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query q as\\nan augmented input [m1, ··· , mk, q], which is fed\\nto the backbone encoder fθ to produce retrieval-\\naugmented encoding. The decoder model gθ uses'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='attention over this representation to generate tex-\\ntual outputs y = y1, ··· , yn token by token.\\np(yi|yi−1) = gθ(yi|fθ(TopK(M|q); q); y1:i−1)\\nwhere y is decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xI plus a text prompt xp. The exter-\\nnal memory Mcontains textual-only entries mT.\\nThe Top-K retrievalsmT\\n1 , ··· , mT\\nk are leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\nory MB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efﬁciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='Changpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containing\\ncrawled image-text pairs ﬁltered by CLIP (Rad-\\nford et al., 2021). We apply rules to ﬁlter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but ﬁltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='For LAION and CC, we use the input image as\\nxI, and ‘generate caption:’ as the text promptxp.\\nFor VQA, we use the input image as xI and the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MB is con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse ﬁxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L = Lgen +\\nLcon as follows:\\nLcon = −log exp(fθ(xI, xp) ·fθ(mT))∑\\nm∈MB\\nexp(fθ(xI, xp) ·fθ(mT))\\nLgen = −log gθ(y|fθ(Mp; xI; xp))\\nMp =\\n{\\nTopK(MB|xI, xp) If (xI, xp) ∈PAQ/VQA\\nØ If (xI, xp) ∈LAION/CC\\nwhere Mp is the retrieved augmentation: if the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='input query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lcon is minimized to dis-\\ncriminate between the positive query-memory pairs'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\\nimplementation, while the lower part deﬁnes ﬁne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fθ(xI; xp)[CLS] and\\ncandidates fθ(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgen is min-\\nimized to generate target tokens y conditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe ﬁnetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='datastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1 The Top-K retrievals\\n{(mI\\n1, mT\\n1 ), ··· , (mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nﬁxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss functionL = Lcon+Lgen based\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.\\non the in-batch memory MB as follows:\\nLcon = −log exp(fθ(xq) ·fθ(mI; mT))∑\\nm∈MB\\nexp(fθ(xq) ·fθ(mI; mT))\\nLgen = −log gθ(y|fθ(TopK(MB|xq); xq))\\nThe in-batch memory MB is constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk, {mI\\ni, mI\\ni}k, {¯mI\\nj, ¯mT\\nj }k),\\nwhere m represents the positive (image, text)\\nsource, and ¯m represents the hard negative'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='(image, text) source provided by the dataset 2.\\nFor a batch with B examples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB =\\n{{mI\\ni, mI\\ni}1, {¯mI\\nj, ¯mT\\nj }1, ··· , {¯mI\\nj, ¯mT\\nj }B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq) and continue to opti-\\nmize Lgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use ﬁxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conﬁgurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we ﬁrst train the\\nmodel on LAION for 1M steps, and then continue'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='training on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then ﬁne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and ﬁxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn’t yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='modal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.\\nDataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model’s generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nﬂuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='rectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors ﬁrst use the template\\nto generate questions and then ask crowd-workers\\nto ﬁlter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. Speciﬁcally, we choose the\\nquestions with types of ‘TextQ’ and ‘ImageQ’ to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='text and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During ﬁne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nﬁrst to select the most plausible source si, and then\\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\\nto autoregressively decode answer A with masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nﬁrst applies a question type classiﬁer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. Speciﬁcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to ﬁnd'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='a set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input S to the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA’s results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiﬁcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reﬂect the high ﬂuency and accuracy\\nof MuRAG’s generation, and the improvement is\\nmore pronounced for full wiki.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='We show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniﬁcant, with 20+% improvement under both\\nsettings. Similarly, we ﬁnd that our model is more\\ncapable of handling full-wiki corpus.\\nEvaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofﬁcial test-set results indicated\\non leaderboard 3 as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to ﬂuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='Metrics Text Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting 35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their ﬁne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='PAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage ﬁne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different ﬁne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractor MuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-Wiki MuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='the performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the ﬁne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG’s pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model’s perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiﬁcantly behind text accuracy.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='We further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difﬁcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciﬁc region is difﬁcult, 3) the questions require'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='optical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the ﬁrst example, the model is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‘the angel is holding a dead body’. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the ﬁrst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiﬁcantly lower than the performance on'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='queries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufﬁciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus’s format (image -> text) is differ-\\nent from ﬁne-tuning (text -> image+text). This\\nmisalignment limits the model’s performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='formance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic ﬁltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision, pages 2425–2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426.\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='learners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition.\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 3558–3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581.\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server.arXiv preprint\\narXiv:1504.00325.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='arXiv:1504.00325.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision, pages 104–120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='Conference on Learning Representations.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning, pages 3887–3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Ma-\\nchine Learning Research, pages 3929–3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 34, pages\\n7879–7886.\\nGautier Izacard and Édouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='sociation for Computational Linguistics: Main Vol-\\nume, pages 874–880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciﬁc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM, 63(7):67–78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128–3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='ral Information Processing Systems, 33:9459–9474.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098–1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision,\\npages 121–137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740–755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='tasks. Advances in neural information processing\\nsystems, 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195–3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning, pages 8748–8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1–67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='hit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems, 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189.\\nChristoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nﬁltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='cleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n2556–2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning ,\\npages 4596–4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 5317–5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Pat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nIn Proceedings of NAACL-HLT, pages 3678–3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems, 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Pattern Recognition, pages 5579–5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nﬁed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non Artiﬁcial Intelligence , volume 34, pages 13041–\\n13049.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to ﬁrst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a ﬁxed sample ratio. We plot\\nthe ﬁrst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we ﬁrst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='uation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEr\\non CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efﬁciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model Conﬁguration\\nWe demonstrate the ViT conﬁguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" n u m _ l a y e r s \" : 24 ,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conﬁguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13'}, page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ругалось на формат ptex_fullbanner, скорее всего если документов будет много будет плохо(долго) работать\n",
        "for doc in docs:\n",
        "    if 'ptex.fullbanner' in doc.metadata:\n",
        "        doc.metadata['ptex_fullbanner'] = doc.metadata.pop('ptex.fullbanner')\n"
      ],
      "metadata": {
        "id": "zPQj4rEaFxK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Измерим метрики после сохранения в Weaviate и выберем оптимальные размеры чанков(точность поиска (precision/recall), время запросов, релевантность ответов.)\n",
        "  #from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "  # Тестируем разные размеры\n",
        "  #sizes = [200, 500, 1000]\n",
        "  #for size in sizes:\n",
        "      #text_splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=int(size*0.1))\n",
        "      #docs = text_splitter.split_documents(pages)\n",
        "      # Далее: векторизация и оценка\n",
        "      #print(f\"Размер {size}: {len(docs)} чанков\")"
      ],
      "metadata": {
        "id": "tcYoc6Lw8tj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание экземпляра векторного хранилища на основе наших документов\n",
        "from langchain.vectorstores import Weaviate\n",
        "vector_db = Weaviate.from_documents(\n",
        "    docs, embeddings, client=client, by_text=False\n",
        ")"
      ],
      "metadata": {
        "id": "_RfM_5ur9rHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.similarity_search(\"what is rag?\", k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpvoh4Py-Kto",
        "outputId": "bd6f96b7-0863-4839-b651-c3e6fdd78ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='ral Information Processing Systems, 33:9459–9474.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='fθ(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfθ and\\ndecoder gθ. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ∈RLi×D, where Li is the length of the im-'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "D1foB-U4YquA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "lR3WSxSeYs2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-mistralai"
      ],
      "metadata": {
        "id": "-b4-DA1MNgOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI"
      ],
      "metadata": {
        "id": "0tz97fh-NTjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "DPLi1j50OWK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatMistralAI(\n",
        "    api_key=api_key,\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n"
      ],
      "metadata": {
        "id": "sisff8WWNq54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser() #для чистого вывода ответа\n",
        "retriever = vector_db.as_retriever() # поисковик на основе наших документов"
      ],
      "metadata": {
        "id": "gv8cKkH4P_Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": vector_db.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "7CxFQi-zPP6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"what is Multimodal RAG?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "Dx6fNkf7P1M7",
        "outputId": "3be95509-8cca-42ca-bb87-9d61fb903537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Multimodal RAG (MuRAG)** is a retrieval-augmented generation model that extends traditional RAG by incorporating **both visual and textual knowledge** (e.g., images and text) to enhance language generation. Unlike prior text-only RAG methods, MuRAG uses a **multimodal encoder-decoder architecture** (e.g., combining Vision Transformers for images and T5 for text) to retrieve and encode cross-modal information.\\n\\nIt leverages **pre-trained multimodal transformers** to fuse features from images and text, enabling tasks like multimodal question answering (e.g., VQA) where answers require reasoning over both modalities. The model’s backbone encodes image-text pairs for retrieval and generation, addressing limitations of unimodal systems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"Retell the summary of the article about multimodul RAG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "MSf3YUxNT2k9",
        "outputId": "67669485-93ad-4138-f48a-942d823b7b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The article introduces **MuRAG (Multimodal Retrieval-Augmented Generation)**, the first retrieval-augmented model capable of leveraging **both visual and textual knowledge**, unlike prior text-only models like RAG or FiD.\\n\\nMuRAG outperforms baselines (e.g., AutoRouting) in multimodal QA tasks, achieving higher **EM (Exact Match) and F1 scores** across text, image, and combined modalities (e.g., 60.8 EM for text vs. 15.4 for question-only baselines). It uses **multimodal transformers** to fuse deep features from vision and language, enabling cross-modal reasoning.\\n\\nAblation studies show that **pre-training on datasets like LAION** significantly boosts performance, while evaluations highlight strong results in tasks like **VQA (72% accuracy)**, image-text retrieval (85% RECALL@1), and text-based reading comprehension (>55% EM). The model demonstrates **efficiency in multi-task learning** and scalable retrieval from multimodal memory.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What is Multimodal RAG?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwD5SNmkT2ED",
        "outputId": "76218642-5365-4c3d-88cc-9783a5bba7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='features for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the ﬁrst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA’s\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='fθ(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfθ and\\ndecoder gθ. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ∈RLi×D, where Li is the length of the im-'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 7, 'page_label': '8', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/content/drive/MyDrive/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='optical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the ﬁrst example, the model is')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y tesseract-ocr\n",
        "!apt install -y libtesseract-dev\n",
        "!apt install -y tesseract-ocr-all\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvc2LsHEJFfc",
        "outputId": "3e75bbe1-7f55-4b1d-c405-a9d012432af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.5 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 0s (20.7 MB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 121229 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-afr tesseract-ocr-amh tesseract-ocr-ara tesseract-ocr-asm\n",
            "  tesseract-ocr-aze tesseract-ocr-aze-cyrl tesseract-ocr-bel tesseract-ocr-ben\n",
            "  tesseract-ocr-bod tesseract-ocr-bos tesseract-ocr-bre tesseract-ocr-bul\n",
            "  tesseract-ocr-cat tesseract-ocr-ceb tesseract-ocr-ces tesseract-ocr-chi-sim\n",
            "  tesseract-ocr-chi-sim-vert tesseract-ocr-chi-tra tesseract-ocr-chi-tra-vert\n",
            "  tesseract-ocr-chr tesseract-ocr-cos tesseract-ocr-cym tesseract-ocr-dan\n",
            "  tesseract-ocr-deu tesseract-ocr-div tesseract-ocr-dzo tesseract-ocr-ell\n",
            "  tesseract-ocr-enm tesseract-ocr-epo tesseract-ocr-est tesseract-ocr-eus\n",
            "  tesseract-ocr-fao tesseract-ocr-fas tesseract-ocr-fil tesseract-ocr-fin\n",
            "  tesseract-ocr-fra tesseract-ocr-frk tesseract-ocr-frm tesseract-ocr-fry\n",
            "  tesseract-ocr-gla tesseract-ocr-gle tesseract-ocr-glg tesseract-ocr-grc\n",
            "  tesseract-ocr-guj tesseract-ocr-hat tesseract-ocr-heb tesseract-ocr-hin\n",
            "  tesseract-ocr-hrv tesseract-ocr-hun tesseract-ocr-hye tesseract-ocr-iku\n",
            "  tesseract-ocr-ind tesseract-ocr-isl tesseract-ocr-ita tesseract-ocr-ita-old\n",
            "  tesseract-ocr-jav tesseract-ocr-jpn tesseract-ocr-jpn-vert tesseract-ocr-kan\n",
            "  tesseract-ocr-kat tesseract-ocr-kat-old tesseract-ocr-kaz tesseract-ocr-khm\n",
            "  tesseract-ocr-kir tesseract-ocr-kmr tesseract-ocr-kor tesseract-ocr-kor-vert\n",
            "  tesseract-ocr-lao tesseract-ocr-lat tesseract-ocr-lav tesseract-ocr-lit\n",
            "  tesseract-ocr-ltz tesseract-ocr-mal tesseract-ocr-mar tesseract-ocr-mkd\n",
            "  tesseract-ocr-mlt tesseract-ocr-mon tesseract-ocr-mri tesseract-ocr-msa\n",
            "  tesseract-ocr-mya tesseract-ocr-nep tesseract-ocr-nld tesseract-ocr-nor\n",
            "  tesseract-ocr-oci tesseract-ocr-ori tesseract-ocr-pan tesseract-ocr-pol\n",
            "  tesseract-ocr-por tesseract-ocr-pus tesseract-ocr-que tesseract-ocr-ron\n",
            "  tesseract-ocr-rus tesseract-ocr-san tesseract-ocr-script-arab\n",
            "  tesseract-ocr-script-armn tesseract-ocr-script-beng\n",
            "  tesseract-ocr-script-cans tesseract-ocr-script-cher\n",
            "  tesseract-ocr-script-cyrl tesseract-ocr-script-deva\n",
            "  tesseract-ocr-script-ethi tesseract-ocr-script-frak\n",
            "  tesseract-ocr-script-geor tesseract-ocr-script-grek\n",
            "  tesseract-ocr-script-gujr tesseract-ocr-script-guru\n",
            "  tesseract-ocr-script-hang tesseract-ocr-script-hang-vert\n",
            "  tesseract-ocr-script-hans tesseract-ocr-script-hans-vert\n",
            "  tesseract-ocr-script-hant tesseract-ocr-script-hant-vert\n",
            "  tesseract-ocr-script-hebr tesseract-ocr-script-jpan\n",
            "  tesseract-ocr-script-jpan-vert tesseract-ocr-script-khmr\n",
            "  tesseract-ocr-script-knda tesseract-ocr-script-laoo\n",
            "  tesseract-ocr-script-latn tesseract-ocr-script-mlym\n",
            "  tesseract-ocr-script-mymr tesseract-ocr-script-orya\n",
            "  tesseract-ocr-script-sinh tesseract-ocr-script-syrc\n",
            "  tesseract-ocr-script-taml tesseract-ocr-script-telu\n",
            "  tesseract-ocr-script-thaa tesseract-ocr-script-thai\n",
            "  tesseract-ocr-script-tibt tesseract-ocr-script-viet tesseract-ocr-sin\n",
            "  tesseract-ocr-slk tesseract-ocr-slv tesseract-ocr-snd tesseract-ocr-spa\n",
            "  tesseract-ocr-spa-old tesseract-ocr-sqi tesseract-ocr-srp\n",
            "  tesseract-ocr-srp-latn tesseract-ocr-sun tesseract-ocr-swa tesseract-ocr-swe\n",
            "  tesseract-ocr-syr tesseract-ocr-tam tesseract-ocr-tat tesseract-ocr-tel\n",
            "  tesseract-ocr-tgk tesseract-ocr-tha tesseract-ocr-tir tesseract-ocr-ton\n",
            "  tesseract-ocr-tur tesseract-ocr-uig tesseract-ocr-ukr tesseract-ocr-urd\n",
            "  tesseract-ocr-uzb tesseract-ocr-uzb-cyrl tesseract-ocr-vie tesseract-ocr-yid\n",
            "  tesseract-ocr-yor\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-afr tesseract-ocr-all tesseract-ocr-amh tesseract-ocr-ara\n",
            "  tesseract-ocr-asm tesseract-ocr-aze tesseract-ocr-aze-cyrl tesseract-ocr-bel\n",
            "  tesseract-ocr-ben tesseract-ocr-bod tesseract-ocr-bos tesseract-ocr-bre\n",
            "  tesseract-ocr-bul tesseract-ocr-cat tesseract-ocr-ceb tesseract-ocr-ces\n",
            "  tesseract-ocr-chi-sim tesseract-ocr-chi-sim-vert tesseract-ocr-chi-tra\n",
            "  tesseract-ocr-chi-tra-vert tesseract-ocr-chr tesseract-ocr-cos\n",
            "  tesseract-ocr-cym tesseract-ocr-dan tesseract-ocr-deu tesseract-ocr-div\n",
            "  tesseract-ocr-dzo tesseract-ocr-ell tesseract-ocr-enm tesseract-ocr-epo\n",
            "  tesseract-ocr-est tesseract-ocr-eus tesseract-ocr-fao tesseract-ocr-fas\n",
            "  tesseract-ocr-fil tesseract-ocr-fin tesseract-ocr-fra tesseract-ocr-frk\n",
            "  tesseract-ocr-frm tesseract-ocr-fry tesseract-ocr-gla tesseract-ocr-gle\n",
            "  tesseract-ocr-glg tesseract-ocr-grc tesseract-ocr-guj tesseract-ocr-hat\n",
            "  tesseract-ocr-heb tesseract-ocr-hin tesseract-ocr-hrv tesseract-ocr-hun\n",
            "  tesseract-ocr-hye tesseract-ocr-iku tesseract-ocr-ind tesseract-ocr-isl\n",
            "  tesseract-ocr-ita tesseract-ocr-ita-old tesseract-ocr-jav tesseract-ocr-jpn\n",
            "  tesseract-ocr-jpn-vert tesseract-ocr-kan tesseract-ocr-kat\n",
            "  tesseract-ocr-kat-old tesseract-ocr-kaz tesseract-ocr-khm tesseract-ocr-kir\n",
            "  tesseract-ocr-kmr tesseract-ocr-kor tesseract-ocr-kor-vert tesseract-ocr-lao\n",
            "  tesseract-ocr-lat tesseract-ocr-lav tesseract-ocr-lit tesseract-ocr-ltz\n",
            "  tesseract-ocr-mal tesseract-ocr-mar tesseract-ocr-mkd tesseract-ocr-mlt\n",
            "  tesseract-ocr-mon tesseract-ocr-mri tesseract-ocr-msa tesseract-ocr-mya\n",
            "  tesseract-ocr-nep tesseract-ocr-nld tesseract-ocr-nor tesseract-ocr-oci\n",
            "  tesseract-ocr-ori tesseract-ocr-pan tesseract-ocr-pol tesseract-ocr-por\n",
            "  tesseract-ocr-pus tesseract-ocr-que tesseract-ocr-ron tesseract-ocr-rus\n",
            "  tesseract-ocr-san tesseract-ocr-script-arab tesseract-ocr-script-armn\n",
            "  tesseract-ocr-script-beng tesseract-ocr-script-cans\n",
            "  tesseract-ocr-script-cher tesseract-ocr-script-cyrl\n",
            "  tesseract-ocr-script-deva tesseract-ocr-script-ethi\n",
            "  tesseract-ocr-script-frak tesseract-ocr-script-geor\n",
            "  tesseract-ocr-script-grek tesseract-ocr-script-gujr\n",
            "  tesseract-ocr-script-guru tesseract-ocr-script-hang\n",
            "  tesseract-ocr-script-hang-vert tesseract-ocr-script-hans\n",
            "  tesseract-ocr-script-hans-vert tesseract-ocr-script-hant\n",
            "  tesseract-ocr-script-hant-vert tesseract-ocr-script-hebr\n",
            "  tesseract-ocr-script-jpan tesseract-ocr-script-jpan-vert\n",
            "  tesseract-ocr-script-khmr tesseract-ocr-script-knda\n",
            "  tesseract-ocr-script-laoo tesseract-ocr-script-latn\n",
            "  tesseract-ocr-script-mlym tesseract-ocr-script-mymr\n",
            "  tesseract-ocr-script-orya tesseract-ocr-script-sinh\n",
            "  tesseract-ocr-script-syrc tesseract-ocr-script-taml\n",
            "  tesseract-ocr-script-telu tesseract-ocr-script-thaa\n",
            "  tesseract-ocr-script-thai tesseract-ocr-script-tibt\n",
            "  tesseract-ocr-script-viet tesseract-ocr-sin tesseract-ocr-slk\n",
            "  tesseract-ocr-slv tesseract-ocr-snd tesseract-ocr-spa tesseract-ocr-spa-old\n",
            "  tesseract-ocr-sqi tesseract-ocr-srp tesseract-ocr-srp-latn tesseract-ocr-sun\n",
            "  tesseract-ocr-swa tesseract-ocr-swe tesseract-ocr-syr tesseract-ocr-tam\n",
            "  tesseract-ocr-tat tesseract-ocr-tel tesseract-ocr-tgk tesseract-ocr-tha\n",
            "  tesseract-ocr-tir tesseract-ocr-ton tesseract-ocr-tur tesseract-ocr-uig\n",
            "  tesseract-ocr-ukr tesseract-ocr-urd tesseract-ocr-uzb tesseract-ocr-uzb-cyrl\n",
            "  tesseract-ocr-vie tesseract-ocr-yid tesseract-ocr-yor\n",
            "0 upgraded, 160 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 281 MB of archives.\n",
            "After this operation, 686 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-afr all 1:4.00~git30-7274cfa-1.1 [1,731 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bul all 1:4.00~git30-7274cfa-1.1 [678 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-cat all 1:4.00~git30-7274cfa-1.1 [579 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ces all 1:4.00~git30-7274cfa-1.1 [1,408 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-dan all 1:4.00~git30-7274cfa-1.1 [1,026 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-deu all 1:4.00~git30-7274cfa-1.1 [744 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ell all 1:4.00~git30-7274cfa-1.1 [594 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fin all 1:4.00~git30-7274cfa-1.1 [3,030 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fra all 1:4.00~git30-7274cfa-1.1 [527 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hun all 1:4.00~git30-7274cfa-1.1 [1,853 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ind all 1:4.00~git30-7274cfa-1.1 [537 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ita all 1:4.00~git30-7274cfa-1.1 [1,067 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lav all 1:4.00~git30-7274cfa-1.1 [981 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lit all 1:4.00~git30-7274cfa-1.1 [1,140 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-nld all 1:4.00~git30-7274cfa-1.1 [2,306 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-nor all 1:4.00~git30-7274cfa-1.1 [1,748 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pol all 1:4.00~git30-7274cfa-1.1 [1,610 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-por all 1:4.00~git30-7274cfa-1.1 [856 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ron all 1:4.00~git30-7274cfa-1.1 [896 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-rus all 1:4.00~git30-7274cfa-1.1 [1,271 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-slk all 1:4.00~git30-7274cfa-1.1 [1,516 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-slv all 1:4.00~git30-7274cfa-1.1 [998 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-spa all 1:4.00~git30-7274cfa-1.1 [951 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-srp all 1:4.00~git30-7274cfa-1.1 [780 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-swe all 1:4.00~git30-7274cfa-1.1 [2,232 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tur all 1:4.00~git30-7274cfa-1.1 [1,578 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ukr all 1:4.00~git30-7274cfa-1.1 [1,303 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-vie all 1:4.00~git30-7274cfa-1.1 [417 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-sim all 1:4.00~git30-7274cfa-1.1 [1,634 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-tra all 1:4.00~git30-7274cfa-1.1 [1,586 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-amh all 1:4.00~git30-7274cfa-1.1 [1,856 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-asm all 1:4.00~git30-7274cfa-1.1 [1,421 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-aze-cyrl all 1:4.00~git30-7274cfa-1.1 [852 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bod all 1:4.00~git30-7274cfa-1.1 [1,180 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bos all 1:4.00~git30-7274cfa-1.1 [965 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ceb all 1:4.00~git30-7274cfa-1.1 [469 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-cym all 1:4.00~git30-7274cfa-1.1 [1,260 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-dzo all 1:4.00~git30-7274cfa-1.1 [390 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fas all 1:4.00~git30-7274cfa-1.1 [301 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-gle all 1:4.00~git30-7274cfa-1.1 [613 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-guj all 1:4.00~git30-7274cfa-1.1 [660 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hat all 1:4.00~git30-7274cfa-1.1 [1,487 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-iku all 1:4.00~git30-7274cfa-1.1 [1,211 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-jav all 1:4.00~git30-7274cfa-1.1 [1,443 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kat all 1:4.00~git30-7274cfa-1.1 [884 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kat-old all 1:4.00~git30-7274cfa-1.1 [380 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kaz all 1:4.00~git30-7274cfa-1.1 [1,687 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-khm all 1:4.00~git30-7274cfa-1.1 [1,032 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kir all 1:4.00~git30-7274cfa-1.1 [3,277 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lao all 1:4.00~git30-7274cfa-1.1 [2,411 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lat all 1:4.00~git30-7274cfa-1.1 [1,536 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mar all 1:4.00~git30-7274cfa-1.1 [862 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mya all 1:4.00~git30-7274cfa-1.1 [2,358 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-nep all 1:4.00~git30-7274cfa-1.1 [478 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ori all 1:4.00~git30-7274cfa-1.1 [1,024 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pan all 1:4.00~git30-7274cfa-1.1 [322 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pus all 1:4.00~git30-7274cfa-1.1 [1,415 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-san all 1:4.00~git30-7274cfa-1.1 [4,228 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sin all 1:4.00~git30-7274cfa-1.1 [1,085 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-srp-latn all 1:4.00~git30-7274cfa-1.1 [1,558 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-syr all 1:4.00~git30-7274cfa-1.1 [1,558 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tgk all 1:4.00~git30-7274cfa-1.1 [949 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tir all 1:4.00~git30-7274cfa-1.1 [297 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-uig all 1:4.00~git30-7274cfa-1.1 [1,743 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-urd all 1:4.00~git30-7274cfa-1.1 [1,000 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-uzb-cyrl all 1:4.00~git30-7274cfa-1.1 [730 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-uzb all 1:4.00~git30-7274cfa-1.1 [2,535 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-yid all 1:4.00~git30-7274cfa-1.1 [345 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ara all 1:4.00~git30-7274cfa-1.1 [645 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-aze all 1:4.00~git30-7274cfa-1.1 [1,342 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bel all 1:4.00~git30-7274cfa-1.1 [1,190 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ben all 1:4.00~git30-7274cfa-1.1 [516 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chr all 1:4.00~git30-7274cfa-1.1 [287 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-enm all 1:4.00~git30-7274cfa-1.1 [1,845 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-epo all 1:4.00~git30-7274cfa-1.1 [1,709 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-est all 1:4.00~git30-7274cfa-1.1 [1,586 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eus all 1:4.00~git30-7274cfa-1.1 [1,765 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-frk all 1:4.00~git30-7274cfa-1.1 [2,730 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-frm all 1:4.00~git30-7274cfa-1.1 [830 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-glg all 1:4.00~git30-7274cfa-1.1 [1,656 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-heb all 1:4.00~git30-7274cfa-1.1 [432 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hin all 1:4.00~git30-7274cfa-1.1 [913 kB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hrv all 1:4.00~git30-7274cfa-1.1 [1,439 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-isl all 1:4.00~git30-7274cfa-1.1 [913 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ita-old all 1:4.00~git30-7274cfa-1.1 [1,567 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-jpn all 1:4.00~git30-7274cfa-1.1 [1,390 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kan all 1:4.00~git30-7274cfa-1.1 [1,659 kB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kor all 1:4.00~git30-7274cfa-1.1 [1,052 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mal all 1:4.00~git30-7274cfa-1.1 [1,678 kB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mkd all 1:4.00~git30-7274cfa-1.1 [718 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mlt all 1:4.00~git30-7274cfa-1.1 [975 kB]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-msa all 1:4.00~git30-7274cfa-1.1 [1,114 kB]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-spa-old all 1:4.00~git30-7274cfa-1.1 [1,474 kB]\n",
            "Get:94 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sqi all 1:4.00~git30-7274cfa-1.1 [720 kB]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-swa all 1:4.00~git30-7274cfa-1.1 [919 kB]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tam all 1:4.00~git30-7274cfa-1.1 [1,071 kB]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tel all 1:4.00~git30-7274cfa-1.1 [1,012 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tha all 1:4.00~git30-7274cfa-1.1 [899 kB]\n",
            "Get:99 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bre all 1:4.00~git30-7274cfa-1.1 [2,861 kB]\n",
            "Get:100 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-sim-vert all 1:4.00~git30-7274cfa-1.1 [1,134 kB]\n",
            "Get:101 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-tra-vert all 1:4.00~git30-7274cfa-1.1 [1,091 kB]\n",
            "Get:102 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-cos all 1:4.00~git30-7274cfa-1.1 [1,263 kB]\n",
            "Get:103 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-div all 1:4.00~git30-7274cfa-1.1 [762 kB]\n",
            "Get:104 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fao all 1:4.00~git30-7274cfa-1.1 [1,667 kB]\n",
            "Get:105 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fil all 1:4.00~git30-7274cfa-1.1 [760 kB]\n",
            "Get:106 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fry all 1:4.00~git30-7274cfa-1.1 [1,177 kB]\n",
            "Get:107 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-gla all 1:4.00~git30-7274cfa-1.1 [1,541 kB]\n",
            "Get:108 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hye all 1:4.00~git30-7274cfa-1.1 [1,191 kB]\n",
            "Get:109 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-jpn-vert all 1:4.00~git30-7274cfa-1.1 [1,889 kB]\n",
            "Get:110 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kor-vert all 1:4.00~git30-7274cfa-1.1 [546 kB]\n",
            "Get:111 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kmr all 1:4.00~git30-7274cfa-1.1 [1,666 kB]\n",
            "Get:112 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ltz all 1:4.00~git30-7274cfa-1.1 [1,720 kB]\n",
            "Get:113 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mon all 1:4.00~git30-7274cfa-1.1 [1,216 kB]\n",
            "Get:114 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mri all 1:4.00~git30-7274cfa-1.1 [514 kB]\n",
            "Get:115 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-oci all 1:4.00~git30-7274cfa-1.1 [2,596 kB]\n",
            "Get:116 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-que all 1:4.00~git30-7274cfa-1.1 [2,136 kB]\n",
            "Get:117 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-snd all 1:4.00~git30-7274cfa-1.1 [1,402 kB]\n",
            "Get:118 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sun all 1:4.00~git30-7274cfa-1.1 [679 kB]\n",
            "Get:119 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tat all 1:4.00~git30-7274cfa-1.1 [902 kB]\n",
            "Get:120 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ton all 1:4.00~git30-7274cfa-1.1 [547 kB]\n",
            "Get:121 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-yor all 1:4.00~git30-7274cfa-1.1 [551 kB]\n",
            "Get:122 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-arab all 1:4.00~git30-7274cfa-1.1 [3,214 kB]\n",
            "Get:123 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-armn all 1:4.00~git30-7274cfa-1.1 [3,094 kB]\n",
            "Get:124 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-beng all 1:4.00~git30-7274cfa-1.1 [2,447 kB]\n",
            "Get:125 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-cans all 1:4.00~git30-7274cfa-1.1 [3,101 kB]\n",
            "Get:126 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-cher all 1:4.00~git30-7274cfa-1.1 [1,640 kB]\n",
            "Get:127 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-cyrl all 1:4.00~git30-7274cfa-1.1 [9,350 kB]\n",
            "Get:128 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-deva all 1:4.00~git30-7274cfa-1.1 [6,119 kB]\n",
            "Get:129 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-ethi all 1:4.00~git30-7274cfa-1.1 [3,034 kB]\n",
            "Get:130 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-frak all 1:4.00~git30-7274cfa-1.1 [4,148 kB]\n",
            "Get:131 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-geor all 1:4.00~git30-7274cfa-1.1 [2,828 kB]\n",
            "Get:132 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-grek all 1:4.00~git30-7274cfa-1.1 [1,076 kB]\n",
            "Get:133 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-gujr all 1:4.00~git30-7274cfa-1.1 [1,809 kB]\n",
            "Get:134 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-guru all 1:4.00~git30-7274cfa-1.1 [1,556 kB]\n",
            "Get:135 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hans all 1:4.00~git30-7274cfa-1.1 [2,860 kB]\n",
            "Get:136 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hans-vert all 1:4.00~git30-7274cfa-1.1 [2,367 kB]\n",
            "Get:137 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hant all 1:4.00~git30-7274cfa-1.1 [2,362 kB]\n",
            "Get:138 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hant-vert all 1:4.00~git30-7274cfa-1.1 [2,352 kB]\n",
            "Get:139 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hang all 1:4.00~git30-7274cfa-1.1 [1,853 kB]\n",
            "Get:140 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hang-vert all 1:4.00~git30-7274cfa-1.1 [1,753 kB]\n",
            "Get:141 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hebr all 1:4.00~git30-7274cfa-1.1 [1,743 kB]\n",
            "Get:142 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-jpan all 1:4.00~git30-7274cfa-1.1 [2,585 kB]\n",
            "Get:143 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-jpan-vert all 1:4.00~git30-7274cfa-1.1 [3,084 kB]\n",
            "Get:144 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-knda all 1:4.00~git30-7274cfa-1.1 [2,474 kB]\n",
            "Get:145 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-khmr all 1:4.00~git30-7274cfa-1.1 [1,663 kB]\n",
            "Get:146 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-laoo all 1:4.00~git30-7274cfa-1.1 [3,664 kB]\n",
            "Get:147 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-latn all 1:4.00~git30-7274cfa-1.1 [30.9 MB]\n",
            "Get:148 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-mlym all 1:4.00~git30-7274cfa-1.1 [2,981 kB]\n",
            "Get:149 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-mymr all 1:4.00~git30-7274cfa-1.1 [3,191 kB]\n",
            "Get:150 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-orya all 1:4.00~git30-7274cfa-1.1 [2,768 kB]\n",
            "Get:151 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-sinh all 1:4.00~git30-7274cfa-1.1 [1,753 kB]\n",
            "Get:152 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-syrc all 1:4.00~git30-7274cfa-1.1 [2,698 kB]\n",
            "Get:153 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-taml all 1:4.00~git30-7274cfa-1.1 [2,497 kB]\n",
            "Get:154 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-telu all 1:4.00~git30-7274cfa-1.1 [2,319 kB]\n",
            "Get:155 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-thaa all 1:4.00~git30-7274cfa-1.1 [2,518 kB]\n",
            "Get:156 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-thai all 1:4.00~git30-7274cfa-1.1 [1,642 kB]\n",
            "Get:157 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-tibt all 1:4.00~git30-7274cfa-1.1 [2,453 kB]\n",
            "Get:158 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-viet all 1:4.00~git30-7274cfa-1.1 [1,430 kB]\n",
            "Get:159 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-grc all 1:4.00~git30-7274cfa-1.1 [916 kB]\n",
            "Get:160 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-all all 4.1.1-2.1build1 [2,040 B]\n",
            "Fetched 281 MB in 4s (65.8 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package tesseract-ocr-afr.\n",
            "(Reading database ... 121362 files and directories currently installed.)\n",
            "Preparing to unpack .../000-tesseract-ocr-afr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-afr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bul.\n",
            "Preparing to unpack .../001-tesseract-ocr-bul_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bul (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-cat.\n",
            "Preparing to unpack .../002-tesseract-ocr-cat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-cat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ces.\n",
            "Preparing to unpack .../003-tesseract-ocr-ces_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ces (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-dan.\n",
            "Preparing to unpack .../004-tesseract-ocr-dan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-dan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-deu.\n",
            "Preparing to unpack .../005-tesseract-ocr-deu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ell.\n",
            "Preparing to unpack .../006-tesseract-ocr-ell_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ell (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fin.\n",
            "Preparing to unpack .../007-tesseract-ocr-fin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fra.\n",
            "Preparing to unpack .../008-tesseract-ocr-fra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hun.\n",
            "Preparing to unpack .../009-tesseract-ocr-hun_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ind.\n",
            "Preparing to unpack .../010-tesseract-ocr-ind_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ind (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ita.\n",
            "Preparing to unpack .../011-tesseract-ocr-ita_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ita (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lav.\n",
            "Preparing to unpack .../012-tesseract-ocr-lav_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lit.\n",
            "Preparing to unpack .../013-tesseract-ocr-lit_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lit (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-nld.\n",
            "Preparing to unpack .../014-tesseract-ocr-nld_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-nld (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-nor.\n",
            "Preparing to unpack .../015-tesseract-ocr-nor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-nor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-pol.\n",
            "Preparing to unpack .../016-tesseract-ocr-pol_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-pol (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-por.\n",
            "Preparing to unpack .../017-tesseract-ocr-por_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ron.\n",
            "Preparing to unpack .../018-tesseract-ocr-ron_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ron (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-rus.\n",
            "Preparing to unpack .../019-tesseract-ocr-rus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-slk.\n",
            "Preparing to unpack .../020-tesseract-ocr-slk_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-slk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-slv.\n",
            "Preparing to unpack .../021-tesseract-ocr-slv_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-slv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-spa.\n",
            "Preparing to unpack .../022-tesseract-ocr-spa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-srp.\n",
            "Preparing to unpack .../023-tesseract-ocr-srp_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-srp (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-swe.\n",
            "Preparing to unpack .../024-tesseract-ocr-swe_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-swe (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tur.\n",
            "Preparing to unpack .../025-tesseract-ocr-tur_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tur (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ukr.\n",
            "Preparing to unpack .../026-tesseract-ocr-ukr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ukr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-vie.\n",
            "Preparing to unpack .../027-tesseract-ocr-vie_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-vie (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-sim.\n",
            "Preparing to unpack .../028-tesseract-ocr-chi-sim_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-tra.\n",
            "Preparing to unpack .../029-tesseract-ocr-chi-tra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-amh.\n",
            "Preparing to unpack .../030-tesseract-ocr-amh_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-amh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-asm.\n",
            "Preparing to unpack .../031-tesseract-ocr-asm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-aze-cyrl.\n",
            "Preparing to unpack .../032-tesseract-ocr-aze-cyrl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-aze-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bod.\n",
            "Preparing to unpack .../033-tesseract-ocr-bod_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bod (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bos.\n",
            "Preparing to unpack .../034-tesseract-ocr-bos_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ceb.\n",
            "Preparing to unpack .../035-tesseract-ocr-ceb_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ceb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-cym.\n",
            "Preparing to unpack .../036-tesseract-ocr-cym_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-cym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-dzo.\n",
            "Preparing to unpack .../037-tesseract-ocr-dzo_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-dzo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fas.\n",
            "Preparing to unpack .../038-tesseract-ocr-fas_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fas (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-gle.\n",
            "Preparing to unpack .../039-tesseract-ocr-gle_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-gle (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-guj.\n",
            "Preparing to unpack .../040-tesseract-ocr-guj_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-guj (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hat.\n",
            "Preparing to unpack .../041-tesseract-ocr-hat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-iku.\n",
            "Preparing to unpack .../042-tesseract-ocr-iku_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-iku (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-jav.\n",
            "Preparing to unpack .../043-tesseract-ocr-jav_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-jav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kat.\n",
            "Preparing to unpack .../044-tesseract-ocr-kat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kat-old.\n",
            "Preparing to unpack .../045-tesseract-ocr-kat-old_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kat-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kaz.\n",
            "Preparing to unpack .../046-tesseract-ocr-kaz_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kaz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-khm.\n",
            "Preparing to unpack .../047-tesseract-ocr-khm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-khm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kir.\n",
            "Preparing to unpack .../048-tesseract-ocr-kir_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lao.\n",
            "Preparing to unpack .../049-tesseract-ocr-lao_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lat.\n",
            "Preparing to unpack .../050-tesseract-ocr-lat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mar.\n",
            "Preparing to unpack .../051-tesseract-ocr-mar_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mya.\n",
            "Preparing to unpack .../052-tesseract-ocr-mya_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-nep.\n",
            "Preparing to unpack .../053-tesseract-ocr-nep_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-nep (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ori.\n",
            "Preparing to unpack .../054-tesseract-ocr-ori_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-pan.\n",
            "Preparing to unpack .../055-tesseract-ocr-pan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-pus.\n",
            "Preparing to unpack .../056-tesseract-ocr-pus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-pus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-san.\n",
            "Preparing to unpack .../057-tesseract-ocr-san_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-san (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-sin.\n",
            "Preparing to unpack .../058-tesseract-ocr-sin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-srp-latn.\n",
            "Preparing to unpack .../059-tesseract-ocr-srp-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-srp-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-syr.\n",
            "Preparing to unpack .../060-tesseract-ocr-syr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-syr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tgk.\n",
            "Preparing to unpack .../061-tesseract-ocr-tgk_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tgk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tir.\n",
            "Preparing to unpack .../062-tesseract-ocr-tir_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-uig.\n",
            "Preparing to unpack .../063-tesseract-ocr-uig_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-uig (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-urd.\n",
            "Preparing to unpack .../064-tesseract-ocr-urd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-uzb-cyrl.\n",
            "Preparing to unpack .../065-tesseract-ocr-uzb-cyrl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-uzb-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-uzb.\n",
            "Preparing to unpack .../066-tesseract-ocr-uzb_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-uzb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-yid.\n",
            "Preparing to unpack .../067-tesseract-ocr-yid_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-yid (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ara.\n",
            "Preparing to unpack .../068-tesseract-ocr-ara_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-aze.\n",
            "Preparing to unpack .../069-tesseract-ocr-aze_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-aze (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bel.\n",
            "Preparing to unpack .../070-tesseract-ocr-bel_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ben.\n",
            "Preparing to unpack .../071-tesseract-ocr-ben_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chr.\n",
            "Preparing to unpack .../072-tesseract-ocr-chr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-enm.\n",
            "Preparing to unpack .../073-tesseract-ocr-enm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-enm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-epo.\n",
            "Preparing to unpack .../074-tesseract-ocr-epo_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-epo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-est.\n",
            "Preparing to unpack .../075-tesseract-ocr-est_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-est (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-eus.\n",
            "Preparing to unpack .../076-tesseract-ocr-eus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-frk.\n",
            "Preparing to unpack .../077-tesseract-ocr-frk_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-frk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-frm.\n",
            "Preparing to unpack .../078-tesseract-ocr-frm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-frm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-glg.\n",
            "Preparing to unpack .../079-tesseract-ocr-glg_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-glg (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-heb.\n",
            "Preparing to unpack .../080-tesseract-ocr-heb_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-heb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hin.\n",
            "Preparing to unpack .../081-tesseract-ocr-hin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hrv.\n",
            "Preparing to unpack .../082-tesseract-ocr-hrv_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hrv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-isl.\n",
            "Preparing to unpack .../083-tesseract-ocr-isl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-isl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ita-old.\n",
            "Preparing to unpack .../084-tesseract-ocr-ita-old_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ita-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-jpn.\n",
            "Preparing to unpack .../085-tesseract-ocr-jpn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-jpn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kan.\n",
            "Preparing to unpack .../086-tesseract-ocr-kan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kor.\n",
            "Preparing to unpack .../087-tesseract-ocr-kor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mal.\n",
            "Preparing to unpack .../088-tesseract-ocr-mal_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mkd.\n",
            "Preparing to unpack .../089-tesseract-ocr-mkd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mkd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mlt.\n",
            "Preparing to unpack .../090-tesseract-ocr-mlt_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mlt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-msa.\n",
            "Preparing to unpack .../091-tesseract-ocr-msa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-msa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-spa-old.\n",
            "Preparing to unpack .../092-tesseract-ocr-spa-old_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-spa-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-sqi.\n",
            "Preparing to unpack .../093-tesseract-ocr-sqi_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sqi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-swa.\n",
            "Preparing to unpack .../094-tesseract-ocr-swa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-swa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tam.\n",
            "Preparing to unpack .../095-tesseract-ocr-tam_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tel.\n",
            "Preparing to unpack .../096-tesseract-ocr-tel_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tha.\n",
            "Preparing to unpack .../097-tesseract-ocr-tha_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tha (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bre.\n",
            "Preparing to unpack .../098-tesseract-ocr-bre_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bre (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-sim-vert.\n",
            "Preparing to unpack .../099-tesseract-ocr-chi-sim-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-sim-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-tra-vert.\n",
            "Preparing to unpack .../100-tesseract-ocr-chi-tra-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-tra-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-cos.\n",
            "Preparing to unpack .../101-tesseract-ocr-cos_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-cos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-div.\n",
            "Preparing to unpack .../102-tesseract-ocr-div_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-div (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fao.\n",
            "Preparing to unpack .../103-tesseract-ocr-fao_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fil.\n",
            "Preparing to unpack .../104-tesseract-ocr-fil_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fil (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fry.\n",
            "Preparing to unpack .../105-tesseract-ocr-fry_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fry (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-gla.\n",
            "Preparing to unpack .../106-tesseract-ocr-gla_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-gla (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hye.\n",
            "Preparing to unpack .../107-tesseract-ocr-hye_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hye (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-jpn-vert.\n",
            "Preparing to unpack .../108-tesseract-ocr-jpn-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-jpn-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kor-vert.\n",
            "Preparing to unpack .../109-tesseract-ocr-kor-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kor-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kmr.\n",
            "Preparing to unpack .../110-tesseract-ocr-kmr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ltz.\n",
            "Preparing to unpack .../111-tesseract-ocr-ltz_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ltz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mon.\n",
            "Preparing to unpack .../112-tesseract-ocr-mon_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mon (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mri.\n",
            "Preparing to unpack .../113-tesseract-ocr-mri_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mri (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-oci.\n",
            "Preparing to unpack .../114-tesseract-ocr-oci_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-oci (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-que.\n",
            "Preparing to unpack .../115-tesseract-ocr-que_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-que (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-snd.\n",
            "Preparing to unpack .../116-tesseract-ocr-snd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-snd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-sun.\n",
            "Preparing to unpack .../117-tesseract-ocr-sun_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tat.\n",
            "Preparing to unpack .../118-tesseract-ocr-tat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ton.\n",
            "Preparing to unpack .../119-tesseract-ocr-ton_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ton (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-yor.\n",
            "Preparing to unpack .../120-tesseract-ocr-yor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-yor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-arab.\n",
            "Preparing to unpack .../121-tesseract-ocr-script-arab_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-arab (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-armn.\n",
            "Preparing to unpack .../122-tesseract-ocr-script-armn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-armn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-beng.\n",
            "Preparing to unpack .../123-tesseract-ocr-script-beng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-beng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-cans.\n",
            "Preparing to unpack .../124-tesseract-ocr-script-cans_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-cans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-cher.\n",
            "Preparing to unpack .../125-tesseract-ocr-script-cher_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-cher (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-cyrl.\n",
            "Preparing to unpack .../126-tesseract-ocr-script-cyrl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-deva.\n",
            "Preparing to unpack .../127-tesseract-ocr-script-deva_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-deva (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-ethi.\n",
            "Preparing to unpack .../128-tesseract-ocr-script-ethi_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-ethi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-frak.\n",
            "Preparing to unpack .../129-tesseract-ocr-script-frak_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-frak (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-geor.\n",
            "Preparing to unpack .../130-tesseract-ocr-script-geor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-geor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-grek.\n",
            "Preparing to unpack .../131-tesseract-ocr-script-grek_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-grek (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-gujr.\n",
            "Preparing to unpack .../132-tesseract-ocr-script-gujr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-gujr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-guru.\n",
            "Preparing to unpack .../133-tesseract-ocr-script-guru_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-guru (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hans.\n",
            "Preparing to unpack .../134-tesseract-ocr-script-hans_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hans-vert.\n",
            "Preparing to unpack .../135-tesseract-ocr-script-hans-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hans-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hant.\n",
            "Preparing to unpack .../136-tesseract-ocr-script-hant_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hant (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hant-vert.\n",
            "Preparing to unpack .../137-tesseract-ocr-script-hant-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hant-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hang.\n",
            "Preparing to unpack .../138-tesseract-ocr-script-hang_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hang (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hang-vert.\n",
            "Preparing to unpack .../139-tesseract-ocr-script-hang-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hang-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hebr.\n",
            "Preparing to unpack .../140-tesseract-ocr-script-hebr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hebr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-jpan.\n",
            "Preparing to unpack .../141-tesseract-ocr-script-jpan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-jpan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-jpan-vert.\n",
            "Preparing to unpack .../142-tesseract-ocr-script-jpan-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-jpan-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-knda.\n",
            "Preparing to unpack .../143-tesseract-ocr-script-knda_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-knda (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-khmr.\n",
            "Preparing to unpack .../144-tesseract-ocr-script-khmr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-khmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-laoo.\n",
            "Preparing to unpack .../145-tesseract-ocr-script-laoo_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-laoo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-latn.\n",
            "Preparing to unpack .../146-tesseract-ocr-script-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-mlym.\n",
            "Preparing to unpack .../147-tesseract-ocr-script-mlym_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-mlym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-mymr.\n",
            "Preparing to unpack .../148-tesseract-ocr-script-mymr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-mymr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-orya.\n",
            "Preparing to unpack .../149-tesseract-ocr-script-orya_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-orya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-sinh.\n",
            "Preparing to unpack .../150-tesseract-ocr-script-sinh_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-sinh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-syrc.\n",
            "Preparing to unpack .../151-tesseract-ocr-script-syrc_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-syrc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-taml.\n",
            "Preparing to unpack .../152-tesseract-ocr-script-taml_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-taml (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-telu.\n",
            "Preparing to unpack .../153-tesseract-ocr-script-telu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-telu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-thaa.\n",
            "Preparing to unpack .../154-tesseract-ocr-script-thaa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-thaa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-thai.\n",
            "Preparing to unpack .../155-tesseract-ocr-script-thai_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-thai (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-tibt.\n",
            "Preparing to unpack .../156-tesseract-ocr-script-tibt_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-tibt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-viet.\n",
            "Preparing to unpack .../157-tesseract-ocr-script-viet_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-viet (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-grc.\n",
            "Preparing to unpack .../158-tesseract-ocr-grc_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-grc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-all.\n",
            "Preparing to unpack .../159-tesseract-ocr-all_4.1.1-2.1build1_all.deb ...\n",
            "Unpacking tesseract-ocr-all (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-jav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-guru (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hang (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-thaa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-tra-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-cos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kaz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hant (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hant-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-swa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-heb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mlt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fry (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-nld (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-afr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ces (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-sinh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-san (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-beng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ton (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-guj (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-orya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ceb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-vie (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-cans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-slv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lit (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bul (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-iku (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-mlym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fas (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-dan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-epo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-nep (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-grc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mon (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kat-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tha (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-swe (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-grek (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ell (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-frm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ita-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-amh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-gle (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ita (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ron (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-isl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-aze-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bod (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-syrc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-cym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bre (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-pol (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hebr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-laoo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-jpn-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-cat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-glg (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-deva (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-slk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-que (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tur (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-uzb-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-sim-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-tibt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-thai (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-uzb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mri (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-srp (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ind (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-oci (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ukr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-msa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-knda (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-gla (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-yor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hye (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fil (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-khmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-enm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-nor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-armn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-spa-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-div (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-jpan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-geor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hrv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-frak (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-yid (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-syr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ltz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-uig (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-mymr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-gujr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-est (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-snd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-jpn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-arab (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-viet (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hans-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-ethi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sqi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-khm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-eus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kor-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-jpan-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-taml (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mkd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tgk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-telu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-pus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hang-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-frk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-dzo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-cher (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-srp-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-aze (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-all (4.1.1-2.1build1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import cv2\n",
        "import pytesseract\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch"
      ],
      "metadata": {
        "id": "oBZIWDzXInPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Ocr:\n",
        "    def preprocess_image(self, image_path, scale_percent=200):\n",
        "        self.scale_percent = scale_percent\n",
        "        img = cv2.imread(image_path)\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        width = int(gray_img.shape[1] * self.scale_percent / 100)\n",
        "        height = int(gray_img.shape[0] * self.scale_percent / 100)\n",
        "        gray_img = cv2.resize(gray_img, (width, height), interpolation=cv2.INTER_CUBIC)\n",
        "        denoised = cv2.fastNlMeansDenoising(gray_img, h=10)\n",
        "        return denoised\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        cleaned = re.sub(r'[^а-яА-ЯёЁa-zA-Z0-9\\s.,;:!?()%\\-—–«»]', '', text)\n",
        "        return cleaned\n",
        "\n",
        "    def img2txt(self, img, min_confidence=60):\n",
        "        self.min_confidence = min_confidence\n",
        "        processed = self.preprocess_image(img)\n",
        "        config = '--psm 6 --oem 3'\n",
        "        data = pytesseract.image_to_data(\n",
        "            processed,\n",
        "            lang='rus+eng',\n",
        "            config=config,\n",
        "            output_type=pytesseract.Output.DICT\n",
        "        )\n",
        "        filtered_text = []\n",
        "        for i, conf in enumerate(data['conf']):\n",
        "            if int(conf) > self.min_confidence:\n",
        "                filtered_text.append(data['text'][i])\n",
        "        return ' '.join(filtered_text)\n",
        "\n",
        "    def process(self, image_path):\n",
        "        text = self.img2txt(image_path)\n",
        "        return self.clean_text(text)\n",
        "\n",
        "\n",
        "class ImageCaptioner:\n",
        "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-large\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "    def describe(self, image_path, max_length=50):\n",
        "        image = Image.open(image_path)\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        output = self.model.generate(**inputs, max_new_tokens=max_length)\n",
        "        caption = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return caption\n",
        "\n",
        "    def describe_with_context(self, image_path, question):\n",
        "        image = Image.open(image_path)\n",
        "        inputs = self.processor(images=image, text=question, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        output = self.model.generate(**inputs)\n",
        "        answer = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "nHUN0OUdItbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captioner = ImageCaptioner()\n",
        "description = captioner.describe(\"The_life_of_Tolstoy.jpg\")\n",
        "print(description)\n",
        "\n",
        "\n",
        "ocr = Ocr()\n",
        "text = ocr.process('The_life_of_Tolstoy.jpg')\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kjxQqIJJA5g",
        "outputId": "6977aad3-11e1-469e-e437-464305c680db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a close up of a book with a text on it\n",
            "94 THE LIFE OF TOLSTOY The results he published in his book, A Criticism of Dogmatic Theology. Freeing himself from the creed of the Church, he was inevitably led to examine the teaching of Christianity as contained in the Bible, and conse- quently the Bible itself. He did this in a lengthy work, The Four Gospels Unified and Trans- lated. In this work, step by step, he analysed the text of the Gospels, throwing aside that which was not clear or not directly connected with the main idea of Christianity. The passages clearly express- ing this principal idea he arranged in a connected, easily understood form, and the whole teaching assumed a complete, harmonious, and popular character. Arriving at the very root of Christianity, Tolstoy undertook a new work to explain his con- ception of it: What is My Faith? It may be said that, with this book, the cycle of his religious development was accomplished.\n"
          ]
        }
      ]
    }
  ]
}