{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XR5weqSUCh-"
      },
      "source": [
        "–í—ã–±–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞\n",
        "\n",
        "–î–µ–¥–ª–∞–π–Ω - 23:59 8 –æ–∫—Ç—è–±—Ä—è\n",
        "–ü–æ—Å–ª–µ –¥–µ–¥–ª–∞–π–Ω–∞ —è –∑–∞–∫—Ä–æ—é —Ç–∞–±–ª–∏—Ü—É –Ω–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "\n",
        "–í –∫–æ–ª–æ–Ω–∫—É \"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\" –º–æ–∂–µ—Ç–µ –∑–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–∏ –∏–¥–µ–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å—Å—è –∫ –∫–∞–∫–æ–π —Ç–æ –∫–æ–º–∞–Ω–¥–µ –∏ –¥–µ–ª–∞—Ç—å –∏—Ö –ø—Ä–æ–µ–∫—Ç, —Ç–æ –º–æ–∂–µ—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å —ç—Ç–æ –ø–æ–ª–µ –ø—É—Å—Ç—ã–º\n",
        "–í–∞–º –≤–∞–∂–Ω–æ –∑–∞ –Ω–µ–¥–µ–ª—é —Ä–∞–∑–±–∏—Ç—å—Å—è –Ω–∞ –≥—Ä—É–ø–ø—ã 3-5 —á–µ–ª–æ–≤–µ–∫ –∏ –≤–ø–∏—Å–∞—Ç—å –≤ –ø–æ–ª–µ \"–§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞\" –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –±—É–¥–µ—Ç–µ –¥–µ–ª–∞—Ç—å –∏ —Å–¥–∞–≤–∞—Ç—å\n",
        "\n",
        "–í—ã –¥–æ–ª–∂–Ω—ã —Å–¥–µ–ª–∞—Ç—å 2 –≤–µ—â–∏:\n",
        "1) –ö 8 –æ–∫—Ç—è–±—Ä—è —É –∫–∞–∂–¥–æ–≥–æ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –≥—Ä–∞—Ñ–∞ \"–§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞\" - —ç—Ç–æ —Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –±—É–¥–µ—Ç–µ –¥–µ–ª–∞—Ç—å –≤–º–µ—Å—Ç–µ —Å –∫–æ–º–∞–Ω–¥–æ–π\n",
        "2) –í–∞–∂–Ω–æ –Ω–µ —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å—Å—è —Å –ø—Ä–æ–µ–∫—Ç–æ–º, –Ω–æ –∏ –Ω–∞–π—Ç–∏ –ª—é–¥–µ–π, —Å –∫–µ–º –≤—ã –µ–≥–æ –±—É–¥–µ—Ç–µ –¥–µ–ª–∞—Ç—å\n",
        "\n",
        "–ù–∞ –ø—Ä–æ–µ–∫—Ç–µ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç 3-5 —á–µ–ª–æ–≤–µ–∫\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã –Ω–µ –≤—ã–±–µ—Ä–µ—Ç–µ –ø—Ä–æ–µ–∫—Ç, —Ç–æ –≤—ã –Ω–µ —Å–º–æ–∂–µ—Ç–µ —Å–¥–∞—Ç—å –∫—É—Ä—Å, –ø–æ—Ç–æ–º—É —á—Ç–æ –ø—Ä–æ–µ–∫—Ç —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 70% –æ—Ü–µ–Ω–∫–∏ –∑–∞ –∫—É—Ä—Å\n",
        "\n",
        "–î–µ–ª–∞—Ç—å –ø—Ä–æ–µ–∫—Ç –≤ –æ–¥–∏–Ω–æ—á–∫—É –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1NVQv2CNGeyoHYbYGBFx0rhv7iDNYAJm8pQBImWeCjwg/edit?usp=sharing\n",
        "\n",
        "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞\n",
        "\n",
        "1) –£ –≤–∞—Å –¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è LLM/VLM –≤ –≤–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ (–º–æ–∂–Ω–æ API, –º–æ–∂–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å)\n",
        "2) –£ –≤–∞—Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤ –ø—Ä–æ–µ–∫—Ç–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω RAG\n",
        "3) –í–∞—à–∞ –∑–∞–¥–∞—á–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ—à–µ–Ω–∞ —Å –≤—ã—Å–æ–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º. –ó–¥–µ—Å—å –≤—ã —Å–∞–º–∏ —Ä–µ—à–∞–µ—Ç–µ –∫–∞–∫ –±—É–¥–µ—Ç–µ –∑–∞–º–µ—Ä—è—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ\n",
        "4) –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∫—Ä–∞—Å–∏–≤–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∫–æ–¥–∞ –ø–æ —Å—Ç–∏–ª—é\n",
        "\n",
        "–û–ø–∏—Å–∞–Ω–∏–µ –≤—ã—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ—à–µ–Ω–∏—é —Å –æ–¥–Ω–∏–º LLM –∞–≥–µ–Ω—Ç–æ–º. –ù–æ –±—É–¥–µ—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è, –µ—Å–ª–∏ –≤—ã —Å–¥–µ–ª–∞–µ—Ç–µ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—É–¥–µ—Ç 2 –∞–≥–µ–Ω—Ç–æ–≤. –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å –º—ã –±—É–¥–µ–º –æ–±—Å—É–∂–¥–∞—Ç—å –≤ –Ω–∞—à–µ–º –∫—É—Ä—Å–µ –±–ª–∏–∂–µ –∫ –Ω–æ—è–±—Ä—é\n",
        "\n",
        "–û—Ç —Å–µ–±—è –¥–æ–±–∞–≤–ª—é, —á—Ç–æ –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ, —Ç–æ —Ç—É—Ç –∑–∞–ø—Ä–µ—Ç–∞ –Ω–µ—Ç. –ù–æ —É—á–∏—Ç—ã–≤–∞–π—Ç–µ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω—ã –º–æ—â–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –∞ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –≤—Ä—è–¥ –ª–∏ –æ–±–µ—Å–ø–µ—á–∞—Ç –≤–∞–º —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤—ã—Ö–æ–¥–µ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWVZcJMTT0dt"
      },
      "source": [
        "2) –†–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å API Mistral\n",
        "3) –†–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å RAG\n",
        "4) –ü—Ä–∏–¥—É–º–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É –∫–∞—á–µ—Å—Ç–≤–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "v0wLof4h3hh7"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from IPython.display import display, Markdown\n",
        "import re\n",
        "import os\n",
        "from mistralai import Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vs76Ef5gW9AmmSGO3GKQWyCtdLPQkk6K\n"
          ]
        }
      ],
      "source": [
        "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "print(api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv5IWwJe3jnz",
        "outputId": "86f47cb4-32c4-4c1b-c6fc-f0d12d5fd414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key loaded successfully from Colab secrets!\n",
            "‚úÖ Connected successfully!\n",
            "Available models: ['mistral-medium-2505', 'mistral-large-latest', 'mistral-medium-2508', 'mistral-medium-latest', 'mistral-medium', 'ministral-3b-2410', 'ministral-3b-latest', 'ministral-8b-2410', 'ministral-8b-latest', 'open-mistral-7b', 'mistral-tiny', 'mistral-tiny-2312', 'open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-2407', 'mistral-tiny-latest', 'mistral-large-2411', 'pixtral-large-2411', 'pixtral-large-latest', 'mistral-large-pixtral-2411', 'codestral-2508', 'codestral-latest', 'devstral-small-2507', 'devstral-small-latest', 'devstral-medium-2507', 'devstral-medium-latest', 'mistral-vibe-cli-latest', 'pixtral-12b-2409', 'pixtral-12b', 'pixtral-12b-latest', 'mistral-small-2506', 'mistral-small-latest', 'magistral-medium-2509', 'magistral-medium-latest', 'magistral-small-2509', 'magistral-small-latest', 'voxtral-mini-2507', 'voxtral-mini-latest', 'voxtral-small-2507', 'voxtral-small-latest', 'devstral-small-2505', 'magistral-small-2506', 'magistral-medium-2506', 'magistral-small-2507', 'magistral-medium-2507', 'mistral-small-2409', 'codestral-2501', 'codestral-2412', 'codestral-2411-rc5', 'mistral-small-2503', 'mistral-small-2501', 'mistral-embed-2312', 'mistral-embed', 'codestral-embed', 'codestral-embed-2505', 'mistral-moderation-2411', 'mistral-moderation-latest', 'mistral-ocr-2505', 'mistral-ocr-latest', 'mistral-ocr-2503', 'voxtral-mini-transcribe-2507', 'voxtral-mini-2507', 'voxtral-mini-latest']\n"
          ]
        }
      ],
      "source": [
        "# Access the secret\n",
        "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"‚ùå MISTRAL_API_KEY not found in Colab secrets!\")\n",
        "else:\n",
        "    print(\"‚úÖ API key loaded successfully from Colab secrets!\")\n",
        "\n",
        "# Initialize Mistral client\n",
        "client_1 = Mistral(api_key=api_key)\n",
        "\n",
        "# Test connection\n",
        "def test_connection():\n",
        "    try:\n",
        "        models = client_1.models.list()\n",
        "        print(\"‚úÖ Connected successfully!\")\n",
        "        print(f\"Available models: {[m.id for m in models.data]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Connection failed: {e}\")\n",
        "        print(\"üí° If key is not active yet, wait a few minutes and try again\")\n",
        "\n",
        "test_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDXOAs2ICncZ"
      },
      "source": [
        "<!-- 1) –ó–∞–≥—Ä—É–∑–∫–∞ pdf-–¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≥—É–≥–ª –∫–æ–ª–ª–∞–± -->\n",
        "2) –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pdf-–¥–æ–∫—É–º–µ–Ω—Ç–∞ (doc, docx) (–∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ –≥—É–≥–ª –¥–∏—Å–∫) (PYpdf) –≤ —Ç–µ–∫—Å—Ç. $\\textbf{–ê–π—Ç–æ—Ä–µ}$\n",
        "\n",
        "\n",
        "3) –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥\n",
        "\n",
        "3.1) –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å OCR –∏ CLIP. $\\textbf{–ö–∏—Ä–∏–ª–ª}$\n",
        "\n",
        "3.2) –£–±–∏—Ä–∞–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–∏–ø–∏–Ω–∞–Ω–∏–µ, —Ä–∞—Å—Å–º–æ—Ç—Ä –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ –∏ –ø—Ä–æ—á–µ–≥–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª—å–∫–∏ $\\textbf{–ò–ª—å—è}$\n",
        "\n",
        "...\n",
        "\n",
        "4) –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è+–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ RAG (mistral embedding), –∑–∞–≥—Ä—É–∑–∏–º –≤ weaviate (—á—Ç–æ –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å?) - (—Å–µ–º–∞–Ω—Ç–∏–∫—É —á–∞–Ω–∫–∞) $\\textbf{–ê–ª–∞–Ω, –ú–∞–≤–¥–∂—É–¥–∞}$\n",
        "\n",
        " <!-- 6) –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è -->\n",
        "\n",
        "7) –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –ø–æ –≤–µ–∫—Ç–æ—Ä—É –≤–æ–ø—Ä–æ—Å–∞ (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π) $\\textbf{–∏–∑–∏}$\n",
        "\n",
        " <!-- 8) –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Mistral ('mistral-large-latest') –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–ø–µ—Ä–µ–¥–∞–µ–º —ç—Ç–∏ —á–∞–Ω–∫–∏ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è Mistral API) -->\n",
        "\n",
        " <!-- 9) –í—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞ -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "76KVRRc1LKf3"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "kc_eVTNDig0Z",
        "outputId": "a9f53eff-7e18-412d-c5dc-c880a3c66402"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/hakimovamavjuda/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/hakimovamavjuda/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FileSystemPathPointer('/Users/hakimovamavjuda/nltk_data/tokenizers/punkt_tab')"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.find('tokenizers/punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Fkt7XYfXkCTp"
      },
      "outputs": [],
      "source": [
        "PDF_FILE_PATH = \"/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "cMu77BV4l4vV"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(PDF_FILE_PATH, extract_images=True)\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "docs = []\n",
        "\n",
        "url = \"https://docs.streamlit.io/develop/quick-reference/release-notes\"\n",
        "\n",
        "try:\n",
        "    loader = WebBaseLoader(url)  # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL\n",
        "    docs.extend(loader.load())   # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Ö –≤ —Å–ø–∏—Å–æ–∫ docs\n",
        "except Exception as e:\n",
        "    print(f\"Error loading document from {url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swi7oW5DDrjt",
        "outputId": "177c5f27-a8a4-46c6-e19d-be7be8dd573d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images ‚Äì much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the\\nÔ¨Årst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-\\naugmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries: These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model‚Äôs parameters. More speciÔ¨Åcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model‚Äôs predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneÔ¨Åcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount\\nof multimodal knowledge available on the web‚Äî\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user Ô¨Årst poses a ques-\\ntion such as ‚ÄúWhat can be found on the White\\nHouse balconies at Christmas‚Äù. The system then\\nretrieves relevant items from its memory, for exam-\\narXiv:2210.02928v2  [cs.CL]  20 Oct 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='ple, the Ô¨Årst image of Figure 1 with the caption\\n‚ÄúWhite House during Christmas‚Äù, which it uses to\\nproduce the answer ‚Äúwreaths and garlands‚Äù. Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview: retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciÔ¨Åcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nÔ¨Årst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes\\nimage-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciÔ¨Åcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model‚Äôs input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-\\nerative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides\\nthe model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during Ô¨Åne-\\ntuning Figure 2 the model‚Äôs input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We Ô¨Åne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to Ô¨Årst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-\\ntractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniÔ¨Åed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was Ô¨Årst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another\\nfamily of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneÔ¨Åt downstream knowledge in-\\ntensive tasks (e.g. Question Answering). REALM\\nis an encoder-only model trained with masked lan-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the Ô¨Årst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneÔ¨Åcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal\\nfeatures for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the Ô¨Årst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA‚Äôs\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and\\nMIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-\\nmodalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form\\nfŒ∏(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a ‚Äúbackbone‚Äù model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model‚Äôs encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfŒ∏ and\\ndecoder gŒ∏. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nÔ¨Årst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ‚ààRLi√óD, where Li is the length of the im-\\nage tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT ‚ààRLt√óD. For k images and n text inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e = [e1\\nI; e1\\nT; ¬∑¬∑¬∑ ; ek\\nI; en\\nT] ‚ààR(kLt+nLi)√óD,\\nwhich is fed to another bi-directional transformer\\nfŒ∏ initialized from T5. We enable cross-attention'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fŒ∏(e) ‚àà R(kLt+nLi)√óD.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fŒ∏(e)[CLS] ‚ààRD for dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query q of any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. SpeciÔ¨Åcally, we apply the backbone encoder\\nfŒ∏ to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m ‚ààM to Ô¨Ånd the Top-K\\nnearest neighbors TopK(M|q) = [ m1, ¬∑¬∑¬∑ , mk].\\nFormally, we deÔ¨Åne TopK(M|q) as follows:\\nTopK(M|q) = TopK\\nm‚ààM\\nfŒ∏(q)[CLS] ¬∑fŒ∏(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query q as\\nan augmented input [m1, ¬∑¬∑¬∑ , mk, q], which is fed\\nto the backbone encoder fŒ∏ to produce retrieval-\\naugmented encoding. The decoder model gŒ∏ uses\\nattention over this representation to generate tex-\\ntual outputs y = y1, ¬∑¬∑¬∑ , yn token by token.\\np(yi|yi‚àí1) = gŒ∏(yi|fŒ∏(TopK(M|q); q); y1:i‚àí1)\\nwhere y is decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xI plus a text prompt xp. The exter-\\nnal memory Mcontains textual-only entries mT.\\nThe Top-K retrievalsmT\\n1 , ¬∑¬∑¬∑ , mT\\nk are leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\nory MB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efÔ¨Åciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;\\nChangpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containing\\ncrawled image-text pairs Ô¨Åltered by CLIP (Rad-\\nford et al., 2021). We apply rules to Ô¨Ålter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but Ô¨Åltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics\\nFor LAION and CC, we use the input image as\\nxI, and ‚Äògenerate caption:‚Äô as the text promptxp.\\nFor VQA, we use the input image as xI and the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MB is con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse Ô¨Åxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L = Lgen +\\nLcon as follows:\\nLcon = ‚àílog exp(fŒ∏(xI, xp) ¬∑fŒ∏(mT))‚àë\\nm‚ààMB\\nexp(fŒ∏(xI, xp) ¬∑fŒ∏(mT))\\nLgen = ‚àílog gŒ∏(y|fŒ∏(Mp; xI; xp))\\nMp =\\n{\\nTopK(MB|xI, xp) If (xI, xp) ‚ààPAQ/VQA\\n√ò If (xI, xp) ‚ààLAION/CC\\nwhere Mp is the retrieved augmentation: if the\\ninput query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lcon is minimized to dis-\\ncriminate between the positive query-memory pairs'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deÔ¨Ånes the pre-training\\nimplementation, while the lower part deÔ¨Ånes Ô¨Åne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fŒ∏(xI; xp)[CLS] and\\ncandidates fŒ∏(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgen is min-\\nimized to generate target tokens y conditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe Ô¨Ånetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge\\ndatastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1 The Top-K retrievals\\n{(mI\\n1, mT\\n1 ), ¬∑¬∑¬∑ , (mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nÔ¨Åxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss functionL = Lcon+Lgen based\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.\\non the in-batch memory MB as follows:\\nLcon = ‚àílog exp(fŒ∏(xq) ¬∑fŒ∏(mI; mT))‚àë\\nm‚ààMB\\nexp(fŒ∏(xq) ¬∑fŒ∏(mI; mT))\\nLgen = ‚àílog gŒ∏(y|fŒ∏(TopK(MB|xq); xq))\\nThe in-batch memory MB is constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk, {mI\\ni, mI\\ni}k, {¬ØmI\\nj, ¬ØmT\\nj }k),\\nwhere m represents the positive (image, text)\\nsource, and ¬Øm represents the hard negative\\n(image, text) source provided by the dataset 2.\\nFor a batch with B examples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB =\\n{{mI\\ni, mI\\ni}1, {¬ØmI\\nj, ¬ØmT\\nj }1, ¬∑¬∑¬∑ , {¬ØmI\\nj, ¬ØmT\\nj }B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq) and continue to opti-\\nmize Lgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use Ô¨Åxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conÔ¨Ågurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we Ô¨Årst train the\\nmodel on LAION for 1M steps, and then continue\\ntraining on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then Ô¨Åne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and Ô¨Åxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn‚Äôt yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-\\nmodal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.\\nDataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model‚Äôs generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nÔ¨Çuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-\\nrectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors Ô¨Årst use the template\\nto generate questions and then ask crowd-workers\\nto Ô¨Ålter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. SpeciÔ¨Åcally, we choose the\\nquestions with types of ‚ÄòTextQ‚Äô and ‚ÄòImageQ‚Äô to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K\\ntext and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During Ô¨Åne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nÔ¨Årst to select the most plausible source si, and then\\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\\nto autoregressively decode answer A with masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nÔ¨Årst applies a question type classiÔ¨Åer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. SpeciÔ¨Åcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to Ô¨Ånd\\na set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input S to the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA‚Äôs results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiÔ¨Åcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reÔ¨Çect the high Ô¨Çuency and accuracy\\nof MuRAG‚Äôs generation, and the improvement is\\nmore pronounced for full wiki.\\nWe show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniÔ¨Åcant, with 20+% improvement under both\\nsettings. Similarly, we Ô¨Ånd that our model is more\\ncapable of handling full-wiki corpus.\\nEvaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofÔ¨Åcial test-set results indicated\\non leaderboard 3 as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to Ô¨Çuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting 35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their Ô¨Åne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,\\nPAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage Ô¨Åne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different Ô¨Åne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractor MuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-Wiki MuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,\\nthe performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the Ô¨Åne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG‚Äôs pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model‚Äôs perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiÔ¨Åcantly behind text accuracy.\\nWe further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difÔ¨Åcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciÔ¨Åc region is difÔ¨Åcult, 3) the questions require\\noptical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the Ô¨Årst example, the model is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‚Äòthe angel is holding a dead body‚Äô. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the Ô¨Årst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiÔ¨Åcantly lower than the performance on\\nqueries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufÔ¨Åciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus‚Äôs format (image -> text) is differ-\\nent from Ô¨Åne-tuning (text -> image+text). This\\nmisalignment limits the model‚Äôs performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-\\nformance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic Ô¨Åltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision, pages 2425‚Äì2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426.\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877‚Äì1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition.\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 3558‚Äì3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581.\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server.arXiv preprint\\narXiv:1504.00325.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision, pages 104‚Äì120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International\\nConference on Learning Representations.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning, pages 3887‚Äì3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Ma-\\nchine Learning Research, pages 3929‚Äì3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on ArtiÔ¨Åcial Intelligence , volume 34, pages\\n7879‚Äì7886.\\nGautier Izacard and √âdouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 874‚Äì880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciÔ¨Åc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM, 63(7):67‚Äì78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128‚Äì3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\nt√§schel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems, 33:9459‚Äì9474.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich K√ºttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098‚Äì1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision,\\npages 121‚Äì137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740‚Äì755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language\\ntasks. Advances in neural information processing\\nsystems, 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195‚Äì3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning, pages 8748‚Äì8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniÔ¨Åed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1‚Äì67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\\nhit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems, 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189.\\nChristoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nÔ¨Åltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A\\ncleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n2556‚Äì2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning ,\\npages 4596‚Äì4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 5317‚Äì5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR.\\nPat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nIn Proceedings of NAACL-HLT, pages 3678‚Äì3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems, 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 5579‚Äì5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nÔ¨Åed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non ArtiÔ¨Åcial Intelligence , volume 34, pages 13041‚Äì\\n13049.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to Ô¨Årst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a Ô¨Åxed sample ratio. We plot\\nthe Ô¨Årst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we Ô¨Årst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-\\nuation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEr\\non CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efÔ¨Åciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model ConÔ¨Åguration\\nWe demonstrate the ViT conÔ¨Åguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" n u m _ l a y e r s \" : 24 ,\\n\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conÔ¨Åguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13'}, page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.')]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ha_HOAdl8hc",
        "outputId": "34cc81c3-5316-4d96-fd79-a24f47741070"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "WEAVIATE_CLUSTER = os.getenv(\"WEAVIATE_CLUSTER\")\n",
        "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "7DJKCrLqy_GS"
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "from weaviate.auth import AuthApiKey\n",
        "\n",
        "\n",
        "WEAVIATE_URL = \"https://\" + WEAVIATE_CLUSTER\n",
        "auth = AuthApiKey(api_key=WEAVIATE_API_KEY)\n",
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_URL,\n",
        "    auth_client_secret=auth\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Uj7F7DWM0OFE"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "3c63c161ec504640adce7bb6456ce07b",
            "3652b1e0d8f74c3880caf9d9e0ff4ac6",
            "e350952d965142a39b75b42cb061f792",
            "ec832bd8ee034f0695dcb2021d7c9278",
            "feb62c60abc64fdf9d511641a86772b6",
            "4e9e450bfc7742c4971982ee6c14487d",
            "789b7bf07ed64687ac6e577bebe73937",
            "fb3fa5994afb488f99f19f783ff542ae",
            "9d031315caed4dacb5a5fee94ff7da73",
            "1ec11d7fb1d446e69c48aa95daea69ab",
            "824f404ec0d84a7193cd1be9666fbaa8",
            "65569198d5e049cd810336b3d50fc6dc",
            "a7abef1421ca4210a371bf9031979a4f",
            "587704931fd3408b85acb2ed15f3af78",
            "81178efff367451f807cae878c58b354",
            "92d883ca481c4ef0af603b97ef1599b0",
            "13142dfa5b4342eb9b41221dedce2f70",
            "f12a0e8657ec4f3fa3aa9750635e8ca0",
            "3e0a3149f4ab437ab4e615eedfc52361",
            "c9393342e318474c8e7c9c33c093f814",
            "98e20ab02c6944328b60e4adc545dd80",
            "f797c298b18f47de98d8e722ce3dc3bd",
            "594ac05acdb74516a661a2d72aef60ef",
            "428763fc2a2f481fb670dd0eba635bac",
            "be4991b29a9b47258c133d3d44833029",
            "776b4095a4de4c9b86de984a7ea856b1",
            "92356ba0ac6244a79c1f219a36b6edec",
            "f8d37f2080fa4ee7b929167ab385475a",
            "7a79f7439fb0432ca38af9fa6505bb40",
            "e6b6075324fa4a12a7e94f10c09ead3b",
            "7ab81a07855c456781406ebefef6d758",
            "9caf6c75e9504309b96cf31ea95f7183",
            "6103707b4777415585103d47c9264ba7",
            "27d739cfe817469399e7161fae16b816",
            "d7db72c12c614637be98c0198d6c1e78",
            "6fbff9f1996445ee8d8a293c4970ba5e",
            "c915c1c8c7644ac6b8a489d0e6adcbdb",
            "e9e2e48cb5a44d3a8283e9b7e370fe7e",
            "e951952fc4764a48900ec794bf039d6d",
            "b61defa64ef841989016665e76d021b3",
            "64c638e532c34ba38c38cc91f03d41d6",
            "f66e776a769d446d91acc3586c84a934",
            "337055f2e7b14dc9ba9466af21a8df3d",
            "462353a18b7145058a077940193a0171",
            "d62648797e4340f0a0378d70fba0d6e7",
            "b450a990f5f34b95947b16794afab63a",
            "af14ff8aa1814688a49340e989edead3",
            "fbd2791ed8084434ac2fe783ff1b74bd",
            "756eda1229d54f1381bac09b4a7b4159",
            "2c31b7f1a7164fa295c15cfbc9c9be4a",
            "2d760d849ed546599aa02ae927279fe2",
            "726ee45e96ac457e946066832b5d1673",
            "96a25420cd9e422b88143df294f40204",
            "d494e2be5ae64a7f97449c3378980e9f",
            "9fbbab679fc144bf8ed410b706a1724c",
            "ac3b138194654737b85062eec317ead5",
            "770dc95a3193446eaaaf688bfeb08f97",
            "de9a4b91c3084c7180c30fd4740dd885",
            "4365a2416c784e77889bda1c324b37b8",
            "dde8335f6b6d48bcbdc2305f1d9126d1",
            "8ae18d5a3d484ec5a0a114b9d34ab587",
            "51fb9641cb684979acc2f06588fae3eb",
            "4e66a8e5ef8e4649a75833f218393441",
            "2d46aa583da8471290093f4328375758",
            "f4fa636f8f8a4c11944743a3ae118dc7",
            "b1737aaea82940e5b7ef79f16836fdd2",
            "0ef43210f4e948e4bbfad66da46062cd",
            "9488a5cc84fd48a782753bcd681ab8e6",
            "ad36412ff6804d589c0e742e08079bad",
            "2fc555ae01294bb7a0dc34e7f4465115",
            "65743bfc423b4dba9071a08c67a9d234",
            "a3128c84e5e04a578c65d6175b762f50",
            "5dcf7a0de03a42d893d9e0e6a5295ef3",
            "bdd40699cf1f450aaa8fefe6185c4e88",
            "27cd6a38ee964c0a80f2f8b8233032a4",
            "851cded58f7741528fa3f75086f7b69b",
            "0499bb6b5d1e4d0680bdb73b7299f7a9",
            "8bfacf5b88444db987144d4d80ba211e",
            "0d355e8018e7411bb987595d3282a16e",
            "b873f912f7d946b8b8922897beaf717e",
            "fb7fa4c16f284e029152a0a18d6471c5",
            "9e0ec260461340209471e0fc1db2dc9d",
            "48a7c5d956474ac09605cc080caaab19",
            "a07054577ddc44f180e375a38039160c",
            "85857aa1ac9f4711b22d05f64090aade",
            "acb56058a5b84bab9e73f33866890bcf",
            "76517d9f4124424e9ac830f090936fb6",
            "bc8f6197fdea4337ba6f7f0967acc0be",
            "96dac914ff1f4febb60047c5415bb96b",
            "e57fb762526f46e982af819b4dbfe754",
            "afbfc6b621bb43e69279f2542b63afed",
            "808acd9205384ba8af805cfbe177face",
            "7291cc309e7a46abb5d501eb31f1b5c0",
            "5cd26bc0718f47b499331b836e7cf697",
            "d04247ca7bdd4b50ae7d1ff211b075b8",
            "2ab98e4735844054a97adade5032d0bc",
            "054a6ec2121f48b59e783ac867ff7f9f",
            "86f3ee01e4754741811d2985afaf80aa",
            "fca7817f0b84498189771d67f948416c",
            "ebf520c424bb4a6c9d2212bbf6c6c813",
            "5ea8e4adc7ab4fcda458e81f076ac0b6",
            "a32ef385b157447a88e176df4e18722e",
            "51b66c3d6fb94cdcac951e87ae474710",
            "b59aafc855d140109a3bce64b9a484e5",
            "f80e56706cab44dda3426de4ce49ec40",
            "8c6d1139e257432b91666ae0a04b79f5",
            "d79495f2623f474bb5acbbdf08bfe327",
            "091e87323df44437b85815e384f6f2bc",
            "217e82ba033648d5b94f4a55796f1643",
            "d934dac5094540818895c7fe52df0bab",
            "92599f8eb31a41588f6b7fc2a74b5f00",
            "9a35b78a36d24bc4981374766a81b025",
            "0aadd6e8490941ae9bdf20403d7886e1",
            "a132f90a261c4061bab95edad89c7c9a",
            "1b274a4b65cb41fbba46e2e2561f30ce",
            "14ef4d65267646319f23195ec3d0b5f3",
            "8fc3978e001d49e68447743679f3fe66",
            "e090e21ffa1741e984af4a2035a7ff0a",
            "304e179b09094c47a10ed3ae6007f7b5",
            "4e57f2717fe44f529defa3963dac1e82",
            "2722025bc7064f46b4e3e034d7083ac3"
          ]
        },
        "id": "LCm1WVD51akE",
        "outputId": "1afb98aa-54da-4c09-a22f-2c9bbfce6342"
      },
      "outputs": [],
      "source": [
        "# –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ Weaviate.from_documents)\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "La4ry4d96k_4"
      },
      "outputs": [],
      "source": [
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ (chunks)\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # –†–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
        "    chunk_overlap=20   # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
        ")\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (pages)\n",
        "docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjTkNACU8WVv",
        "outputId": "b1c17cfc-c624-4fd2-f7d8-7f2da40a510c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images ‚Äì much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='Ô¨Årst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='augmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries: These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model‚Äôs parameters. More speciÔ¨Åcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model‚Äôs predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneÔ¨Åcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='of multimodal knowledge available on the web‚Äî\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user Ô¨Årst poses a ques-\\ntion such as ‚ÄúWhat can be found on the White\\nHouse balconies at Christmas‚Äù. The system then\\nretrieves relevant items from its memory, for exam-\\narXiv:2210.02928v2  [cs.CL]  20 Oct 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='ple, the Ô¨Årst image of Figure 1 with the caption\\n‚ÄúWhite House during Christmas‚Äù, which it uses to\\nproduce the answer ‚Äúwreaths and garlands‚Äù. Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview: retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciÔ¨Åcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nÔ¨Årst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='image-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciÔ¨Åcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model‚Äôs input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-\\nerative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='the model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during Ô¨Åne-\\ntuning Figure 2 the model‚Äôs input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We Ô¨Åne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to Ô¨Årst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='tractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniÔ¨Åed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was Ô¨Årst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='family of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneÔ¨Åt downstream knowledge in-\\ntensive tasks (e.g. Question Answering). REALM\\nis an encoder-only model trained with masked lan-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the Ô¨Årst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneÔ¨Åcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='features for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the Ô¨Årst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA‚Äôs\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='MIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-\\nmodalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='fŒ∏(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a ‚Äúbackbone‚Äù model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model‚Äôs encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfŒ∏ and\\ndecoder gŒ∏. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nÔ¨Årst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ‚ààRLi√óD, where Li is the length of the im-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='age tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT ‚ààRLt√óD. For k images and n text inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e = [e1\\nI; e1\\nT; ¬∑¬∑¬∑ ; ek\\nI; en\\nT] ‚ààR(kLt+nLi)√óD,\\nwhich is fed to another bi-directional transformer\\nfŒ∏ initialized from T5. We enable cross-attention'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fŒ∏(e) ‚àà R(kLt+nLi)√óD.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fŒ∏(e)[CLS] ‚ààRD for dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query q of any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. SpeciÔ¨Åcally, we apply the backbone encoder\\nfŒ∏ to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m ‚ààM to Ô¨Ånd the Top-K\\nnearest neighbors TopK(M|q) = [ m1, ¬∑¬∑¬∑ , mk].\\nFormally, we deÔ¨Åne TopK(M|q) as follows:\\nTopK(M|q) = TopK\\nm‚ààM\\nfŒ∏(q)[CLS] ¬∑fŒ∏(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query q as\\nan augmented input [m1, ¬∑¬∑¬∑ , mk, q], which is fed\\nto the backbone encoder fŒ∏ to produce retrieval-\\naugmented encoding. The decoder model gŒ∏ uses'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='attention over this representation to generate tex-\\ntual outputs y = y1, ¬∑¬∑¬∑ , yn token by token.\\np(yi|yi‚àí1) = gŒ∏(yi|fŒ∏(TopK(M|q); q); y1:i‚àí1)\\nwhere y is decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xI plus a text prompt xp. The exter-\\nnal memory Mcontains textual-only entries mT.\\nThe Top-K retrievalsmT\\n1 , ¬∑¬∑¬∑ , mT\\nk are leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\nory MB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efÔ¨Åciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='Changpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containing\\ncrawled image-text pairs Ô¨Åltered by CLIP (Rad-\\nford et al., 2021). We apply rules to Ô¨Ålter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but Ô¨Åltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='For LAION and CC, we use the input image as\\nxI, and ‚Äògenerate caption:‚Äô as the text promptxp.\\nFor VQA, we use the input image as xI and the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MB is con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse Ô¨Åxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L = Lgen +\\nLcon as follows:\\nLcon = ‚àílog exp(fŒ∏(xI, xp) ¬∑fŒ∏(mT))‚àë\\nm‚ààMB\\nexp(fŒ∏(xI, xp) ¬∑fŒ∏(mT))\\nLgen = ‚àílog gŒ∏(y|fŒ∏(Mp; xI; xp))\\nMp =\\n{\\nTopK(MB|xI, xp) If (xI, xp) ‚ààPAQ/VQA\\n√ò If (xI, xp) ‚ààLAION/CC\\nwhere Mp is the retrieved augmentation: if the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='input query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lcon is minimized to dis-\\ncriminate between the positive query-memory pairs'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deÔ¨Ånes the pre-training\\nimplementation, while the lower part deÔ¨Ånes Ô¨Åne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fŒ∏(xI; xp)[CLS] and\\ncandidates fŒ∏(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgen is min-\\nimized to generate target tokens y conditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe Ô¨Ånetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='datastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1 The Top-K retrievals\\n{(mI\\n1, mT\\n1 ), ¬∑¬∑¬∑ , (mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nÔ¨Åxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss functionL = Lcon+Lgen based\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.\\non the in-batch memory MB as follows:\\nLcon = ‚àílog exp(fŒ∏(xq) ¬∑fŒ∏(mI; mT))‚àë\\nm‚ààMB\\nexp(fŒ∏(xq) ¬∑fŒ∏(mI; mT))\\nLgen = ‚àílog gŒ∏(y|fŒ∏(TopK(MB|xq); xq))\\nThe in-batch memory MB is constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk, {mI\\ni, mI\\ni}k, {¬ØmI\\nj, ¬ØmT\\nj }k),\\nwhere m represents the positive (image, text)\\nsource, and ¬Øm represents the hard negative'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='(image, text) source provided by the dataset 2.\\nFor a batch with B examples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB =\\n{{mI\\ni, mI\\ni}1, {¬ØmI\\nj, ¬ØmT\\nj }1, ¬∑¬∑¬∑ , {¬ØmI\\nj, ¬ØmT\\nj }B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq) and continue to opti-\\nmize Lgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use Ô¨Åxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conÔ¨Ågurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we Ô¨Årst train the\\nmodel on LAION for 1M steps, and then continue'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='training on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then Ô¨Åne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and Ô¨Åxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn‚Äôt yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='modal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.\\nDataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model‚Äôs generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nÔ¨Çuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='rectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors Ô¨Årst use the template\\nto generate questions and then ask crowd-workers\\nto Ô¨Ålter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. SpeciÔ¨Åcally, we choose the\\nquestions with types of ‚ÄòTextQ‚Äô and ‚ÄòImageQ‚Äô to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='text and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During Ô¨Åne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nÔ¨Årst to select the most plausible source si, and then\\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\\nto autoregressively decode answer A with masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nÔ¨Årst applies a question type classiÔ¨Åer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. SpeciÔ¨Åcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to Ô¨Ånd'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='a set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input S to the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA‚Äôs results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiÔ¨Åcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reÔ¨Çect the high Ô¨Çuency and accuracy\\nof MuRAG‚Äôs generation, and the improvement is\\nmore pronounced for full wiki.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='We show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniÔ¨Åcant, with 20+% improvement under both\\nsettings. Similarly, we Ô¨Ånd that our model is more\\ncapable of handling full-wiki corpus.\\nEvaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofÔ¨Åcial test-set results indicated\\non leaderboard 3 as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to Ô¨Çuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='Metrics Text Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting 35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their Ô¨Åne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='PAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage Ô¨Åne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different Ô¨Åne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractor MuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-Wiki MuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='the performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the Ô¨Åne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG‚Äôs pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model‚Äôs perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiÔ¨Åcantly behind text accuracy.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='We further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difÔ¨Åcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciÔ¨Åc region is difÔ¨Åcult, 3) the questions require'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='optical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the Ô¨Årst example, the model is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‚Äòthe angel is holding a dead body‚Äô. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the Ô¨Årst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiÔ¨Åcantly lower than the performance on'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='queries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufÔ¨Åciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus‚Äôs format (image -> text) is differ-\\nent from Ô¨Åne-tuning (text -> image+text). This\\nmisalignment limits the model‚Äôs performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='formance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic Ô¨Åltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision, pages 2425‚Äì2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426.\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='learners. Advances in neural information processing\\nsystems, 33:1877‚Äì1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition.\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 3558‚Äì3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581.\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server.arXiv preprint\\narXiv:1504.00325.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='arXiv:1504.00325.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision, pages 104‚Äì120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='Conference on Learning Representations.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning, pages 3887‚Äì3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Ma-\\nchine Learning Research, pages 3929‚Äì3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on ArtiÔ¨Åcial Intelligence , volume 34, pages\\n7879‚Äì7886.\\nGautier Izacard and √âdouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='sociation for Computational Linguistics: Main Vol-\\nume, pages 874‚Äì880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciÔ¨Åc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM, 63(7):67‚Äì78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128‚Äì3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\nt√§schel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='ral Information Processing Systems, 33:9459‚Äì9474.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich K√ºttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098‚Äì1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision,\\npages 121‚Äì137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740‚Äì755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='tasks. Advances in neural information processing\\nsystems, 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195‚Äì3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning, pages 8748‚Äì8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniÔ¨Åed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1‚Äì67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='hit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems, 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189.\\nChristoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nÔ¨Åltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='cleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n2556‚Äì2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning ,\\npages 4596‚Äì4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 5317‚Äì5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Pat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nIn Proceedings of NAACL-HLT, pages 3678‚Äì3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems, 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Pattern Recognition, pages 5579‚Äì5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nÔ¨Åed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non ArtiÔ¨Åcial Intelligence , volume 34, pages 13041‚Äì\\n13049.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to Ô¨Årst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a Ô¨Åxed sample ratio. We plot\\nthe Ô¨Årst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we Ô¨Årst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='uation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEr\\non CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efÔ¨Åciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model ConÔ¨Åguration\\nWe demonstrate the ViT conÔ¨Åguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" n u m _ l a y e r s \" : 24 ,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conÔ¨Åguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13'}, page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.')]"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "zPQj4rEaFxK2"
      },
      "outputs": [],
      "source": [
        "# —Ä—É–≥–∞–ª–æ—Å—å –Ω–∞ —Ñ–æ—Ä–º–∞—Ç ptex_fullbanner, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±—É–¥–µ—Ç –º–Ω–æ–≥–æ –±—É–¥–µ—Ç –ø–ª–æ—Ö–æ(–¥–æ–ª–≥–æ) —Ä–∞–±–æ—Ç–∞—Ç—å\n",
        "for doc in docs:\n",
        "    if 'ptex.fullbanner' in doc.metadata:\n",
        "        doc.metadata['ptex_fullbanner'] = doc.metadata.pop('ptex.fullbanner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "tcYoc6Lw8tj3"
      },
      "outputs": [],
      "source": [
        "  # –ò–∑–º–µ—Ä–∏–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Weaviate –∏ –≤—ã–±–µ—Ä–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤(—Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ (precision/recall), –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–æ–≤, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤.)\n",
        "  #from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "  # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã\n",
        "  #sizes = [200, 500, 1000]\n",
        "  #for size in sizes:\n",
        "      #text_splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=int(size*0.1))\n",
        "      #docs = text_splitter.split_documents(pages)\n",
        "      # –î–∞–ª–µ–µ: –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞\n",
        "      #print(f\"–†–∞–∑–º–µ—Ä {size}: {len(docs)} —á–∞–Ω–∫–æ–≤\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pty.py:95: DeprecationWarning: This process (pid=61244) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
            "  pid, fd = os.forkpty()\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.3.27\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain==0.3.27)\n",
            "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./venv/lib/python3.13/site-packages (from langchain==0.3.27) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in ./venv/lib/python3.13/site-packages (from langchain==0.3.27) (0.4.43)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./venv/lib/python3.13/site-packages (from langchain==0.3.27) (2.12.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.13/site-packages (from langchain==0.3.27) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.13/site-packages (from langchain==0.3.27) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.13/site-packages (from langchain==0.3.27) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in ./venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.27) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.27) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.27) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests<3,>=2->langchain==0.3.27) (2025.11.12)\n",
            "Requirement already satisfied: anyio in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.3.1)\n",
            "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
            "Installing collected packages: langchain-core, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.0.5\n",
            "    Uninstalling langchain-core-1.0.5:\n",
            "      Successfully uninstalled langchain-core-1.0.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.0.5\n",
            "    Uninstalling langchain-1.0.5:\n",
            "      Successfully uninstalled langchain-1.0.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-weaviate 0.0.6 requires weaviate-client<5.0.0,>=4.0.0, but you have weaviate-client 3.26.7 which is incompatible.\n",
            "langgraph-prebuilt 1.0.4 requires langchain-core>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\n",
            "langchain-huggingface 1.0.1 requires langchain-core<2.0.0,>=1.0.3, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-mistralai 1.0.1 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-community 0.4.1 requires langchain-core<2.0.0,>=1.0.1, but you have langchain-core 0.3.79 which is incompatible.\n",
            "langchain-chroma 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.3.27 langchain-core-0.3.79\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.26.7\n"
          ]
        }
      ],
      "source": [
        "import weaviate\n",
        "print(weaviate.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0.5\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "_RfM_5ur9rHT"
      },
      "outputs": [],
      "source": [
        "# —Å–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "from langchain.vectorstores import Weaviate\n",
        "vector_db = Weaviate.from_documents(\n",
        "    docs, embeddings, client=client, by_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpvoh4Py-Kto",
        "outputId": "bd6f96b7-0863-4839-b651-c3e6fdd78ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='ral Information Processing Systems, 33:9459‚Äì9474.'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='fŒ∏(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a ‚Äúbackbone‚Äù model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model‚Äôs encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfŒ∏ and\\ndecoder gŒ∏. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nÔ¨Årst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ‚ààRLi√óD, where Li is the length of the im-'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/docs/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the Ô¨Årst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneÔ¨Åcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal')]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.similarity_search(\"what is rag?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "D1foB-U4YquA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "lR3WSxSeYs2Q"
      },
      "outputs": [],
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "-b4-DA1MNgOy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pty.py:95: DeprecationWarning: This process (pid=61244) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
            "  pid, fd = os.forkpty()\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "0tz97fh-NTjg"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import ChatMistralAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DPLi1j50OWK4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "sisff8WWNq54"
      },
      "outputs": [],
      "source": [
        "model = ChatMistralAI(\n",
        "    api_key=api_key,\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "gv8cKkH4P_Wq"
      },
      "outputs": [],
      "source": [
        "output_parser = StrOutputParser() #–¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞\n",
        "retriever = vector_db.as_retriever() # –ø–æ–∏—Å–∫–æ–≤–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "7CxFQi-zPP6L"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": vector_db.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m retriever_prompt = ChatPromptTemplate.from_messages([\n\u001b[32m     12\u001b[39m     MessagesPlaceholder(variable_name=\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞\u001b[39;00m\n\u001b[32m     13\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGiven the above conversation, generate a search query to look up in order to get information relevant to the conversation. Query: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m     14\u001b[39m ])\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# –°–æ–∑–¥–∞—ë–º retriever, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m history_aware_retriever = \u001b[43mcreate_history_aware_retriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistral-large-latest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# –¢–æ—Ç –∂–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# –í–∞—à –±–∞–∑–æ–≤—ã–π retriever\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretriever_prompt\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# –®–∞–≥ 2: –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (—Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)\u001b[39;00m\n\u001b[32m     24\u001b[39m qa_prompt = ChatPromptTemplate.from_messages([\n\u001b[32m     25\u001b[39m     MessagesPlaceholder(variable_name=\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞\u001b[39;00m\n\u001b[32m     26\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant. Use the following context to answer the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms question.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     27\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m     28\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m     29\u001b[39m ])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm_project_streamlit/venv/lib/python3.13/site-packages/langchain/chains/history_aware_retriever.py:66\u001b[39m, in \u001b[36mcreate_history_aware_retriever\u001b[39m\u001b[34m(llm, retriever, prompt)\u001b[39m\n\u001b[32m     52\u001b[39m     msg = (\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected `input` to be a prompt variable, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt.input_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m     )\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m     58\u001b[39m retrieve_documents: RetrieverOutputLike = RunnableBranch(\n\u001b[32m     59\u001b[39m     (\n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# Both empty string and empty list evaluate to False\u001b[39;00m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m x.get(\u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# If no chat history, then we just pass input to retriever\u001b[39;00m\n\u001b[32m     63\u001b[39m         (\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m]) | retriever,\n\u001b[32m     64\u001b[39m     ),\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# If chat history, then we pass inputs to LLM chain, then to retriever\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m | StrOutputParser() | retriever,\n\u001b[32m     67\u001b[39m ).with_config(run_name=\u001b[33m\"\u001b[39m\u001b[33mchat_retriever_chain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retrieve_documents\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm_project_streamlit/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:635\u001b[39m, in \u001b[36mRunnable.__or__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__or__\u001b[39m(\n\u001b[32m    617\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    618\u001b[39m     other: Runnable[Any, Other]\n\u001b[32m   (...)\u001b[39m\u001b[32m    622\u001b[39m     | Mapping[\u001b[38;5;28mstr\u001b[39m, Runnable[Any, Other] | Callable[[Any], Other] | Any],\n\u001b[32m    623\u001b[39m ) -> RunnableSerializable[Input, Other]:\n\u001b[32m    624\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runnable \"or\" operator.\u001b[39;00m\n\u001b[32m    625\u001b[39m \n\u001b[32m    626\u001b[39m \u001b[33;03m    Compose this `Runnable` with another object to create a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m \u001b[33;03m        A new `Runnable`.\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[38;5;28mself\u001b[39m, \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llm_project_streamlit/venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:6039\u001b[39m, in \u001b[36mcoerce_to_runnable\u001b[39m\u001b[34m(thing)\u001b[39m\n\u001b[32m   6034\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mRunnable[Input, Output]\u001b[39m\u001b[33m\"\u001b[39m, RunnableParallel(thing))\n\u001b[32m   6035\u001b[39m msg = (\n\u001b[32m   6036\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Runnable, callable or dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6037\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   6038\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m6039\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
            "\u001b[31mTypeError\u001b[39m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
          ]
        }
      ],
      "source": [
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever  # –ò–º–ø–æ—Ä—Ç –¥–ª—è history-aware retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ vector_db —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–∞—à –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å)\n",
        "# llm = ChatMistralAI(api_key=\"–≤–∞—à_–∫–ª—é—á\", model=\"mistral-medium\")  # –í–∞—à LLM\n",
        "\n",
        "# –®–∞–≥ 1: –°–æ–∑–¥–∞—Ç—å history-aware retriever\n",
        "# –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏\n",
        "retriever_prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),  # –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Query: {input}\"),\n",
        "])\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º retriever, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm=\"mistral-large-latest\",  # –¢–æ—Ç –∂–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞\n",
        "    retriever=vector_db.as_retriever(),  # –í–∞—à –±–∞–∑–æ–≤—ã–π retriever\n",
        "    prompt=retriever_prompt\n",
        ")\n",
        "\n",
        "# –®–∞–≥ 2: –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (—Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),  # –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞\n",
        "    (\"system\", \"You are a helpful assistant. Use the following context to answer the user's question.\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    (\"user\", \"Question: {input}\"),\n",
        "])\n",
        "\n",
        "# –®–∞–≥ 3: –°–æ–±—Ä–∞—Ç—å RAG-—Ü–µ–ø–æ—á–∫—É —Å –∏—Å—Ç–æ—Ä–∏–µ–π\n",
        "rag_chain_with_history = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=history_aware_retriever  # Retriever –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏\n",
        "    )\n",
        "    | qa_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ —Ü–µ–ø–æ—á–∫–∏\n",
        "# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å messages (—Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç HumanMessage –∏ AIMessage)\n",
        "# –ò input ‚Äî —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "# response = rag_chain_with_history.invoke({\"messages\": messages, \"input\": \"–í–∞—à –≤–æ–ø—Ä–æ—Å\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import create_stuff_documents_chain, create_retrieval_chain  # –î–ª—è stuffing –∏ retrieval chain\n",
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ _get_context_retriever_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è messages\n",
        "from langchain_community.llms import ChatMistralAI  # –í–∞—à LLM\n",
        "from langchain_core.output_parsers import StrOutputParser  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "\n",
        "# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ vector_db —É–∂–µ —Å–æ–∑–¥–∞–Ω–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# vector_db = FAISS.load_local(\"your_index\")  # –ò–ª–∏ –≤–∞—à–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "def get_conversational_rag_chain(llm, vector_db):  # –î–æ–±–∞–≤–∏–ª vector_db –∫–∞–∫ –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏\n",
        "    # –®–∞–≥ 1: –°–æ–∑–¥–∞—Ç—å history-aware retriever (–∞–Ω–∞–ª–æ–≥ _get_context_retriever_chain)\n",
        "    # –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏\n",
        "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),  # –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞\n",
        "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Query: {input}\"),\n",
        "    ])\n",
        "    \n",
        "    # –°–æ–∑–¥–∞—ë–º retriever_chain (history-aware)\n",
        "    retriever_chain = create_history_aware_retriever(\n",
        "        llm=llm,  # LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞\n",
        "        retriever=vector_db.as_retriever(),  # –ë–∞–∑–æ–≤—ã–π retriever –∏–∑ –≤–∞—à–µ–π –ë–î\n",
        "        prompt=retriever_prompt\n",
        "    )\n",
        "    \n",
        "    # –®–∞–≥ 2: –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (—Å –∏—Å—Ç–æ—Ä–∏–µ–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful assistant. You will have to answer to user's queries.\n",
        "You will have some context to help with your answers, but not always would be completely related or helpful.\n",
        "You can also use your knowledge to assist answering the user's queries.\n",
        "Context: {context}\"\"\"),  # {context} –±—É–¥–µ—Ç –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),  # –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞\n",
        "        (\"user\", \"{input}\"),  # –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å\n",
        "    ])\n",
        "    \n",
        "    # –®–∞–≥ 3: –°–æ–∑–¥–∞—Ç—å stuff_documents_chain (–Ω–∞–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø—Ä–æ–º–ø—Ç –∏ –≤—ã–∑—ã–≤–∞–µ—Ç LLM)\n",
        "    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    \n",
        "    # –®–∞–≥ 4: –°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—É—é retrieval chain (retriever + document chain)\n",
        "    rag_chain = create_retrieval_chain(retriever_chain, stuff_documents_chain)\n",
        "    \n",
        "    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä—Å–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —á–∏—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É)\n",
        "    # rag_chain = rag_chain | StrOutputParser()\n",
        "    \n",
        "    return rag_chain\n",
        "\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∏ vector_db (–∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ)\n",
        "llm = ChatMistralAI(api_key=\"–≤–∞—à_–∫–ª—é—á_api_mistral\", model=\"mistral-medium\")  # –ò–ª–∏ –≤–∞—à–∞ –º–æ–¥–µ–ª—å\n",
        "# vector_db = ...  # –í–∞—à–∞ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î, –Ω–∞–ø—Ä–∏–º–µ—Ä, FAISS —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏\n",
        "\n",
        "# –°–æ–∑–¥–∞—Ç—å —Ü–µ–ø–æ—á–∫—É\n",
        "chain = get_conversational_rag_chain(llm, vector_db)\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ (messages) –∏ –≤—ã–∑–æ–≤–∞\n",
        "messages = [\n",
        "    HumanMessage(content=\"–†–∞—Å—Å–∫–∞–∂–∏ –æ LangChain\"),  # –ü—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "    AIMessage(content=\"LangChain ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è LLM-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π...\")  # –û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n",
        "]\n",
        "\n",
        "# –í—ã–∑–æ–≤ —Ü–µ–ø–æ—á–∫–∏\n",
        "response = chain.invoke({\n",
        "    \"messages\": messages,\n",
        "    \"input\": \"–ê –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG –≤ –Ω—ë–º?\"  # –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å\n",
        "})\n",
        "print(response[\"answer\"])  # –í—ã–≤–æ–¥: –û—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –ë–î\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "Dx6fNkf7P1M7",
        "outputId": "3be95509-8cca-42ca-bb87-9d61fb903537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'**Multimodal RAG (MuRAG)** is a retrieval-augmented model that extends traditional RAG (Retrieval-Augmented Generation) by incorporating **both visual and textual knowledge**‚Äîunlike prior text-only methods. It uses a **multimodal transformer** to encode and retrieve information from images and text, augmenting language generation tasks.\\n\\nBuilt on a backbone model (e.g., ViT for visual input and T5 for text), MuRAG processes sequences of image-text pairs, enabling cross-modal retrieval to enhance answers for tasks like **multimodal question answering** (e.g., VQA, OK-VQA). Its focus is on fusing deep features from multiple modalities to improve context-aware generation.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"what is Multimodal RAG?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "MSf3YUxNT2k9",
        "outputId": "67669485-93ad-4138-f48a-942d823b7b5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The article introduces **MuRAG (Multimodal Retrieval-Augmented Generation)**, the first retrieval-augmented model capable of leveraging **both textual and visual knowledge**‚Äîunlike prior text-only RAG models.\\n\\nKey points include:\\n- MuRAG outperforms baselines (e.g., AutoRouting) in multimodal tasks, achieving **higher EM/F1 scores** (e.g., 60.8/67.5 for text, 58.2/58.2 for images).\\n- It uses **multimodal transformers** to fuse visual and textual features, improving cross-modal representation.\\n- Ablation studies show **pre-training on datasets like LAION** significantly boosts performance.\\n- The model excels in tasks like **VQA (72% accuracy)**, image-text retrieval, and text-only QA (55%+ EM).\\n\\nMuRAG demonstrates **strong multi-tasking efficiency**, with **85% RECALL@1** from a 4K multimodal memory.'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"Retell the summary of the article about multimodul RAG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwD5SNmkT2ED",
        "outputId": "76218642-5365-4c3d-88cc-9783a5bba7a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the Ô¨Årst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneÔ¨Åcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='features for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the Ô¨Årst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA‚Äôs\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='fŒ∏(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a ‚Äúbackbone‚Äù model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model‚Äôs encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfŒ∏ and\\ndecoder gŒ∏. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nÔ¨Årst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ‚ààRLi√óD, where Li is the length of the im-'),\n",
              " Document(metadata={'author': '', 'creationdate': '2022-10-21T00:59:50Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2022-10-21T00:59:50Z', 'page': 7, 'page_label': '8', 'producer': 'pdfTeX-1.40.21', 'ptex_fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '/Users/hakimovamavjuda/Desktop/llm_project_streamlit/MulRAG.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': '/False'}, page_content='optical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the Ô¨Årst example, the model is')]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"What is Multimodal RAG?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yvc2LsHEJFfc",
        "outputId": "3e75bbe1-7f55-4b1d-c405-a9d012432af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.5 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 0s (20.7 MB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 121229 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-afr tesseract-ocr-amh tesseract-ocr-ara tesseract-ocr-asm\n",
            "  tesseract-ocr-aze tesseract-ocr-aze-cyrl tesseract-ocr-bel tesseract-ocr-ben\n",
            "  tesseract-ocr-bod tesseract-ocr-bos tesseract-ocr-bre tesseract-ocr-bul\n",
            "  tesseract-ocr-cat tesseract-ocr-ceb tesseract-ocr-ces tesseract-ocr-chi-sim\n",
            "  tesseract-ocr-chi-sim-vert tesseract-ocr-chi-tra tesseract-ocr-chi-tra-vert\n",
            "  tesseract-ocr-chr tesseract-ocr-cos tesseract-ocr-cym tesseract-ocr-dan\n",
            "  tesseract-ocr-deu tesseract-ocr-div tesseract-ocr-dzo tesseract-ocr-ell\n",
            "  tesseract-ocr-enm tesseract-ocr-epo tesseract-ocr-est tesseract-ocr-eus\n",
            "  tesseract-ocr-fao tesseract-ocr-fas tesseract-ocr-fil tesseract-ocr-fin\n",
            "  tesseract-ocr-fra tesseract-ocr-frk tesseract-ocr-frm tesseract-ocr-fry\n",
            "  tesseract-ocr-gla tesseract-ocr-gle tesseract-ocr-glg tesseract-ocr-grc\n",
            "  tesseract-ocr-guj tesseract-ocr-hat tesseract-ocr-heb tesseract-ocr-hin\n",
            "  tesseract-ocr-hrv tesseract-ocr-hun tesseract-ocr-hye tesseract-ocr-iku\n",
            "  tesseract-ocr-ind tesseract-ocr-isl tesseract-ocr-ita tesseract-ocr-ita-old\n",
            "  tesseract-ocr-jav tesseract-ocr-jpn tesseract-ocr-jpn-vert tesseract-ocr-kan\n",
            "  tesseract-ocr-kat tesseract-ocr-kat-old tesseract-ocr-kaz tesseract-ocr-khm\n",
            "  tesseract-ocr-kir tesseract-ocr-kmr tesseract-ocr-kor tesseract-ocr-kor-vert\n",
            "  tesseract-ocr-lao tesseract-ocr-lat tesseract-ocr-lav tesseract-ocr-lit\n",
            "  tesseract-ocr-ltz tesseract-ocr-mal tesseract-ocr-mar tesseract-ocr-mkd\n",
            "  tesseract-ocr-mlt tesseract-ocr-mon tesseract-ocr-mri tesseract-ocr-msa\n",
            "  tesseract-ocr-mya tesseract-ocr-nep tesseract-ocr-nld tesseract-ocr-nor\n",
            "  tesseract-ocr-oci tesseract-ocr-ori tesseract-ocr-pan tesseract-ocr-pol\n",
            "  tesseract-ocr-por tesseract-ocr-pus tesseract-ocr-que tesseract-ocr-ron\n",
            "  tesseract-ocr-rus tesseract-ocr-san tesseract-ocr-script-arab\n",
            "  tesseract-ocr-script-armn tesseract-ocr-script-beng\n",
            "  tesseract-ocr-script-cans tesseract-ocr-script-cher\n",
            "  tesseract-ocr-script-cyrl tesseract-ocr-script-deva\n",
            "  tesseract-ocr-script-ethi tesseract-ocr-script-frak\n",
            "  tesseract-ocr-script-geor tesseract-ocr-script-grek\n",
            "  tesseract-ocr-script-gujr tesseract-ocr-script-guru\n",
            "  tesseract-ocr-script-hang tesseract-ocr-script-hang-vert\n",
            "  tesseract-ocr-script-hans tesseract-ocr-script-hans-vert\n",
            "  tesseract-ocr-script-hant tesseract-ocr-script-hant-vert\n",
            "  tesseract-ocr-script-hebr tesseract-ocr-script-jpan\n",
            "  tesseract-ocr-script-jpan-vert tesseract-ocr-script-khmr\n",
            "  tesseract-ocr-script-knda tesseract-ocr-script-laoo\n",
            "  tesseract-ocr-script-latn tesseract-ocr-script-mlym\n",
            "  tesseract-ocr-script-mymr tesseract-ocr-script-orya\n",
            "  tesseract-ocr-script-sinh tesseract-ocr-script-syrc\n",
            "  tesseract-ocr-script-taml tesseract-ocr-script-telu\n",
            "  tesseract-ocr-script-thaa tesseract-ocr-script-thai\n",
            "  tesseract-ocr-script-tibt tesseract-ocr-script-viet tesseract-ocr-sin\n",
            "  tesseract-ocr-slk tesseract-ocr-slv tesseract-ocr-snd tesseract-ocr-spa\n",
            "  tesseract-ocr-spa-old tesseract-ocr-sqi tesseract-ocr-srp\n",
            "  tesseract-ocr-srp-latn tesseract-ocr-sun tesseract-ocr-swa tesseract-ocr-swe\n",
            "  tesseract-ocr-syr tesseract-ocr-tam tesseract-ocr-tat tesseract-ocr-tel\n",
            "  tesseract-ocr-tgk tesseract-ocr-tha tesseract-ocr-tir tesseract-ocr-ton\n",
            "  tesseract-ocr-tur tesseract-ocr-uig tesseract-ocr-ukr tesseract-ocr-urd\n",
            "  tesseract-ocr-uzb tesseract-ocr-uzb-cyrl tesseract-ocr-vie tesseract-ocr-yid\n",
            "  tesseract-ocr-yor\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-afr tesseract-ocr-all tesseract-ocr-amh tesseract-ocr-ara\n",
            "  tesseract-ocr-asm tesseract-ocr-aze tesseract-ocr-aze-cyrl tesseract-ocr-bel\n",
            "  tesseract-ocr-ben tesseract-ocr-bod tesseract-ocr-bos tesseract-ocr-bre\n",
            "  tesseract-ocr-bul tesseract-ocr-cat tesseract-ocr-ceb tesseract-ocr-ces\n",
            "  tesseract-ocr-chi-sim tesseract-ocr-chi-sim-vert tesseract-ocr-chi-tra\n",
            "  tesseract-ocr-chi-tra-vert tesseract-ocr-chr tesseract-ocr-cos\n",
            "  tesseract-ocr-cym tesseract-ocr-dan tesseract-ocr-deu tesseract-ocr-div\n",
            "  tesseract-ocr-dzo tesseract-ocr-ell tesseract-ocr-enm tesseract-ocr-epo\n",
            "  tesseract-ocr-est tesseract-ocr-eus tesseract-ocr-fao tesseract-ocr-fas\n",
            "  tesseract-ocr-fil tesseract-ocr-fin tesseract-ocr-fra tesseract-ocr-frk\n",
            "  tesseract-ocr-frm tesseract-ocr-fry tesseract-ocr-gla tesseract-ocr-gle\n",
            "  tesseract-ocr-glg tesseract-ocr-grc tesseract-ocr-guj tesseract-ocr-hat\n",
            "  tesseract-ocr-heb tesseract-ocr-hin tesseract-ocr-hrv tesseract-ocr-hun\n",
            "  tesseract-ocr-hye tesseract-ocr-iku tesseract-ocr-ind tesseract-ocr-isl\n",
            "  tesseract-ocr-ita tesseract-ocr-ita-old tesseract-ocr-jav tesseract-ocr-jpn\n",
            "  tesseract-ocr-jpn-vert tesseract-ocr-kan tesseract-ocr-kat\n",
            "  tesseract-ocr-kat-old tesseract-ocr-kaz tesseract-ocr-khm tesseract-ocr-kir\n",
            "  tesseract-ocr-kmr tesseract-ocr-kor tesseract-ocr-kor-vert tesseract-ocr-lao\n",
            "  tesseract-ocr-lat tesseract-ocr-lav tesseract-ocr-lit tesseract-ocr-ltz\n",
            "  tesseract-ocr-mal tesseract-ocr-mar tesseract-ocr-mkd tesseract-ocr-mlt\n",
            "  tesseract-ocr-mon tesseract-ocr-mri tesseract-ocr-msa tesseract-ocr-mya\n",
            "  tesseract-ocr-nep tesseract-ocr-nld tesseract-ocr-nor tesseract-ocr-oci\n",
            "  tesseract-ocr-ori tesseract-ocr-pan tesseract-ocr-pol tesseract-ocr-por\n",
            "  tesseract-ocr-pus tesseract-ocr-que tesseract-ocr-ron tesseract-ocr-rus\n",
            "  tesseract-ocr-san tesseract-ocr-script-arab tesseract-ocr-script-armn\n",
            "  tesseract-ocr-script-beng tesseract-ocr-script-cans\n",
            "  tesseract-ocr-script-cher tesseract-ocr-script-cyrl\n",
            "  tesseract-ocr-script-deva tesseract-ocr-script-ethi\n",
            "  tesseract-ocr-script-frak tesseract-ocr-script-geor\n",
            "  tesseract-ocr-script-grek tesseract-ocr-script-gujr\n",
            "  tesseract-ocr-script-guru tesseract-ocr-script-hang\n",
            "  tesseract-ocr-script-hang-vert tesseract-ocr-script-hans\n",
            "  tesseract-ocr-script-hans-vert tesseract-ocr-script-hant\n",
            "  tesseract-ocr-script-hant-vert tesseract-ocr-script-hebr\n",
            "  tesseract-ocr-script-jpan tesseract-ocr-script-jpan-vert\n",
            "  tesseract-ocr-script-khmr tesseract-ocr-script-knda\n",
            "  tesseract-ocr-script-laoo tesseract-ocr-script-latn\n",
            "  tesseract-ocr-script-mlym tesseract-ocr-script-mymr\n",
            "  tesseract-ocr-script-orya tesseract-ocr-script-sinh\n",
            "  tesseract-ocr-script-syrc tesseract-ocr-script-taml\n",
            "  tesseract-ocr-script-telu tesseract-ocr-script-thaa\n",
            "  tesseract-ocr-script-thai tesseract-ocr-script-tibt\n",
            "  tesseract-ocr-script-viet tesseract-ocr-sin tesseract-ocr-slk\n",
            "  tesseract-ocr-slv tesseract-ocr-snd tesseract-ocr-spa tesseract-ocr-spa-old\n",
            "  tesseract-ocr-sqi tesseract-ocr-srp tesseract-ocr-srp-latn tesseract-ocr-sun\n",
            "  tesseract-ocr-swa tesseract-ocr-swe tesseract-ocr-syr tesseract-ocr-tam\n",
            "  tesseract-ocr-tat tesseract-ocr-tel tesseract-ocr-tgk tesseract-ocr-tha\n",
            "  tesseract-ocr-tir tesseract-ocr-ton tesseract-ocr-tur tesseract-ocr-uig\n",
            "  tesseract-ocr-ukr tesseract-ocr-urd tesseract-ocr-uzb tesseract-ocr-uzb-cyrl\n",
            "  tesseract-ocr-vie tesseract-ocr-yid tesseract-ocr-yor\n",
            "0 upgraded, 160 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 281 MB of archives.\n",
            "After this operation, 686 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-afr all 1:4.00~git30-7274cfa-1.1 [1,731 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bul all 1:4.00~git30-7274cfa-1.1 [678 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-cat all 1:4.00~git30-7274cfa-1.1 [579 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ces all 1:4.00~git30-7274cfa-1.1 [1,408 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-dan all 1:4.00~git30-7274cfa-1.1 [1,026 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-deu all 1:4.00~git30-7274cfa-1.1 [744 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ell all 1:4.00~git30-7274cfa-1.1 [594 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fin all 1:4.00~git30-7274cfa-1.1 [3,030 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fra all 1:4.00~git30-7274cfa-1.1 [527 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hun all 1:4.00~git30-7274cfa-1.1 [1,853 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ind all 1:4.00~git30-7274cfa-1.1 [537 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ita all 1:4.00~git30-7274cfa-1.1 [1,067 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lav all 1:4.00~git30-7274cfa-1.1 [981 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lit all 1:4.00~git30-7274cfa-1.1 [1,140 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-nld all 1:4.00~git30-7274cfa-1.1 [2,306 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-nor all 1:4.00~git30-7274cfa-1.1 [1,748 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pol all 1:4.00~git30-7274cfa-1.1 [1,610 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-por all 1:4.00~git30-7274cfa-1.1 [856 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ron all 1:4.00~git30-7274cfa-1.1 [896 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-rus all 1:4.00~git30-7274cfa-1.1 [1,271 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-slk all 1:4.00~git30-7274cfa-1.1 [1,516 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-slv all 1:4.00~git30-7274cfa-1.1 [998 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-spa all 1:4.00~git30-7274cfa-1.1 [951 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-srp all 1:4.00~git30-7274cfa-1.1 [780 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-swe all 1:4.00~git30-7274cfa-1.1 [2,232 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tur all 1:4.00~git30-7274cfa-1.1 [1,578 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ukr all 1:4.00~git30-7274cfa-1.1 [1,303 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-vie all 1:4.00~git30-7274cfa-1.1 [417 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-sim all 1:4.00~git30-7274cfa-1.1 [1,634 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-tra all 1:4.00~git30-7274cfa-1.1 [1,586 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-amh all 1:4.00~git30-7274cfa-1.1 [1,856 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-asm all 1:4.00~git30-7274cfa-1.1 [1,421 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-aze-cyrl all 1:4.00~git30-7274cfa-1.1 [852 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bod all 1:4.00~git30-7274cfa-1.1 [1,180 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bos all 1:4.00~git30-7274cfa-1.1 [965 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ceb all 1:4.00~git30-7274cfa-1.1 [469 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-cym all 1:4.00~git30-7274cfa-1.1 [1,260 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-dzo all 1:4.00~git30-7274cfa-1.1 [390 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fas all 1:4.00~git30-7274cfa-1.1 [301 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-gle all 1:4.00~git30-7274cfa-1.1 [613 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-guj all 1:4.00~git30-7274cfa-1.1 [660 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hat all 1:4.00~git30-7274cfa-1.1 [1,487 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-iku all 1:4.00~git30-7274cfa-1.1 [1,211 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-jav all 1:4.00~git30-7274cfa-1.1 [1,443 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kat all 1:4.00~git30-7274cfa-1.1 [884 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kat-old all 1:4.00~git30-7274cfa-1.1 [380 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kaz all 1:4.00~git30-7274cfa-1.1 [1,687 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-khm all 1:4.00~git30-7274cfa-1.1 [1,032 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kir all 1:4.00~git30-7274cfa-1.1 [3,277 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lao all 1:4.00~git30-7274cfa-1.1 [2,411 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-lat all 1:4.00~git30-7274cfa-1.1 [1,536 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mar all 1:4.00~git30-7274cfa-1.1 [862 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mya all 1:4.00~git30-7274cfa-1.1 [2,358 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-nep all 1:4.00~git30-7274cfa-1.1 [478 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ori all 1:4.00~git30-7274cfa-1.1 [1,024 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pan all 1:4.00~git30-7274cfa-1.1 [322 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pus all 1:4.00~git30-7274cfa-1.1 [1,415 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-san all 1:4.00~git30-7274cfa-1.1 [4,228 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sin all 1:4.00~git30-7274cfa-1.1 [1,085 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-srp-latn all 1:4.00~git30-7274cfa-1.1 [1,558 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-syr all 1:4.00~git30-7274cfa-1.1 [1,558 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tgk all 1:4.00~git30-7274cfa-1.1 [949 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tir all 1:4.00~git30-7274cfa-1.1 [297 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-uig all 1:4.00~git30-7274cfa-1.1 [1,743 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-urd all 1:4.00~git30-7274cfa-1.1 [1,000 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-uzb-cyrl all 1:4.00~git30-7274cfa-1.1 [730 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-uzb all 1:4.00~git30-7274cfa-1.1 [2,535 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-yid all 1:4.00~git30-7274cfa-1.1 [345 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ara all 1:4.00~git30-7274cfa-1.1 [645 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-aze all 1:4.00~git30-7274cfa-1.1 [1,342 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bel all 1:4.00~git30-7274cfa-1.1 [1,190 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ben all 1:4.00~git30-7274cfa-1.1 [516 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chr all 1:4.00~git30-7274cfa-1.1 [287 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-enm all 1:4.00~git30-7274cfa-1.1 [1,845 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-epo all 1:4.00~git30-7274cfa-1.1 [1,709 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-est all 1:4.00~git30-7274cfa-1.1 [1,586 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eus all 1:4.00~git30-7274cfa-1.1 [1,765 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-frk all 1:4.00~git30-7274cfa-1.1 [2,730 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-frm all 1:4.00~git30-7274cfa-1.1 [830 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-glg all 1:4.00~git30-7274cfa-1.1 [1,656 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-heb all 1:4.00~git30-7274cfa-1.1 [432 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hin all 1:4.00~git30-7274cfa-1.1 [913 kB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hrv all 1:4.00~git30-7274cfa-1.1 [1,439 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-isl all 1:4.00~git30-7274cfa-1.1 [913 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ita-old all 1:4.00~git30-7274cfa-1.1 [1,567 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-jpn all 1:4.00~git30-7274cfa-1.1 [1,390 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kan all 1:4.00~git30-7274cfa-1.1 [1,659 kB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kor all 1:4.00~git30-7274cfa-1.1 [1,052 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mal all 1:4.00~git30-7274cfa-1.1 [1,678 kB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mkd all 1:4.00~git30-7274cfa-1.1 [718 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mlt all 1:4.00~git30-7274cfa-1.1 [975 kB]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-msa all 1:4.00~git30-7274cfa-1.1 [1,114 kB]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-spa-old all 1:4.00~git30-7274cfa-1.1 [1,474 kB]\n",
            "Get:94 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sqi all 1:4.00~git30-7274cfa-1.1 [720 kB]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-swa all 1:4.00~git30-7274cfa-1.1 [919 kB]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tam all 1:4.00~git30-7274cfa-1.1 [1,071 kB]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tel all 1:4.00~git30-7274cfa-1.1 [1,012 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tha all 1:4.00~git30-7274cfa-1.1 [899 kB]\n",
            "Get:99 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-bre all 1:4.00~git30-7274cfa-1.1 [2,861 kB]\n",
            "Get:100 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-sim-vert all 1:4.00~git30-7274cfa-1.1 [1,134 kB]\n",
            "Get:101 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-tra-vert all 1:4.00~git30-7274cfa-1.1 [1,091 kB]\n",
            "Get:102 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-cos all 1:4.00~git30-7274cfa-1.1 [1,263 kB]\n",
            "Get:103 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-div all 1:4.00~git30-7274cfa-1.1 [762 kB]\n",
            "Get:104 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fao all 1:4.00~git30-7274cfa-1.1 [1,667 kB]\n",
            "Get:105 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fil all 1:4.00~git30-7274cfa-1.1 [760 kB]\n",
            "Get:106 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fry all 1:4.00~git30-7274cfa-1.1 [1,177 kB]\n",
            "Get:107 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-gla all 1:4.00~git30-7274cfa-1.1 [1,541 kB]\n",
            "Get:108 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hye all 1:4.00~git30-7274cfa-1.1 [1,191 kB]\n",
            "Get:109 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-jpn-vert all 1:4.00~git30-7274cfa-1.1 [1,889 kB]\n",
            "Get:110 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kor-vert all 1:4.00~git30-7274cfa-1.1 [546 kB]\n",
            "Get:111 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kmr all 1:4.00~git30-7274cfa-1.1 [1,666 kB]\n",
            "Get:112 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ltz all 1:4.00~git30-7274cfa-1.1 [1,720 kB]\n",
            "Get:113 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mon all 1:4.00~git30-7274cfa-1.1 [1,216 kB]\n",
            "Get:114 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mri all 1:4.00~git30-7274cfa-1.1 [514 kB]\n",
            "Get:115 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-oci all 1:4.00~git30-7274cfa-1.1 [2,596 kB]\n",
            "Get:116 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-que all 1:4.00~git30-7274cfa-1.1 [2,136 kB]\n",
            "Get:117 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-snd all 1:4.00~git30-7274cfa-1.1 [1,402 kB]\n",
            "Get:118 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sun all 1:4.00~git30-7274cfa-1.1 [679 kB]\n",
            "Get:119 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tat all 1:4.00~git30-7274cfa-1.1 [902 kB]\n",
            "Get:120 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ton all 1:4.00~git30-7274cfa-1.1 [547 kB]\n",
            "Get:121 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-yor all 1:4.00~git30-7274cfa-1.1 [551 kB]\n",
            "Get:122 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-arab all 1:4.00~git30-7274cfa-1.1 [3,214 kB]\n",
            "Get:123 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-armn all 1:4.00~git30-7274cfa-1.1 [3,094 kB]\n",
            "Get:124 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-beng all 1:4.00~git30-7274cfa-1.1 [2,447 kB]\n",
            "Get:125 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-cans all 1:4.00~git30-7274cfa-1.1 [3,101 kB]\n",
            "Get:126 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-cher all 1:4.00~git30-7274cfa-1.1 [1,640 kB]\n",
            "Get:127 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-cyrl all 1:4.00~git30-7274cfa-1.1 [9,350 kB]\n",
            "Get:128 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-deva all 1:4.00~git30-7274cfa-1.1 [6,119 kB]\n",
            "Get:129 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-ethi all 1:4.00~git30-7274cfa-1.1 [3,034 kB]\n",
            "Get:130 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-frak all 1:4.00~git30-7274cfa-1.1 [4,148 kB]\n",
            "Get:131 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-geor all 1:4.00~git30-7274cfa-1.1 [2,828 kB]\n",
            "Get:132 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-grek all 1:4.00~git30-7274cfa-1.1 [1,076 kB]\n",
            "Get:133 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-gujr all 1:4.00~git30-7274cfa-1.1 [1,809 kB]\n",
            "Get:134 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-guru all 1:4.00~git30-7274cfa-1.1 [1,556 kB]\n",
            "Get:135 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hans all 1:4.00~git30-7274cfa-1.1 [2,860 kB]\n",
            "Get:136 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hans-vert all 1:4.00~git30-7274cfa-1.1 [2,367 kB]\n",
            "Get:137 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hant all 1:4.00~git30-7274cfa-1.1 [2,362 kB]\n",
            "Get:138 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hant-vert all 1:4.00~git30-7274cfa-1.1 [2,352 kB]\n",
            "Get:139 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hang all 1:4.00~git30-7274cfa-1.1 [1,853 kB]\n",
            "Get:140 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hang-vert all 1:4.00~git30-7274cfa-1.1 [1,753 kB]\n",
            "Get:141 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-hebr all 1:4.00~git30-7274cfa-1.1 [1,743 kB]\n",
            "Get:142 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-jpan all 1:4.00~git30-7274cfa-1.1 [2,585 kB]\n",
            "Get:143 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-jpan-vert all 1:4.00~git30-7274cfa-1.1 [3,084 kB]\n",
            "Get:144 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-knda all 1:4.00~git30-7274cfa-1.1 [2,474 kB]\n",
            "Get:145 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-khmr all 1:4.00~git30-7274cfa-1.1 [1,663 kB]\n",
            "Get:146 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-laoo all 1:4.00~git30-7274cfa-1.1 [3,664 kB]\n",
            "Get:147 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-latn all 1:4.00~git30-7274cfa-1.1 [30.9 MB]\n",
            "Get:148 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-mlym all 1:4.00~git30-7274cfa-1.1 [2,981 kB]\n",
            "Get:149 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-mymr all 1:4.00~git30-7274cfa-1.1 [3,191 kB]\n",
            "Get:150 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-orya all 1:4.00~git30-7274cfa-1.1 [2,768 kB]\n",
            "Get:151 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-sinh all 1:4.00~git30-7274cfa-1.1 [1,753 kB]\n",
            "Get:152 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-syrc all 1:4.00~git30-7274cfa-1.1 [2,698 kB]\n",
            "Get:153 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-taml all 1:4.00~git30-7274cfa-1.1 [2,497 kB]\n",
            "Get:154 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-telu all 1:4.00~git30-7274cfa-1.1 [2,319 kB]\n",
            "Get:155 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-thaa all 1:4.00~git30-7274cfa-1.1 [2,518 kB]\n",
            "Get:156 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-thai all 1:4.00~git30-7274cfa-1.1 [1,642 kB]\n",
            "Get:157 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-tibt all 1:4.00~git30-7274cfa-1.1 [2,453 kB]\n",
            "Get:158 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-viet all 1:4.00~git30-7274cfa-1.1 [1,430 kB]\n",
            "Get:159 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-grc all 1:4.00~git30-7274cfa-1.1 [916 kB]\n",
            "Get:160 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-all all 4.1.1-2.1build1 [2,040 B]\n",
            "Fetched 281 MB in 4s (65.8 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package tesseract-ocr-afr.\n",
            "(Reading database ... 121362 files and directories currently installed.)\n",
            "Preparing to unpack .../000-tesseract-ocr-afr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-afr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bul.\n",
            "Preparing to unpack .../001-tesseract-ocr-bul_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bul (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-cat.\n",
            "Preparing to unpack .../002-tesseract-ocr-cat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-cat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ces.\n",
            "Preparing to unpack .../003-tesseract-ocr-ces_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ces (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-dan.\n",
            "Preparing to unpack .../004-tesseract-ocr-dan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-dan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-deu.\n",
            "Preparing to unpack .../005-tesseract-ocr-deu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ell.\n",
            "Preparing to unpack .../006-tesseract-ocr-ell_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ell (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fin.\n",
            "Preparing to unpack .../007-tesseract-ocr-fin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fra.\n",
            "Preparing to unpack .../008-tesseract-ocr-fra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hun.\n",
            "Preparing to unpack .../009-tesseract-ocr-hun_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ind.\n",
            "Preparing to unpack .../010-tesseract-ocr-ind_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ind (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ita.\n",
            "Preparing to unpack .../011-tesseract-ocr-ita_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ita (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lav.\n",
            "Preparing to unpack .../012-tesseract-ocr-lav_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lit.\n",
            "Preparing to unpack .../013-tesseract-ocr-lit_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lit (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-nld.\n",
            "Preparing to unpack .../014-tesseract-ocr-nld_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-nld (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-nor.\n",
            "Preparing to unpack .../015-tesseract-ocr-nor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-nor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-pol.\n",
            "Preparing to unpack .../016-tesseract-ocr-pol_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-pol (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-por.\n",
            "Preparing to unpack .../017-tesseract-ocr-por_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ron.\n",
            "Preparing to unpack .../018-tesseract-ocr-ron_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ron (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-rus.\n",
            "Preparing to unpack .../019-tesseract-ocr-rus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-slk.\n",
            "Preparing to unpack .../020-tesseract-ocr-slk_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-slk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-slv.\n",
            "Preparing to unpack .../021-tesseract-ocr-slv_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-slv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-spa.\n",
            "Preparing to unpack .../022-tesseract-ocr-spa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-srp.\n",
            "Preparing to unpack .../023-tesseract-ocr-srp_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-srp (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-swe.\n",
            "Preparing to unpack .../024-tesseract-ocr-swe_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-swe (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tur.\n",
            "Preparing to unpack .../025-tesseract-ocr-tur_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tur (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ukr.\n",
            "Preparing to unpack .../026-tesseract-ocr-ukr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ukr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-vie.\n",
            "Preparing to unpack .../027-tesseract-ocr-vie_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-vie (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-sim.\n",
            "Preparing to unpack .../028-tesseract-ocr-chi-sim_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-tra.\n",
            "Preparing to unpack .../029-tesseract-ocr-chi-tra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-amh.\n",
            "Preparing to unpack .../030-tesseract-ocr-amh_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-amh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-asm.\n",
            "Preparing to unpack .../031-tesseract-ocr-asm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-aze-cyrl.\n",
            "Preparing to unpack .../032-tesseract-ocr-aze-cyrl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-aze-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bod.\n",
            "Preparing to unpack .../033-tesseract-ocr-bod_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bod (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bos.\n",
            "Preparing to unpack .../034-tesseract-ocr-bos_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ceb.\n",
            "Preparing to unpack .../035-tesseract-ocr-ceb_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ceb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-cym.\n",
            "Preparing to unpack .../036-tesseract-ocr-cym_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-cym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-dzo.\n",
            "Preparing to unpack .../037-tesseract-ocr-dzo_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-dzo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fas.\n",
            "Preparing to unpack .../038-tesseract-ocr-fas_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fas (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-gle.\n",
            "Preparing to unpack .../039-tesseract-ocr-gle_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-gle (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-guj.\n",
            "Preparing to unpack .../040-tesseract-ocr-guj_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-guj (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hat.\n",
            "Preparing to unpack .../041-tesseract-ocr-hat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-iku.\n",
            "Preparing to unpack .../042-tesseract-ocr-iku_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-iku (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-jav.\n",
            "Preparing to unpack .../043-tesseract-ocr-jav_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-jav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kat.\n",
            "Preparing to unpack .../044-tesseract-ocr-kat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kat-old.\n",
            "Preparing to unpack .../045-tesseract-ocr-kat-old_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kat-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kaz.\n",
            "Preparing to unpack .../046-tesseract-ocr-kaz_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kaz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-khm.\n",
            "Preparing to unpack .../047-tesseract-ocr-khm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-khm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kir.\n",
            "Preparing to unpack .../048-tesseract-ocr-kir_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lao.\n",
            "Preparing to unpack .../049-tesseract-ocr-lao_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-lat.\n",
            "Preparing to unpack .../050-tesseract-ocr-lat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-lat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mar.\n",
            "Preparing to unpack .../051-tesseract-ocr-mar_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mya.\n",
            "Preparing to unpack .../052-tesseract-ocr-mya_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-nep.\n",
            "Preparing to unpack .../053-tesseract-ocr-nep_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-nep (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ori.\n",
            "Preparing to unpack .../054-tesseract-ocr-ori_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-pan.\n",
            "Preparing to unpack .../055-tesseract-ocr-pan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-pus.\n",
            "Preparing to unpack .../056-tesseract-ocr-pus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-pus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-san.\n",
            "Preparing to unpack .../057-tesseract-ocr-san_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-san (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-sin.\n",
            "Preparing to unpack .../058-tesseract-ocr-sin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-srp-latn.\n",
            "Preparing to unpack .../059-tesseract-ocr-srp-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-srp-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-syr.\n",
            "Preparing to unpack .../060-tesseract-ocr-syr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-syr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tgk.\n",
            "Preparing to unpack .../061-tesseract-ocr-tgk_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tgk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tir.\n",
            "Preparing to unpack .../062-tesseract-ocr-tir_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-uig.\n",
            "Preparing to unpack .../063-tesseract-ocr-uig_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-uig (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-urd.\n",
            "Preparing to unpack .../064-tesseract-ocr-urd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-uzb-cyrl.\n",
            "Preparing to unpack .../065-tesseract-ocr-uzb-cyrl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-uzb-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-uzb.\n",
            "Preparing to unpack .../066-tesseract-ocr-uzb_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-uzb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-yid.\n",
            "Preparing to unpack .../067-tesseract-ocr-yid_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-yid (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ara.\n",
            "Preparing to unpack .../068-tesseract-ocr-ara_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-aze.\n",
            "Preparing to unpack .../069-tesseract-ocr-aze_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-aze (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bel.\n",
            "Preparing to unpack .../070-tesseract-ocr-bel_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ben.\n",
            "Preparing to unpack .../071-tesseract-ocr-ben_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chr.\n",
            "Preparing to unpack .../072-tesseract-ocr-chr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-enm.\n",
            "Preparing to unpack .../073-tesseract-ocr-enm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-enm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-epo.\n",
            "Preparing to unpack .../074-tesseract-ocr-epo_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-epo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-est.\n",
            "Preparing to unpack .../075-tesseract-ocr-est_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-est (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-eus.\n",
            "Preparing to unpack .../076-tesseract-ocr-eus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-frk.\n",
            "Preparing to unpack .../077-tesseract-ocr-frk_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-frk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-frm.\n",
            "Preparing to unpack .../078-tesseract-ocr-frm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-frm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-glg.\n",
            "Preparing to unpack .../079-tesseract-ocr-glg_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-glg (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-heb.\n",
            "Preparing to unpack .../080-tesseract-ocr-heb_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-heb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hin.\n",
            "Preparing to unpack .../081-tesseract-ocr-hin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hrv.\n",
            "Preparing to unpack .../082-tesseract-ocr-hrv_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hrv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-isl.\n",
            "Preparing to unpack .../083-tesseract-ocr-isl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-isl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ita-old.\n",
            "Preparing to unpack .../084-tesseract-ocr-ita-old_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ita-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-jpn.\n",
            "Preparing to unpack .../085-tesseract-ocr-jpn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-jpn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kan.\n",
            "Preparing to unpack .../086-tesseract-ocr-kan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kor.\n",
            "Preparing to unpack .../087-tesseract-ocr-kor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mal.\n",
            "Preparing to unpack .../088-tesseract-ocr-mal_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mkd.\n",
            "Preparing to unpack .../089-tesseract-ocr-mkd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mkd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mlt.\n",
            "Preparing to unpack .../090-tesseract-ocr-mlt_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mlt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-msa.\n",
            "Preparing to unpack .../091-tesseract-ocr-msa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-msa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-spa-old.\n",
            "Preparing to unpack .../092-tesseract-ocr-spa-old_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-spa-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-sqi.\n",
            "Preparing to unpack .../093-tesseract-ocr-sqi_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sqi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-swa.\n",
            "Preparing to unpack .../094-tesseract-ocr-swa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-swa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tam.\n",
            "Preparing to unpack .../095-tesseract-ocr-tam_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tel.\n",
            "Preparing to unpack .../096-tesseract-ocr-tel_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tha.\n",
            "Preparing to unpack .../097-tesseract-ocr-tha_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tha (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-bre.\n",
            "Preparing to unpack .../098-tesseract-ocr-bre_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-bre (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-sim-vert.\n",
            "Preparing to unpack .../099-tesseract-ocr-chi-sim-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-sim-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-chi-tra-vert.\n",
            "Preparing to unpack .../100-tesseract-ocr-chi-tra-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-tra-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-cos.\n",
            "Preparing to unpack .../101-tesseract-ocr-cos_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-cos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-div.\n",
            "Preparing to unpack .../102-tesseract-ocr-div_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-div (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fao.\n",
            "Preparing to unpack .../103-tesseract-ocr-fao_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fil.\n",
            "Preparing to unpack .../104-tesseract-ocr-fil_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fil (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fry.\n",
            "Preparing to unpack .../105-tesseract-ocr-fry_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fry (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-gla.\n",
            "Preparing to unpack .../106-tesseract-ocr-gla_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-gla (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-hye.\n",
            "Preparing to unpack .../107-tesseract-ocr-hye_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-hye (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-jpn-vert.\n",
            "Preparing to unpack .../108-tesseract-ocr-jpn-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-jpn-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kor-vert.\n",
            "Preparing to unpack .../109-tesseract-ocr-kor-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kor-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-kmr.\n",
            "Preparing to unpack .../110-tesseract-ocr-kmr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-kmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ltz.\n",
            "Preparing to unpack .../111-tesseract-ocr-ltz_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ltz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mon.\n",
            "Preparing to unpack .../112-tesseract-ocr-mon_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mon (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-mri.\n",
            "Preparing to unpack .../113-tesseract-ocr-mri_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-mri (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-oci.\n",
            "Preparing to unpack .../114-tesseract-ocr-oci_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-oci (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-que.\n",
            "Preparing to unpack .../115-tesseract-ocr-que_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-que (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-snd.\n",
            "Preparing to unpack .../116-tesseract-ocr-snd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-snd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-sun.\n",
            "Preparing to unpack .../117-tesseract-ocr-sun_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tat.\n",
            "Preparing to unpack .../118-tesseract-ocr-tat_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-ton.\n",
            "Preparing to unpack .../119-tesseract-ocr-ton_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ton (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-yor.\n",
            "Preparing to unpack .../120-tesseract-ocr-yor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-yor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-arab.\n",
            "Preparing to unpack .../121-tesseract-ocr-script-arab_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-arab (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-armn.\n",
            "Preparing to unpack .../122-tesseract-ocr-script-armn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-armn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-beng.\n",
            "Preparing to unpack .../123-tesseract-ocr-script-beng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-beng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-cans.\n",
            "Preparing to unpack .../124-tesseract-ocr-script-cans_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-cans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-cher.\n",
            "Preparing to unpack .../125-tesseract-ocr-script-cher_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-cher (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-cyrl.\n",
            "Preparing to unpack .../126-tesseract-ocr-script-cyrl_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-deva.\n",
            "Preparing to unpack .../127-tesseract-ocr-script-deva_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-deva (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-ethi.\n",
            "Preparing to unpack .../128-tesseract-ocr-script-ethi_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-ethi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-frak.\n",
            "Preparing to unpack .../129-tesseract-ocr-script-frak_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-frak (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-geor.\n",
            "Preparing to unpack .../130-tesseract-ocr-script-geor_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-geor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-grek.\n",
            "Preparing to unpack .../131-tesseract-ocr-script-grek_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-grek (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-gujr.\n",
            "Preparing to unpack .../132-tesseract-ocr-script-gujr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-gujr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-guru.\n",
            "Preparing to unpack .../133-tesseract-ocr-script-guru_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-guru (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hans.\n",
            "Preparing to unpack .../134-tesseract-ocr-script-hans_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hans-vert.\n",
            "Preparing to unpack .../135-tesseract-ocr-script-hans-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hans-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hant.\n",
            "Preparing to unpack .../136-tesseract-ocr-script-hant_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hant (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hant-vert.\n",
            "Preparing to unpack .../137-tesseract-ocr-script-hant-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hant-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hang.\n",
            "Preparing to unpack .../138-tesseract-ocr-script-hang_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hang (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hang-vert.\n",
            "Preparing to unpack .../139-tesseract-ocr-script-hang-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hang-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-hebr.\n",
            "Preparing to unpack .../140-tesseract-ocr-script-hebr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-hebr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-jpan.\n",
            "Preparing to unpack .../141-tesseract-ocr-script-jpan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-jpan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-jpan-vert.\n",
            "Preparing to unpack .../142-tesseract-ocr-script-jpan-vert_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-jpan-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-knda.\n",
            "Preparing to unpack .../143-tesseract-ocr-script-knda_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-knda (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-khmr.\n",
            "Preparing to unpack .../144-tesseract-ocr-script-khmr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-khmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-laoo.\n",
            "Preparing to unpack .../145-tesseract-ocr-script-laoo_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-laoo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-latn.\n",
            "Preparing to unpack .../146-tesseract-ocr-script-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-mlym.\n",
            "Preparing to unpack .../147-tesseract-ocr-script-mlym_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-mlym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-mymr.\n",
            "Preparing to unpack .../148-tesseract-ocr-script-mymr_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-mymr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-orya.\n",
            "Preparing to unpack .../149-tesseract-ocr-script-orya_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-orya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-sinh.\n",
            "Preparing to unpack .../150-tesseract-ocr-script-sinh_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-sinh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-syrc.\n",
            "Preparing to unpack .../151-tesseract-ocr-script-syrc_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-syrc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-taml.\n",
            "Preparing to unpack .../152-tesseract-ocr-script-taml_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-taml (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-telu.\n",
            "Preparing to unpack .../153-tesseract-ocr-script-telu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-telu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-thaa.\n",
            "Preparing to unpack .../154-tesseract-ocr-script-thaa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-thaa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-thai.\n",
            "Preparing to unpack .../155-tesseract-ocr-script-thai_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-thai (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-tibt.\n",
            "Preparing to unpack .../156-tesseract-ocr-script-tibt_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-tibt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-viet.\n",
            "Preparing to unpack .../157-tesseract-ocr-script-viet_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-viet (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-grc.\n",
            "Preparing to unpack .../158-tesseract-ocr-grc_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-grc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-all.\n",
            "Preparing to unpack .../159-tesseract-ocr-all_4.1.1-2.1build1_all.deb ...\n",
            "Unpacking tesseract-ocr-all (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-jav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-guru (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hang (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-thaa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-tra-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-cos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kaz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hant (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hant-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-swa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-heb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mlt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fry (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-nld (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-afr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ces (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-sinh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-san (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-beng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ton (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-guj (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-orya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ceb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-vie (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-cans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-slv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lit (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bul (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-iku (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-mlym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hans (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mya (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fas (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-dan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-epo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-nep (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-grc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mon (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kat-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tha (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-swe (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bos (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-grek (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ell (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-frm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ita-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-amh (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-gle (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ita (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ron (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-isl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-aze-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bod (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-syrc (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-cym (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-bre (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-pol (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hebr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-laoo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-jpn-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-cat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-glg (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-deva (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-slk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-que (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tur (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-uzb-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-sim-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-tibt (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fao (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-thai (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-uzb (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mri (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-srp (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ind (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-oci (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ukr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-msa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-cyrl (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-knda (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-gla (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-yor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hye (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fil (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-khmr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-enm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-nor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-armn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-spa-old (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-div (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-jpan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-geor (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hrv (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-lav (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-frak (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-yid (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-syr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hat (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ltz (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-uig (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-mymr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-gujr (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-est (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-snd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-jpn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hun (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-arab (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-viet (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hans-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-ethi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tir (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sqi (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-khm (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-eus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-kor-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-jpan-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-taml (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-mkd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tgk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-telu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-pus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-hang-vert (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-frk (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-dzo (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-cher (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-srp-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-aze (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-all (4.1.1-2.1build1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!apt install -y tesseract-ocr\n",
        "!apt install -y libtesseract-dev\n",
        "!apt install -y tesseract-ocr-all\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBZIWDzXInPw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import cv2\n",
        "import pytesseract\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHUN0OUdItbS"
      },
      "outputs": [],
      "source": [
        "class Ocr:\n",
        "    def preprocess_image(self, image_path, scale_percent=200):\n",
        "        self.scale_percent = scale_percent\n",
        "        img = cv2.imread(image_path)\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        width = int(gray_img.shape[1] * self.scale_percent / 100)\n",
        "        height = int(gray_img.shape[0] * self.scale_percent / 100)\n",
        "        gray_img = cv2.resize(gray_img, (width, height), interpolation=cv2.INTER_CUBIC)\n",
        "        denoised = cv2.fastNlMeansDenoising(gray_img, h=10)\n",
        "        return denoised\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        cleaned = re.sub(r'[^–∞-—è–ê-–Ø—ë–Åa-zA-Z0-9\\s.,;:!?()%\\-‚Äî‚Äì¬´¬ª]', '', text)\n",
        "        return cleaned\n",
        "\n",
        "    def img2txt(self, img, min_confidence=60):\n",
        "        self.min_confidence = min_confidence\n",
        "        processed = self.preprocess_image(img)\n",
        "        config = '--psm 6 --oem 3'\n",
        "        data = pytesseract.image_to_data(\n",
        "            processed,\n",
        "            lang='rus+eng',\n",
        "            config=config,\n",
        "            output_type=pytesseract.Output.DICT\n",
        "        )\n",
        "        filtered_text = []\n",
        "        for i, conf in enumerate(data['conf']):\n",
        "            if int(conf) > self.min_confidence:\n",
        "                filtered_text.append(data['text'][i])\n",
        "        return ' '.join(filtered_text)\n",
        "\n",
        "    def process(self, image_path):\n",
        "        text = self.img2txt(image_path)\n",
        "        return self.clean_text(text)\n",
        "\n",
        "\n",
        "class ImageCaptioner:\n",
        "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-large\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "    def describe(self, image_path, max_length=50):\n",
        "        image = Image.open(image_path)\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        output = self.model.generate(**inputs, max_new_tokens=max_length)\n",
        "        caption = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return caption\n",
        "\n",
        "    def describe_with_context(self, image_path, question):\n",
        "        image = Image.open(image_path)\n",
        "        inputs = self.processor(images=image, text=question, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        output = self.model.generate(**inputs)\n",
        "        answer = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kjxQqIJJA5g",
        "outputId": "6977aad3-11e1-469e-e437-464305c680db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a close up of a book with a text on it\n",
            "94 THE LIFE OF TOLSTOY The results he published in his book, A Criticism of Dogmatic Theology. Freeing himself from the creed of the Church, he was inevitably led to examine the teaching of Christianity as contained in the Bible, and conse- quently the Bible itself. He did this in a lengthy work, The Four Gospels Unified and Trans- lated. In this work, step by step, he analysed the text of the Gospels, throwing aside that which was not clear or not directly connected with the main idea of Christianity. The passages clearly express- ing this principal idea he arranged in a connected, easily understood form, and the whole teaching assumed a complete, harmonious, and popular character. Arriving at the very root of Christianity, Tolstoy undertook a new work to explain his con- ception of it: What is My Faith? It may be said that, with this book, the cycle of his religious development was accomplished.\n"
          ]
        }
      ],
      "source": [
        "captioner = ImageCaptioner()\n",
        "description = captioner.describe(\"The_life_of_Tolstoy.jpg\")\n",
        "print(description)\n",
        "\n",
        "\n",
        "ocr = Ocr()\n",
        "text = ocr.process('The_life_of_Tolstoy.jpg')\n",
        "print(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0499bb6b5d1e4d0680bdb73b7299f7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054a6ec2121f48b59e783ac867ff7f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "091e87323df44437b85815e384f6f2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0aadd6e8490941ae9bdf20403d7886e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e090e21ffa1741e984af4a2035a7ff0a",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_304e179b09094c47a10ed3ae6007f7b5",
            "value": 190
          }
        },
        "0d355e8018e7411bb987595d3282a16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a7c5d956474ac09605cc080caaab19",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a07054577ddc44f180e375a38039160c",
            "value": "vocab.txt:‚Äá"
          }
        },
        "0ef43210f4e948e4bbfad66da46062cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9488a5cc84fd48a782753bcd681ab8e6",
              "IPY_MODEL_ad36412ff6804d589c0e742e08079bad",
              "IPY_MODEL_2fc555ae01294bb7a0dc34e7f4465115"
            ],
            "layout": "IPY_MODEL_65743bfc423b4dba9071a08c67a9d234"
          }
        },
        "13142dfa5b4342eb9b41221dedce2f70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14ef4d65267646319f23195ec3d0b5f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b274a4b65cb41fbba46e2e2561f30ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ec11d7fb1d446e69c48aa95daea69ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217e82ba033648d5b94f4a55796f1643": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2722025bc7064f46b4e3e034d7083ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27cd6a38ee964c0a80f2f8b8233032a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27d739cfe817469399e7161fae16b816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7db72c12c614637be98c0198d6c1e78",
              "IPY_MODEL_6fbff9f1996445ee8d8a293c4970ba5e",
              "IPY_MODEL_c915c1c8c7644ac6b8a489d0e6adcbdb"
            ],
            "layout": "IPY_MODEL_e9e2e48cb5a44d3a8283e9b7e370fe7e"
          }
        },
        "2ab98e4735844054a97adade5032d0bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2c31b7f1a7164fa295c15cfbc9c9be4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d46aa583da8471290093f4328375758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d760d849ed546599aa02ae927279fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fc555ae01294bb7a0dc34e7f4465115": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_851cded58f7741528fa3f75086f7b69b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0499bb6b5d1e4d0680bdb73b7299f7a9",
            "value": "‚Äá363/363‚Äá[00:00&lt;00:00,‚Äá14.9kB/s]"
          }
        },
        "304e179b09094c47a10ed3ae6007f7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "337055f2e7b14dc9ba9466af21a8df3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3652b1e0d8f74c3880caf9d9e0ff4ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e9e450bfc7742c4971982ee6c14487d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_789b7bf07ed64687ac6e577bebe73937",
            "value": "modules.json:‚Äá100%"
          }
        },
        "3c63c161ec504640adce7bb6456ce07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3652b1e0d8f74c3880caf9d9e0ff4ac6",
              "IPY_MODEL_e350952d965142a39b75b42cb061f792",
              "IPY_MODEL_ec832bd8ee034f0695dcb2021d7c9278"
            ],
            "layout": "IPY_MODEL_feb62c60abc64fdf9d511641a86772b6"
          }
        },
        "3e0a3149f4ab437ab4e615eedfc52361": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428763fc2a2f481fb670dd0eba635bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8d37f2080fa4ee7b929167ab385475a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a79f7439fb0432ca38af9fa6505bb40",
            "value": "README.md:‚Äá"
          }
        },
        "4365a2416c784e77889bda1c324b37b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4fa636f8f8a4c11944743a3ae118dc7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b1737aaea82940e5b7ef79f16836fdd2",
            "value": "‚Äá438M/438M‚Äá[00:03&lt;00:00,‚Äá205MB/s]"
          }
        },
        "462353a18b7145058a077940193a0171": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48a7c5d956474ac09605cc080caaab19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e57f2717fe44f529defa3963dac1e82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e66a8e5ef8e4649a75833f218393441": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9e450bfc7742c4971982ee6c14487d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b66c3d6fb94cdcac951e87ae474710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_217e82ba033648d5b94f4a55796f1643",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d934dac5094540818895c7fe52df0bab",
            "value": "‚Äá239/239‚Äá[00:00&lt;00:00,‚Äá22.4kB/s]"
          }
        },
        "51fb9641cb684979acc2f06588fae3eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "587704931fd3408b85acb2ed15f3af78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e0a3149f4ab437ab4e615eedfc52361",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9393342e318474c8e7c9c33c093f814",
            "value": 116
          }
        },
        "594ac05acdb74516a661a2d72aef60ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_428763fc2a2f481fb670dd0eba635bac",
              "IPY_MODEL_be4991b29a9b47258c133d3d44833029",
              "IPY_MODEL_776b4095a4de4c9b86de984a7ea856b1"
            ],
            "layout": "IPY_MODEL_92356ba0ac6244a79c1f219a36b6edec"
          }
        },
        "5cd26bc0718f47b499331b836e7cf697": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcf7a0de03a42d893d9e0e6a5295ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ea8e4adc7ab4fcda458e81f076ac0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f80e56706cab44dda3426de4ce49ec40",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8c6d1139e257432b91666ae0a04b79f5",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "6103707b4777415585103d47c9264ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64c638e532c34ba38c38cc91f03d41d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65569198d5e049cd810336b3d50fc6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7abef1421ca4210a371bf9031979a4f",
              "IPY_MODEL_587704931fd3408b85acb2ed15f3af78",
              "IPY_MODEL_81178efff367451f807cae878c58b354"
            ],
            "layout": "IPY_MODEL_92d883ca481c4ef0af603b97ef1599b0"
          }
        },
        "65743bfc423b4dba9071a08c67a9d234": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fbff9f1996445ee8d8a293c4970ba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c638e532c34ba38c38cc91f03d41d6",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f66e776a769d446d91acc3586c84a934",
            "value": 53
          }
        },
        "726ee45e96ac457e946066832b5d1673": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7291cc309e7a46abb5d501eb31f1b5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756eda1229d54f1381bac09b4a7b4159": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76517d9f4124424e9ac830f090936fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770dc95a3193446eaaaf688bfeb08f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae18d5a3d484ec5a0a114b9d34ab587",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_51fb9641cb684979acc2f06588fae3eb",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "776b4095a4de4c9b86de984a7ea856b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9caf6c75e9504309b96cf31ea95f7183",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6103707b4777415585103d47c9264ba7",
            "value": "‚Äá11.6k/?‚Äá[00:00&lt;00:00,‚Äá561kB/s]"
          }
        },
        "789b7bf07ed64687ac6e577bebe73937": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a79f7439fb0432ca38af9fa6505bb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ab81a07855c456781406ebefef6d758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "808acd9205384ba8af805cfbe177face": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f3ee01e4754741811d2985afaf80aa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fca7817f0b84498189771d67f948416c",
            "value": "‚Äá466k/?‚Äá[00:00&lt;00:00,‚Äá19.2MB/s]"
          }
        },
        "81178efff367451f807cae878c58b354": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98e20ab02c6944328b60e4adc545dd80",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f797c298b18f47de98d8e722ce3dc3bd",
            "value": "‚Äá116/116‚Äá[00:00&lt;00:00,‚Äá6.06kB/s]"
          }
        },
        "824f404ec0d84a7193cd1be9666fbaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "851cded58f7741528fa3f75086f7b69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85857aa1ac9f4711b22d05f64090aade": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "86f3ee01e4754741811d2985afaf80aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae18d5a3d484ec5a0a114b9d34ab587": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bfacf5b88444db987144d4d80ba211e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d355e8018e7411bb987595d3282a16e",
              "IPY_MODEL_b873f912f7d946b8b8922897beaf717e",
              "IPY_MODEL_fb7fa4c16f284e029152a0a18d6471c5"
            ],
            "layout": "IPY_MODEL_9e0ec260461340209471e0fc1db2dc9d"
          }
        },
        "8c6d1139e257432b91666ae0a04b79f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fc3978e001d49e68447743679f3fe66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92356ba0ac6244a79c1f219a36b6edec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92599f8eb31a41588f6b7fc2a74b5f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a35b78a36d24bc4981374766a81b025",
              "IPY_MODEL_0aadd6e8490941ae9bdf20403d7886e1",
              "IPY_MODEL_a132f90a261c4061bab95edad89c7c9a"
            ],
            "layout": "IPY_MODEL_1b274a4b65cb41fbba46e2e2561f30ce"
          }
        },
        "92d883ca481c4ef0af603b97ef1599b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9488a5cc84fd48a782753bcd681ab8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3128c84e5e04a578c65d6175b762f50",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5dcf7a0de03a42d893d9e0e6a5295ef3",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "96a25420cd9e422b88143df294f40204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96dac914ff1f4febb60047c5415bb96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e57fb762526f46e982af819b4dbfe754",
              "IPY_MODEL_afbfc6b621bb43e69279f2542b63afed",
              "IPY_MODEL_808acd9205384ba8af805cfbe177face"
            ],
            "layout": "IPY_MODEL_7291cc309e7a46abb5d501eb31f1b5c0"
          }
        },
        "98e20ab02c6944328b60e4adc545dd80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a35b78a36d24bc4981374766a81b025": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14ef4d65267646319f23195ec3d0b5f3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8fc3978e001d49e68447743679f3fe66",
            "value": "config.json:‚Äá100%"
          }
        },
        "9caf6c75e9504309b96cf31ea95f7183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d031315caed4dacb5a5fee94ff7da73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e0ec260461340209471e0fc1db2dc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fbbab679fc144bf8ed410b706a1724c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a07054577ddc44f180e375a38039160c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a132f90a261c4061bab95edad89c7c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e57f2717fe44f529defa3963dac1e82",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2722025bc7064f46b4e3e034d7083ac3",
            "value": "‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá10.3kB/s]"
          }
        },
        "a3128c84e5e04a578c65d6175b762f50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32ef385b157447a88e176df4e18722e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d79495f2623f474bb5acbbdf08bfe327",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091e87323df44437b85815e384f6f2bc",
            "value": 239
          }
        },
        "a7abef1421ca4210a371bf9031979a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13142dfa5b4342eb9b41221dedce2f70",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f12a0e8657ec4f3fa3aa9750635e8ca0",
            "value": "config_sentence_transformers.json:‚Äá100%"
          }
        },
        "ac3b138194654737b85062eec317ead5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_770dc95a3193446eaaaf688bfeb08f97",
              "IPY_MODEL_de9a4b91c3084c7180c30fd4740dd885",
              "IPY_MODEL_4365a2416c784e77889bda1c324b37b8"
            ],
            "layout": "IPY_MODEL_dde8335f6b6d48bcbdc2305f1d9126d1"
          }
        },
        "acb56058a5b84bab9e73f33866890bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad36412ff6804d589c0e742e08079bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd40699cf1f450aaa8fefe6185c4e88",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27cd6a38ee964c0a80f2f8b8233032a4",
            "value": 363
          }
        },
        "af14ff8aa1814688a49340e989edead3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_726ee45e96ac457e946066832b5d1673",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96a25420cd9e422b88143df294f40204",
            "value": 571
          }
        },
        "afbfc6b621bb43e69279f2542b63afed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab98e4735844054a97adade5032d0bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_054a6ec2121f48b59e783ac867ff7f9f",
            "value": 1
          }
        },
        "b1737aaea82940e5b7ef79f16836fdd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b450a990f5f34b95947b16794afab63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c31b7f1a7164fa295c15cfbc9c9be4a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2d760d849ed546599aa02ae927279fe2",
            "value": "config.json:‚Äá100%"
          }
        },
        "b59aafc855d140109a3bce64b9a484e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61defa64ef841989016665e76d021b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b873f912f7d946b8b8922897beaf717e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85857aa1ac9f4711b22d05f64090aade",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acb56058a5b84bab9e73f33866890bcf",
            "value": 1
          }
        },
        "bc8f6197fdea4337ba6f7f0967acc0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdd40699cf1f450aaa8fefe6185c4e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4991b29a9b47258c133d3d44833029": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b6075324fa4a12a7e94f10c09ead3b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ab81a07855c456781406ebefef6d758",
            "value": 1
          }
        },
        "c915c1c8c7644ac6b8a489d0e6adcbdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337055f2e7b14dc9ba9466af21a8df3d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_462353a18b7145058a077940193a0171",
            "value": "‚Äá53.0/53.0‚Äá[00:00&lt;00:00,‚Äá3.87kB/s]"
          }
        },
        "c9393342e318474c8e7c9c33c093f814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d04247ca7bdd4b50ae7d1ff211b075b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d494e2be5ae64a7f97449c3378980e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d62648797e4340f0a0378d70fba0d6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b450a990f5f34b95947b16794afab63a",
              "IPY_MODEL_af14ff8aa1814688a49340e989edead3",
              "IPY_MODEL_fbd2791ed8084434ac2fe783ff1b74bd"
            ],
            "layout": "IPY_MODEL_756eda1229d54f1381bac09b4a7b4159"
          }
        },
        "d79495f2623f474bb5acbbdf08bfe327": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7db72c12c614637be98c0198d6c1e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e951952fc4764a48900ec794bf039d6d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b61defa64ef841989016665e76d021b3",
            "value": "sentence_bert_config.json:‚Äá100%"
          }
        },
        "d934dac5094540818895c7fe52df0bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dde8335f6b6d48bcbdc2305f1d9126d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9a4b91c3084c7180c30fd4740dd885": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e66a8e5ef8e4649a75833f218393441",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d46aa583da8471290093f4328375758",
            "value": 437971872
          }
        },
        "e090e21ffa1741e984af4a2035a7ff0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e350952d965142a39b75b42cb061f792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3fa5994afb488f99f19f783ff542ae",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d031315caed4dacb5a5fee94ff7da73",
            "value": 349
          }
        },
        "e57fb762526f46e982af819b4dbfe754": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cd26bc0718f47b499331b836e7cf697",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d04247ca7bdd4b50ae7d1ff211b075b8",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "e6b6075324fa4a12a7e94f10c09ead3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e951952fc4764a48900ec794bf039d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e2e48cb5a44d3a8283e9b7e370fe7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebf520c424bb4a6c9d2212bbf6c6c813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ea8e4adc7ab4fcda458e81f076ac0b6",
              "IPY_MODEL_a32ef385b157447a88e176df4e18722e",
              "IPY_MODEL_51b66c3d6fb94cdcac951e87ae474710"
            ],
            "layout": "IPY_MODEL_b59aafc855d140109a3bce64b9a484e5"
          }
        },
        "ec832bd8ee034f0695dcb2021d7c9278": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec11d7fb1d446e69c48aa95daea69ab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_824f404ec0d84a7193cd1be9666fbaa8",
            "value": "‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá23.5kB/s]"
          }
        },
        "f12a0e8657ec4f3fa3aa9750635e8ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4fa636f8f8a4c11944743a3ae118dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66e776a769d446d91acc3586c84a934": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f797c298b18f47de98d8e722ce3dc3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f80e56706cab44dda3426de4ce49ec40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d37f2080fa4ee7b929167ab385475a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb3fa5994afb488f99f19f783ff542ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb7fa4c16f284e029152a0a18d6471c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76517d9f4124424e9ac830f090936fb6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc8f6197fdea4337ba6f7f0967acc0be",
            "value": "‚Äá232k/?‚Äá[00:00&lt;00:00,‚Äá9.66MB/s]"
          }
        },
        "fbd2791ed8084434ac2fe783ff1b74bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d494e2be5ae64a7f97449c3378980e9f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9fbbab679fc144bf8ed410b706a1724c",
            "value": "‚Äá571/571‚Äá[00:00&lt;00:00,‚Äá44.9kB/s]"
          }
        },
        "fca7817f0b84498189771d67f948416c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feb62c60abc64fdf9d511641a86772b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
