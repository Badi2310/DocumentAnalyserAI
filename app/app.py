import locale

import streamlit as st

from dotenv import load_dotenv

from utils.session_state import init_session_state, clear_chat
from utils.file_loader import handle_uploaded_files
from utils.database import clear_weaviate_data

from models.llm import init_llm
from models.rag import generate_response

locale.getpreferredencoding = lambda: "UTF-8"



init_session_state()

col1, col2 = st.columns(2)

with col1:
    st.button("–û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç", on_click=clear_chat, type="primary")

with col2:
    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –ë–î", type="secondary"):
        clear_weaviate_data()
        st.session_state.vector_db = None
        st.session_state.rag_sources = []
        st.session_state.uploader_key += 1
        st.rerun()



uploaded_files = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
    type=["pdf","txt","docx","md"],
    accept_multiple_files=True,
    key=f"uploader_{st.session_state.uploader_key}"
)

if uploaded_files:
    count = handle_uploaded_files(uploaded_files, st.session_state.rag_sources)
    st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])



load_dotenv()

llm_stream = init_llm()



if prompt := st.chat_input("–í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"):
    st.session_state.messages.append({"role":"user","content":prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        for chunk in generate_response(llm_stream, st.session_state.messages, st.session_state.rag_sources):
            full_response += str(chunk)
            placeholder.markdown(full_response, unsafe_allow_html=True)
        st.session_state.messages.append({"role":"assistant","content":full_response})
